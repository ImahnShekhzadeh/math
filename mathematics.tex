\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{comment, listings, centernot, amssymb, hyperref, graphicx, amsmath, textcomp, breqn, mathtools, csquotes, cancel, enumitem, amsthm, caption, bm, tikz} % amsthm provides `proof`environment
\usepackage[backend=biber]{biblatex}
\usepackage[english]{babel}
\usepackage{array}
\addbibresource{references.bib}

% NEW (16-04-21, 19p49)
\usepackage[margin = 1in]{geometry}
\numberwithin{equation}{section}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section] % reset theorem numbering for each chapter

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers
\newtheorem{lemma}[thm]{Lemma} % same for example numbers
\newtheorem{remark}[thm]{Remark} % same for example numbers
\newtheorem{theorem}[thm]{Theorem} 
\newtheorem{corollary}[thm]{Corollary}
\newcommand{\norm}[2]{\left\vert\left\vert #1 \right\vert\right\vert_{#2}}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand{\inner}[1]{\left\langle #1 \right\rangle}
\newcommand{\epi}[1]{\text{epi($ #1 $) } }
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\seq}[1][\varphi]{\left( #1 \right)_{n \in \mathbb{N}}}
\renewcommand{\qedsymbol}{$\blacksquare$}

\title{Mathematics}
% \subtitle{Questions}
\author{Imahn Shekhzadeh}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage 
	
	\section{Algebra}
	
	\begin{defn}[Action]\label{defn:algebra__group_action}
		Let $G$ be a group (where $e$ is the neutral element), and $X$ a set. Then a (left) group action $\alpha$ of $G$ on $X$ is a function
		\begin{align}
			\alpha: G\times X \rightarrow X
		\end{align}
		satisfying the following two axioms $(\forall g, h\in G) \wedge (\forall x\in X)$:
		\begin{align*}
			\alpha(e, x) &= x, 
			\\ \alpha(g, \alpha(h, x)) &= \alpha(gh, x)
		\end{align*}
		The group action is commonly written as $g\cdot x$ s.t.~the axioms become
		\begin{align}\label{eqn:left__group_action}
			e\cdot x &= x, 
			\\ g\cdot \left(h\cdot x\right) &= \left(gh\right)\cdot x
		\end{align}
		A right group action of $G$ on $X$ is a function
		\begin{align}
			\alpha: X\times G \rightarrow X
		\end{align}
		satisfying the following two axioms $(\forall g, h\in G) \wedge (\forall x\in X)$:
		\begin{align}
			\alpha(x, e) &= x, 
			\\ \alpha(\alpha(x, g), h) &= \alpha(x, gh)
		\end{align}
	\end{defn}
	
	\begin{exmp}
		Consider $G = \text{SO}(2)$, i.e.~the group of all rotations in $\mathbb R^2$, with the group operation given by the composition of rotations, and let $X = S^1$ be the unit circle in $\mathbb C$. Now consider the group action
		\begin{align}
			\Phi:\text{SO}(2) \times S^{1} \rightarrow S^{1}, (R_{\theta}, p) \mapsto R_{\theta}(p)
		\end{align}
		This is a group action on $X = S^{1}$, since 
		\begin{align*}
			\Phi(R_{\theta_1}, \Phi(R_{\theta_2}, p)) = \Phi(R_{\theta_1 + \theta_2}, p).
		\end{align*}
		In the short notation, one would write this as
		\begin{align*}
			R_{\theta_1}\cdot \left( R_{\theta_2} \cdot p \right) = R_{\theta_1 + \theta_2} \cdot p.
		\end{align*}
	\end{exmp}
	
	\begin{defn}[Signals\footnote{\cite[p.~11]{bronstein2021geometric}}] 
		Let $V$ be a vector space\footnote{In computer vision, its dimensions would be called \textit{channels}.} and $\Omega$ a set, possibly with additional structure. Then the space of $V$-valued signals on $\Omega$,
		\begin{align}
			\mathcal X(\Omega, V) = \{ x: \Omega \rightarrow V \}
		\end{align}
		is a function space with a vector space structure. (Note that each $x\in \mathcal X(\Omega, V)$ is a signal.)
		
		Addition and scalar multiplication of signals are defined as 
		\begin{align}
			(\alpha x + \beta y)(u) := \alpha x(u) + \beta y(u) \quad \forall u\in\Omega\ \forall \alpha, \beta\in \mathbb R.
		\end{align}
	\end{defn}

	\begin{lemma}
		If $V$ is endowed with an inner product $\langle v, w\rangle_{V}$ and a measure $\mu$ on a $\sigma$-algebra defined on the set $\Omega$ (wrt which an integral can be defined), we have the following induced inner product on $\mathcal X(\Omega, V)$
		\begin{align}
			\langle x, y\rangle_{\mathcal X(\Omega, V)} := \int_{\Omega}\langle \underbrace{x(u)}_{\in V}, \underbrace{y(u)}_{\in V}\rangle_{V}d\mu(u)
		\end{align}
	\end{lemma}

	\begin{proof}
		Trivial.
	\end{proof}

	\begin{remark}
		The domain (set) $\Omega$ can be discrete, in which case $\mu$ can be chosen to be the counting measure, in which case the integral becomes a sum.
	\end{remark}

	\begin{exmp}
		Let $\Omega = \mathbb Z_{n}\times Z_{n}$, i.e.~a two-dimensional grid, and $x$ an RGB image, i.e.~a signal $x:\Omega\rightarrow \mathbb R^3$, and $f$ a function (such as a single-layer perceptron) operating on $3n^2$-dimensional inputs.
	\end{exmp}
	
	\begin{defn}[Homomorphism]
		Let $f:A\rightarrow B$ be a map between two sets $A$ and $B$ that are equipped with the same structure. Also assume that $\bm{\cdot}$ is an operation of the structure. Then $f$ is said to be a homomorphism if $\forall x, y\in A$,
		\begin{align}
			f(x\bm{\cdot} y) = f(x) \bm{\cdot} f(y).
		\end{align}
	\end{defn}
	
	\begin{defn}[Group representation]
		An $n$-dimensional real \textbf{representation} of a group $G$ is a map $\rho:G\rightarrow \text{GL}(n, \mathbb R)$ (correspondingly, one defines an $n$-dimensional complex representation) \cite[p.~15]{bronstein2021geometric}, where each $g\in G$ is mapped to an invertible matrix, under the \textit{condition} that the map is a homomorphism, i.e.~$\rho(gh) = \rho(g)\rho(h)$.
	\end{defn}

	\begin{exmp}[Group representation]
		Let $G = \mathbb Z_{n}$\footnote{Cyclic group of order $n$, $\mathbb Z_n = \{ 0, 1, \dots, n - 1 \}$.}, where the group operation is addition. The group representation 
		\begin{align}
			\rho: \mathbb Z_{n} \rightarrow \text{GL}(1, \mathbb C), g\mapsto \exp\left(\frac{2\pi ig}{n}\right)
		\end{align}
		To see that this is indeed a group representation, note that
		\begin{align}
			\rho(gh) = \rho(g + h) = \exp\left( \frac{2\pi igh}{n} \right) = \exp\left( \frac{2\pi i\left(g + h\right)}{n} \right) = \exp\left( \frac{2\pi ig}{n} \right)\exp\left( \frac{2\pi ih}{n} \right),
		\end{align}
		i.e.~the group representation is a homomorphism.
	\end{exmp}
	
	\begin{defn}[Equivariance]
		Let $\rho$ be a representation of a group $G$. A function $f:\mathcal X(\Omega, V) \rightarrow \mathcal X(\Omega, V)$ is $G$-equivariant if 
		\begin{align}\label{eqn:group_action}
			f(\rho(g)x) = \rho(g)f(x) \quad \forall g\in G.
		\end{align}
	\end{defn}
	
	\begin{remark}
		Note that $\rho(g)\in \text{GL}(n, \mathbb K)$, $x\in \mathcal X(\Omega, V)$, and hence $\rho(g)x$ is a group action mediated by the representation $\rho$ on the group element $g$, i.e.~$\rho(g)x\in \mathcal X(\Omega, V)$. Also note that $f(x)\in \mathcal X(\Omega, V)$, i.e.~$\rho(g)f(x)$ is a group action as well, where the group element $g$ -- mediated by the representation $\rho$ -- and hence $\rho(g)f(x) \in \mathcal X(\Omega, V)$.
		
		The fact that we have a group action implies according to Def.~\ref{defn:algebra__group_action} and Eq.~\eqref{eqn:group_action},
		\begin{align}
			f\left(\rho(g)(\rho(h)x)\right) \overset{\scriptsize \eqref{eqn:left__group_action}}{=} f\left( (\rho(g)\rho(h))x \right) = \left(\rho(g)\rho(h)\right)f(x).
		\end{align}
	\end{remark}
	
	\newpage
	
	\section{Measure Theory}
	\begin{defn}[Generated $\sigma$-algebra \cite{generated-sigma-algebras}]
		Let $X$ be a set and $\mathcal E\subset \mathcal P(X)$ a non-empty collection of subsets of $X$. The \textit{smallest} $\sigma$-algebra containing all the sets of $\mathcal E$ is denoted by $\sigma(\mathcal E)$. 
	\end{defn}

	\begin{corollary}
		Let $\mathcal E_1$, $\mathcal E_2\subset \mathcal P(X)$ be such that $\mathcal E_1 \subset \mathcal E_2$. Then $\sigma(\mathcal E_1) \subset \sigma(\mathcal E_2)$.    
	\end{corollary}
	
	\begin{defn}[Measurable function]
		Let $(X, \mathcal E)$ and $(Y, \mathcal F)$ be measurable spaces. A map $f: X\rightarrow Y$ is said to be \textit{$\mathcal E$-measurable} if 
		\begin{align} \label{measurable-mapping-eq}
			f^{-1}(\mathcal F) := \left\{ f^{-1}(A) \vert A\in \mathcal F \right\} := \left\{ \left\{ x\in X \vert f(x) \in A \right\} \vert A\in \mathcal F \right\} \subset \mathcal E.
		\end{align} 
	\end{defn}

	\begin{theorem}[Generator and measurable function \cite{measurable-functions}] \label{generator-and-measurable-function} Let $(X, \mathcal E)$ and $(Y, \mathcal F)$ be measurable spaces and $\mathcal F = \sigma(\mathcal G)$, i.e. $\mathcal F$ is the $\sigma$-algebra generated by a family $\mathcal G \subset \mathcal P(Y)$, where $\mathcal P(Y)$ denotes the power set of $Y$. Then $f: X\rightarrow Y$ is measurable if and only if 
		\begin{align}
			f^{-1}\left(G\right) \in \mathcal E \ \forall G\in\mathcal G. 
		\end{align}
	\end{theorem}
	\noindent \textit{Proof}. \enquote{$\Rightarrow$} Since $\mathcal F = \sigma(\mathcal G)$, it obviously holds that $\mathcal G\subset \mathcal F$ and therefore $f^{-1}(G)\in \mathcal E \ \forall G\in\mathcal G$ is ensured by $f$ being measurable. 
	\\ \\ 
	\enquote{$\Leftarrow$} Define the set $\mathcal M:= \left\{ B\subset Y \mid f^{-1}(B)\in \mathcal A \right\}$. First we want to convince ourselves that $\mathcal M$ is a $\sigma$-algebra on $Y$. 
	\begin{enumerate}
		\item $\emptyset \in \mathcal M$, since $f^{-1}(\emptyset) = \left\{ x\in X \mid f(x)\in \emptyset \right\} = \emptyset$. 
		
		\item Let $B\in \mathcal M$, then also $Y\backslash B\in \mathcal M$, since $f^{-1}\left(Y\backslash B\right) = f^{-1}(Y)\backslash f^{-1}(B)$, as can be easily shown by using the definition of the complement of a set. Since $f^{-1}(Y) = X$, it follows that $f^{-1}(Y\backslash B) = X\backslash f^{-1}(B)$. Since by assumption $B\in \mathcal M$ (and therefore $f^{-1}(B)\in \mathcal A$) and $\mathcal A$ itself is a $\sigma$-algebra, it follows that $X\backslash f^{-1}(B)\in\mathcal A$. 
		
		\item  Let $B_i \in \mathcal M$ for $i\in \mathbb N$, then also $\cup_{i\in\mathbb N}B_i\in \mathcal M$, since $$f^{-1}\left(\bigcup_{i\in\mathbb N}B_i\right) = \bigcup_{i\in\mathbb N}f^{-1}\left(B_i\right).$$ 
	\end{enumerate}
	Since $\mathcal M$ is a $\sigma$-algebra and since $\mathcal G\subset \mathcal M \Rightarrow \mathcal F = \sigma(\mathcal G)\subset \mathcal M = \sigma(\mathcal M) $, it follows that $f$ is measurable. 
	 \newline \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$

	\begin{lemma}[Push-forward measure]\label{push-forward-measure}
		Let $(X, \mathcal E)$ and $(Y, \mathcal F)$ be measurable spaces. Given a measurable map $f: X\rightarrow Y$ and a measure $\mu$ on $\mathcal E$, let $f_{\#}\mu$ be defined by 
		\begin{align}
			f_{\#}\mu(A) := \mu(f^{-1}(A)) \quad \forall A\in \mathcal F. 
		\end{align}
		$f_{\#}\mu$ is a measure on $\mathcal F$ and called the \textit{push-forward} of $\mu$ under $f$. 
	\end{lemma}

	\noindent 
	\textit{Proof}. Obviously, $f_{\#}\mu(\emptyset) = \mu(f^{-1}(\emptyset)) \overset{\footnotesize\eqref{measurable-mapping-eq}}{=} \mu(\emptyset) = 0$. Also, no matter what kind of set $A\in \mathcal F$ we take, since $\mu(A) \geq 0$, the same holds for $f_{\#}\mu(A)$. Finally, let $(A_n)_{n\in\mathbb{N}} \subset \mathcal F$ be a sequence of mutually disjoint sets, then: 
	\begin{align}
		&f_{\#}\mu\left(\bigcup_{n\in\mathbb N}A_n \right) 
					\\ &= 
		\mu\left(f^{-1}\left(\bigcup_{n\in\mathbb N}A_n\right) \right) 
					\\ &\overset{\eqref{measurable-mapping-eq}}{=}
		\mu\left( \left\{ x\in X \bigg\vert f(x) \in  \bigcup_{n\in\mathbb N}A_n \right\}\right) 
					\\ &= 		
		\mu\left( \bigcup_{n\in\mathbb N}\left\{ x\in X \vert f(x)\in A_n \right\} \right) 
					\\ &=
		\mu\left( \bigcup_{n\in\mathbb{N}}f^{-1}(A_n) \right)
					\\ &=
		\sum_{n\in\mathbb N}\mu\left(f^{-1}(A_n)\right)
					\\ &=
		\sum_{n\in\mathbb{N}}f_{\#}\mu(A_n) 
	\end{align} 
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{corollary}[Push-forward of a probability measure]
		Let $(X, \mathcal E)$ and $(Y, \mathcal F)$	be measurable spaces. Given a measurable map $f: X\rightarrow Y$ and a \underline{probability} measure on $\mathcal E$, the push-forward of $\mu$ under $f$, denoted by $f_{\#}\mu$, is also a probability measure. 
	\end{corollary}
	\noindent\textit{Proof}. Since $\mu$ is in particular a measure and thus $f_{\#}\mu$ is also a measure, we only need to show that  
	\begin{align}
		f_{\#}\mu(Y) = \mu\left(f^{-1}\left(\mu\right)\right) = \mu\left(\left\{ x\in X \mid f(x)\in Y  \right\}\right) = \mu(X) = 1. 
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}[$\sigma$-finite measure] Let $(X, \mathcal A)$ be a measurable space and $\mu$ a measure on it. If there are sets $A_1$, $A_2$, $\dots \in \mathcal A$ with $\mu(A_n) < \infty \ \forall n\in \mathbb N$ that satisfy 
	\begin{align}
		\bigcup_{n\in\mathbb N}A_n = X
	\end{align}  
	then we say $\mu$ is \textit{$\sigma$-finite}. 
	\end{defn}

	\begin{remark}
		Obviously, every finite measure is $\sigma$-finite; however, the converse does not necessarily hold \cite{sigma-finite}. 
	\end{remark}
	
	
	\begin{defn}[Absolutely continuous measures.] 
	Let $\mu$ and $\nu$ be two measures on a $\sigma$-algebra $\mathcal A$. $\nu$ is called \textit{absolutely continuous} w.r.t. $\mu$, written as 
		\begin{align}
			\nu \ll \mu, 
		\end{align}
	if for each $A\in \mathcal A$, $\mu(A) = 0$ implies $\nu(A) = 0$. If $\mu$ and $\nu$ are both absolutely continuous w.r.t each other $\mu$ and $\nu$ are called \textit{equivalent}. 
	\end{defn}

	\begin{theorem}[Radon-Nikodym Theorem \cite{measure-integration}]
	Let $\mu$ be a $\sigma$-finite measure on a measurable space ($S$, $\mathcal{A}$). Then it is equivalent: 
		\begin{enumerate}
			\item $\nu \ll \mu$, 
			\item $d\nu = hd\mu$ for some measurable function $h: S\rightarrow \mathbb R_{+}$. 
		\end{enumerate}
	The density $h$ then is $\mu$-a.e. finite and $\mu$-a.e. unique. 
	\end{theorem}
	
	\begin{lemma}[Frechét Inception Distance]
		For two multivariate Gaussian distributions $\mathcal G(\mu_{x}, \Sigma_{x})$, $\mathcal G(\mu_{y}, \Sigma_{y})$, the \textit{Frechét Inception Distance} (FID) is defined as [\url{https://arxiv.org/pdf/1706.08500.pdf}]: 
		\begin{align}
			d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) := \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \text{Tr}\left(\Sigma_{x} + \Sigma_{y} - 2\left(\Sigma_{x}\Sigma_{y}\right)^{\frac{1}{2}}\right)}. 
		\end{align}
		It is a metric.\
	\end{lemma}
	\begin{proof}
		Clearly, $d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{x}, \Sigma_{x})\right) = 0$.\ Also, $d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) = d\left(\mathcal G(\mu_{y}, \Sigma_{y}), \mathcal G(\mu_{x}, \Sigma_{x})\right)$, which holds because $\text{Tr}(AB) = \text{Tr}(BA)$ for any matrices $A$ and $B$.\ To see that $d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) \geq 0$, note that $\text{Tr}(\Sigma_{x} + \Sigma_{y} - 2\left(\Sigma_{x}\Sigma_{y}\right)^{1/2}) = \text{Tr}\left(\left(\Sigma_{x}^{1/2} - \Sigma_{y}^{1/2}\right)^{2}\right) \geq 0$, since the covariance matrices contain the variances on the diagonal, which are obviously non-negative.\ It now remains to be shown that also the triangle inequality is fulfilled.\ For this, note that 
		\begin{align}
			d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \text{Tr}\left(\Sigma_{x} + \Sigma_{y} - 2\left(\Sigma_{x}\Sigma_{y}\right)^{\frac{1}{2}}\right)} 
			\\ &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \text{Tr}\left(\left(\Sigma_{x}^{1/2} - \Sigma_{y}^{1/2}\right)^{2}\right)}
			\\ &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{y}\right\vert\right\vert_{2}^{2}}, 
		\end{align}
		where $\sigma_{x}$ and $\sigma_{y}$ denote vectors containing the standard deviations of the two Gaussian distributions.\ Clearly, 
		\begin{align}
			d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{z}, \Sigma_{z})\right) &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{z} \right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{z}\right\vert\right\vert_{2}^{2}} 
			\\ &\leq \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \mu_{y} - \mu_{z}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{y}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{y} - \sigma_{z}\right\vert\right\vert_{2}^{2}} 
			\\ &\leq \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{y}\right\vert\right\vert_{2}^{2}} \ + \ \sqrt{\left\vert\left\vert \mu_{y} - \mu_{z}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{y} - \sigma_{z}\right\vert\right\vert_{2}^{2}} 
			\\ &= d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) + d\left(\mathcal G(\mu_{y}, \Sigma_{y}), \mathcal G(\mu_{z}, \Sigma_{z})\right), 
		\end{align}
		since $\sqrt{x+y} \leq \sqrt{x} + \sqrt{y}$, as one can directly show by squaring for non-negative $x$, $y\in\mathbb R$.\ 
	\end{proof}

	\begin{defn}[Convergence in probability]
		Assume we have a sequence of random variables $(X_{n})_{n\in \mathbb N}$, defined on a probability space $(\Omega, \mathcal F, \mathbb P)$.\ We say this sequence converges to another random variable $X$ if 
		\begin{align}
			\forall \epsilon > 0: \lim\limits_{n\rightarrow\infty}\mathbb P\left\{ \left\vert X_{n} - X\right\vert > \epsilon \right\} = 0. 
		\end{align}
	\end{defn}
	
	\newpage 
	\section{Functional Analysis}
	\begin{defn}[Inner Product Space]
		Let $\mathbb K$ be a field ($\mathbb K = \mathbb R$ or $\mathbb K = \mathbb C$). An inner product space is a vector space $V$ over $\mathbb K$ that allows for an \textbf{inner product} 
		\begin{align}
			\left\langle \bm{\cdot}, \bm{\cdot}\right\rangle: V\times V\rightarrow \mathbb K
		\end{align}
		satisfying the following properties $\forall \alpha, \beta\in \mathbb K; x, y, z\in V$:
		\begin{enumerate}
			\item \textbf{Conjugate symmetry}:
			\begin{align}
				\langle x, y\rangle = \overline{\langle y, x\rangle},
			\end{align}
			which implies that $\langle x, x\rangle\in \mathbb R$, even if $\mathbb K = \mathbb C$.\footnote{Set $y = x$.} 
			
			For $\mathbb K = \mathbb R$, conjugate symmetry is exact symmetry.
			
			\item \textbf{Linearity} (in the first argument): 
			\begin{align}
				\langle \alpha x + \beta y, z\rangle = \alpha\langle x, z\rangle + \beta\langle y, z\rangle
			\end{align}
			From the conjugate symmetry property, this means that we have semi-linearity in the second argument:
			\begin{align}
				\langle x, \alpha y + \beta z\rangle = \overline{\langle \alpha y + \beta z, x \rangle} = \overline{\alpha\langle y, x \rangle + \beta\langle z, x \rangle} = \overline{\alpha}\langle x, y\rangle + \overline{\beta}\langle x, z\rangle.
			\end{align}
		
			\item \textbf{Positive-definiteness}:
			\begin{align}
				\langle x, x\rangle \geq 0
			\end{align}
			and 
			\begin{align}
				\langle x, x\rangle = 0 \Leftrightarrow x = 0.
			\end{align}
		\end{enumerate}
	\end{defn}

	\begin{defn}[Normed Space]\label{defn:normed_space}
		Let $\mathbb K$ be a space ($\mathbb{K} = \mathbb{R}$ or $\mathbb{K} = \mathbb{C}$). Then a mapping $\norm{\cdot}{}: X\to\mathbb{R}$ is called a \textbf{norm} if it satisfies the following properties for all $\varphi$, $\psi\in X$ and $\alpha\in \mathbb{K}$:
		
		\begin{enumerate}
			\item \textbf{Positivity}:
			\begin{align}
				\norm{\varphi}{} \geq 0
			\end{align}

			\item \textbf{Definiteness}:
			\begin{align}
				\norm{\varphi}{} = 0 \Leftrightarrow \varphi = 0
			\end{align}
		
			\item \textbf{Homogeneity}:
			\begin{align}
				\norm{\alpha\varphi}{} = \abs{\alpha}\norm{\varphi}{}
			\end{align}
		
			\item \textbf{Triangle inequality}:
			\begin{align}
				\norm{\varphi + \psi}{} \leq \norm{\varphi}{} + \norm{\psi}{}
			\end{align}
			
			A linear space $X$ with a norm $\norm{\cdot}{}$ is called a \textbf{normed linear space} or \textbf{normed space} for short. For a normed space, we shall use the notation $\left(X, \norm{\cdot}{}\right)$.
			
		\end{enumerate}
	\end{defn}

	\begin{exmp}\label{exmp:lp-norm-vectors}
		Let $X = \mathbb{R}^d$. Then for $1 \leq p < \infty$, the $L^p$ norm of a vector $x\in X$ is defined as:
		
		\begin{align}\label{eq:L^p_norm}
			\norm{x}{p} := \left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}}
		\end{align}
		In the limit $p\to\infty$, we obain the so-called \textit{supremum norm}:
		\begin{align}\label{eq:sup_norm}
			\norm{x}{\infty} := \max_{1\leq j\leq d}{\abs{x_j}}.
		\end{align} 
		In the special case of $p = 2$, we recover the Euclidean norm.
	\end{exmp}
	
	\noindent\textit{Proof}: First, we show that Eq. \eqref{eq:sup_norm} is indeed the limit of Eq. \eqref{eq:L^p_norm}:
	
	\begin{align}
		\norm{x}{\infty} &= \max_{1\leq j\leq d}{\abs{x_j}} \leq \sum_{j=1}^{d}\abs{x_j} \leq d \cdot \norm{x}{\infty}
		\\ \Rightarrow \norm{x}{\infty}^p &= \left(\max_{1\leq j\leq d}{\abs{x_j}}\right)^p = \max_{1\leq j\leq d}{\abs{x_j}^p} \leq \sum_{j=1}^{d}\abs{x_j}^p \leq d \cdot \max_{1\leq j\leq d}{\abs{x_j}^p} = d\norm{x}{\infty}^p
		\\ \Rightarrow \norm{x}{\infty} &\leq \left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}} \leq d^{\frac{1}{p}}\norm{x}{\infty}
		\\ \Rightarrow \lim\limits_{p\to\infty}\norm{x}{\infty} &= \norm{x}{\infty} \leq \lim\limits_{p\to\infty}\left\{\left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}}\right\} \leq \lim\limits_{p\to\infty}\left\{d^{\frac{1}{p}}\norm{x}{\infty}\right\} = \norm{x}{\infty}
		\\ \Rightarrow \lim\limits_{p\to\infty}\left\{\left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}}\right\} &= \norm{x}{\infty}
	\end{align}

	To show the norm property of the $L^p$ norm for $1 \leq p < \infty$, we will explicitly show the fulfilling properties of a norm, cf. Defn. \eqref{defn:normed_space}, for all $x$, $y$, $z\in X$ and $\alpha\in \mathbb{R}$:

	\begin{enumerate}
		\item Positivity: $\norm{x}{p} \geq 0 \ \forall x\in X$,
		\item Definiteness: $\norm{x}{p} = 0\Leftrightarrow x = 0$,
		\item Homogeneity: $$\norm{\alpha x}{p} = \left(\sum_{j=1}^{d}\abs{\alpha x_j}^p\right)^{\frac{1}{p}} = \left(\sum_{j=1}^{d}\abs{\alpha}^p\abs{x_j}^p\right)^{\frac{1}{p}} = \abs{\alpha}\norm{x}{p},$$
		\item Triangle inequality: $$\norm{x + y}{p} = \left(\sum_{j=1}^{d}\abs{x_j + y_j}^p\right)^{\frac{1}{p}} \leq \left(\sum_{j=1}^{d}\abs{x_j}^p + \abs{y_j}^p\right)^{\frac{1}{p}} = \norm{x}{p} + \norm{y}{p}.$$	
	\end{enumerate}
	
	In case of $p = \infty$, we will only show the triangle inequality, since the other properties are trivial to prove:
	
	$$\norm{x + y}{\infty} = \max_{1\leq j\leq d}{\abs{x_j + y_j}} \leq \max_{1\leq j\leq d}\left\{\abs{x_j} + \abs{y_j}\right\} \leq \max_{1\leq j\leq d}\abs{x_j} + \max_{1\leq j\leq d}\abs{y_j} = \norm{x}{\infty} + \norm{y}{\infty},$$
	
	where the last inequality holds since for any $1\leq j \leq d$, it holds that
	\begin{align*}
		&\left(\abs{x_j} \leq \max_{1\leq k\leq d}\abs{x_k}\right) \wedge \left(\abs{y_j} \leq \max_{1\leq k\leq d}\abs{y_k}\right) 
		\\ &\Rightarrow \abs{x_j} + \abs{y_j} \leq \max_{1\leq k\leq d}\abs{x_k} + \max_{1\leq k\leq d}\abs{y_k} 
		\\ &\Rightarrow \max_{1\leq j\leq d}\left\{\abs{x_j} + \abs{y_j}\right\} \leq \max_{1\leq k\leq d}\abs{x_k} + \max_{1\leq k\leq d}\abs{y_k}
	\end{align*}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{theorem}\label{second-triangle-inequality}
		Let $(X, \norm{\cdot}{})$ be a normed space. Then the \enquote{second triangle inequality} holds: 
			\begin{align}
				\abs{\norm{\varphi}{} - \norm{\psi}{}} \leq \norm{\varphi - \psi}{} \quad \forall \varphi, \psi \in X. 
			\end{align}
	\end{theorem}
	\noindent\textit{Proof}: For $\varphi$, $\psi \in X$ we have 
	\begin{align}
		\norm{\varphi}{} = \norm{\varphi - \psi + \psi}{} \leq \norm{\varphi - \psi}{} + \norm{\psi}{} \Leftrightarrow \norm{\varphi}{} - \norm{\psi}{} \leq \norm{\varphi - \psi}{}. 
	\end{align}
	By exchanging the roles of $\varphi$ and $\psi$ we obtain 
	\begin{align}
		\norm{\psi}{} - \norm{\varphi}{} \leq \norm{\varphi - \psi}{}
	\end{align}
	and thus 
	\begin{align}
		\abs{\norm{\varphi}{} - \norm{\psi}{}} \leq \norm{\varphi - \psi}{}. 
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{theorem}\label{norms_continuities}
		Let $(X, \norm{\cdot}{})$ be a normed space. Then the addition, scalar multiplication and the norm itself are continuous. 
	\end{theorem}
	\noindent\textit{Proof}: 
	\begin{itemize}
		\item ad continuity of the addition: Let $(\varphi_n)_{n\in\mathbb{N}}$ and $(\psi_n)_{n\in\mathbb{N}}$ be convergent sequences in $X$ with limit elements $\varphi$, $\psi\in X$, i.e. $\varphi_n \longrightarrow \varphi$ and $\psi_n \longrightarrow \psi$ for $n\to\infty$. Thus 
		\begin{align}
			0\leq \norm{(\varphi_n + \psi_n) - (\varphi + \psi)}{} \leq \norm{\varphi_n - \varphi}{} + \norm{\psi_n - \psi}{} \longrightarrow 0 \quad\text{for } n\to \infty
		\end{align}
		and hence $\varphi_n + \psi_n \rightarrow \varphi + \psi$ for $n\to\infty$. 
		\item ad continuity of the scalar multiplication: Let $\left(\alpha_n\right)_{n\in\mathbb N} \in \mathbb K$ converge to $\alpha\in \mathbb K$ and \\ $\left(\varphi_n\right)_{n\in\mathbb N}\in X \rightarrow \varphi\in X$ for $n\to\infty$. Then 	
		\begin{align}
			0&\leq\norm{\alpha_n\varphi_n-\alpha\varphi}{} = \norm{\alpha_n\left(\varphi_n-\varphi\right) + \left(\alpha_n-\alpha\right)\varphi}{} \leq \norm{\alpha_n(\varphi_n-\varphi)}{} + \norm{\left(\alpha_n - \alpha\right)\varphi}{}
			\\ &\leq \abs{\alpha_n}\norm{\varphi_n-\varphi}{} + \abs{\alpha_n-\alpha}\norm{\varphi}{} \overset{n\to\infty}{\longrightarrow} 0. 
		\end{align}
		This implies $\alpha_n\varphi_n  \rightarrow \alpha\varphi$ for $n\to\infty$. 
		\item ad continuity of the norm: Let $\varphi_n \rightarrow\varphi$. With Theorem \ref{second-triangle-inequality} we have: 
		\begin{align}
			0\leq \abs{\ \norm{\varphi_n}{}-\norm{\varphi}{}\ } \leq \norm{\varphi_n - \varphi}{} \overset{n\to\infty}{\longrightarrow}  
		\end{align}
		and hence $\norm{\varphi_n}{} \rightarrow \norm{\varphi}{}$ for $n\to\infty$. 
	\end{itemize}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	\begin{defn}
		Two norms $\left\vert\left\vert \cdot \right\vert\right\vert_{a}$ and $\left\vert\left\vert \cdot \right\vert\right\vert_{b}$ on a linear space $X$ are said to be equivalent if and only if there exist positive constants $0 < c \leq C < \infty$ such that 
		\begin{align}\label{equivalence_norms}
			c\norm{\varphi}{b} \leq \norm{\varphi}{a} \leq C\norm{\varphi}{b} \quad \forall \varphi \in X. 
		\end{align}	
		(It is also possible to write this as $\tilde{c}\norm{\varphi}{a} \leq \norm{\varphi}{b} \leq \tilde{C}\norm{\varphi}{a}$ with $\tilde{c} := C^{-1}$ and $\tilde{C} := c^{-1}$, where $0 < \tilde{c} \leq \tilde{C}<\infty$.)
	\end{defn}

	\begin{lemma}
		Let $X$ be a linear space and the pairs $\left(\norm{\cdot}{a}, \norm{\cdot}{c}\right)$ and $\left(\norm{\cdot}{b}, \norm{\cdot}{c}\right)$ be equivalent. Then also the pair $\left(\norm{\cdot}{a}, \norm{\cdot}{b}\right)$ is equivalent. 
	\end{lemma}
	\noindent\textit{Proof}: By assumption, we know that 
	\begin{align}
		c\norm{\varphi}{c} &\leq \norm{\varphi}{a} \leq C\norm{\varphi}{c} \quad \forall \varphi\in X \label{equivalent-norms-lemma-proof}
	\end{align}
	and 
	\begin{align}
		d\norm{\varphi}{c} &\leq \norm{\varphi}{b} \leq D\norm{\varphi}{c} \quad \forall \varphi\in X
							\\ 
		\Leftrightarrow \norm{\varphi}{c} &\leq \frac{1}{d}\norm{\varphi}{b} \leq \frac{D}{d}\norm{\varphi}{c}. 
							\\
		\overset{\tiny\eqref{equivalent-norms-lemma-proof}}{\Leftrightarrow} \frac{1}{C}\norm{\varphi}{a} &\leq \norm{\varphi}{c} \leq \frac{1}{d}\norm{\varphi}{b} \leq \frac{D}{d}\norm{\varphi}{c} \leq \frac{D}{d\cdot c}\norm{\varphi}{a}  
							\\ 
		\Leftrightarrow \frac{1}{C}\norm{\varphi}{a} &\leq \frac{1}{d}\norm{\varphi}{b} \leq \frac{D}{d\cdot c}\norm{\varphi}{a}
							\\ 
		\Leftrightarrow \frac{d}{C}\norm{\varphi}{a} &\leq \norm{\varphi}{b} \leq \frac{D}{c}\norm{\varphi}{a}. 
	\end{align}
	It is clear that $0 < dC^{-1} \leq Dc^{-1} < \infty$ holds. \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$

	\begin{theorem}\label{finite_dimensional_norm_equivalence}
		On a \textit{finite-dimensional} space $X$ over a field $\mathbb{K}$ all norms are equivalent. 
	\end{theorem}
	\noindent\textit{Proof \cite[p. 27f.]{werner-fa}}: Let $\dim(X) = n$, $\{e_1, \dots, e_n\}$ be a basis of $X$ and $\norm{\cdot}{}$ a norm on $X$. We can now show that $\norm{\cdot}{}$ is equivalent to the Euclidean norm $\norm{\sum_{i = 1}^n\alpha_ie_i}{2} = \left( \sum_{i = 1}^{n}\left\vert \alpha_i \right\vert^2 \right)^{1/2}$ as follows: \\[6pt] Set $K:= \max\left\{ \norm{e_1}{}, \dots, \norm{e_n}{}\right\} > 0$. Then from the triangle inequality for $\norm{\cdot}{}$ we have: 
	\begin{align}\label{equivalence-of-norms}
		\norm{x}{} = \norm{\sum_{i= 1}^{n}\alpha_ie_i}{} \leq \sum_{i= 1}^n \norm{\alpha_ie_i}{} = \sum_{i= 1}^n \abs{\alpha_i} \norm{e_i}{}
	\end{align}
	Since $(\abs{\alpha_1}, \dots, \abs{\alpha_n})^T$, $(\norm{e_1}, \dots, \norm{e_n}{})^T\in \mathbb R^n$ and 
	\begin{align}
		\left\langle \left(\abs{\alpha_1}, \dots, \abs{\alpha_n}\right), (\norm{e_1}, \dots, \norm{e_n}{} ) \right\rangle = \sum_{i = 1}^n \abs{\alpha_i}\norm{e_i}{}
	\end{align}
	we can use the Cauchy-Schwarz inequality: 
	\begin{align}
		\langle (\abs{\alpha_1}, \dots, \abs{\alpha_n}), (\norm{e_1}, \dots, \norm{e_n}{} ) \rangle &\leq \norm{\sum_{i=1}^n \alpha_ie_i}{2}\cdot \norm{\sum_{i= 1}^n e_i}{2} = \sqrt{\sum_{i = 1}^n\abs{\alpha_i}^2} \cdot \sqrt{\sum_{i = 1}^n\norm{e_i}{}^2}
		\\[8pt] &= \norm{x}{2}\cdot \sqrt{\sum_{i=1}^n K^2} = K\sqrt{n}\norm{x}{2} \quad \forall x\in X, 
	\end{align}
	where in the last line we used that $K = \max\{\norm{e_1}, \dots, \norm{e_n}{}\}$ and $x = \sum_{i = 1}^n \alpha_i e_i$. Putting the last Eq. into Eq. \eqref{equivalence-of-norms}, we have: 
	\begin{align}\label{fa_equiv_norms_intermed_1}
		\norm{x}{} \leq \sum_{i=1}^{n} \abs{\alpha_i}\norm{e_i}{} \leq K\sqrt{n}\norm{x}{2} \quad \forall x\in X.
	\end{align}
	Now define the set 
	\begin{align}
		S:= \{ x\in X \mid \norm{x}{2} = 1 \}. 
	\end{align}
	This set is closed since it is the preimage of the closed set $\{1\}\subset \mathbb R$ under the continuous function $\norm{\cdot}{2}$, cf. Theorem \ref{norms_continuities}, \ref{defn:preimages_continuous_functions}. $S$ is also closed since $S\subset B_{r}(0) = \{\psi\in X\mid \norm{\psi}{2} < r\}$ for $r>0$ (here, we take into account that every norm induces a metric). Thus, $S$ is compact according to Heine-Borel (which applies to every finite-dimensional normed vector space). Since every continuous function takes its minimum on a compact set, we know that $\norm{\cdot}{}$ has a minimum $m > 0$ on $S$. Since $x\cdot \norm{x}{2}^{-1}\in S$ for $x\ne 0$, we have ($m$ is the minimum of the function $\norm{\cdot}{}$): 
	\begin{align}
		m\norm{x}{2} \leq \norm{x}{} \quad \forall x\in X.
	\end{align}
	All in all, we proved: 
	\begin{align}
		m\norm{x}{2} \leq \norm{x}{} \leq K\sqrt{n}\norm{x}{2} \quad \forall x\in X. 
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}\label{defn:strong_equivalence}
		Let $X$ be a linear space equipped with two metrics $d$ and $d'$. Then the metrics are \textit{strongly equivalent} if and only if there exist positive constants $0 < \alpha \leq \beta < \infty$ such that 
		\begin{align}
			\alpha d(x, y) \leq d'(x, y) \leq \beta d(x, y) \quad\forall x, y\in X. 
		\end{align}
		\cite{equivalence-metrics}
	\end{defn}
	
	\begin{remark}\label{equivalence_metrics_finite_dimensional}
		Obviously, Eq. \eqref{equivalence_norms} can also be written as 
		\begin{align}
			c\norm{\varphi-\psi}{b} \leq \norm{\varphi-\psi}{a} \leq C\norm{\varphi-\psi}{b} \quad \forall \varphi, \psi\in X. 
		\end{align} 
		Thus, also Theorem \ref{finite_dimensional_norm_equivalence} holds for metrics as well: In a finite-dimensional space $X$, all metrics are strongly equivalent. 
	\end{remark}
	
	\newpage 
	\section{Topology}
	\begin{defn}[Metric Space \cite{fa2019}]\label{defn:metric_space}
		Let $X$ be a non-empty set. Then the map 
		\begin{align*}
			d: X\times X\rightarrow \mathbb R
		\end{align*}
		is called a \textit{metric} on $X$ if for all $\varphi$, $\psi$, $\chi\in X$ the following properties are given: 
		\begin{itemize}
			\item $d(\varphi, \psi) \geq 0$, 
			\item $d(\varphi, \psi) = d(\psi, \varphi)$, 
			\item $d(\varphi, \varphi) = 0$, 
			\item $d(\varphi, \psi) \leq d(\varphi, \chi) + d(\chi, \psi)$. 
		\end{itemize}
	\end{defn}

	\begin{defn}[Topological Space \cite{topology-singh}]
		A topology on a set $X$ is a collection $\tau$ of subsets of $X$ such that 
		\begin{itemize}
			\item the intersection of two members of $\tau$ is in $\tau$, 
			\item the union of any collection of members of $\tau$ is in $\tau$, 
			\item the empty set $\{\}$ and $X$ itself are in $\tau$.
		\end{itemize}
		A set $X$ endowed with a topological structure $\tau$ on it is called a \textit{topological space}. The elements of $X$ are called \textit{points}, and the members of $\tau$ are called the \textit{open sets}. 
	\end{defn} 
	
	\begin{remark}[Discrete Metric]\label{remark:discrete-metric}
		Let $X$ be an arbitrary set and $d(\varphi, \psi) = 1 \ \forall \varphi$, $\psi\in X$ and $d(\varphi, \varphi) = 0$. Then $d$ is a metric, called the \textit{discrete metric} on $X$. 
	\end{remark}
	
	\begin{defn}[Open Ball]
		Let $(X, d)$ be a metric space, $\varphi\in X$ and $r > 0$. Then 
		\begin{align}
			B_{r}(\varphi) := \left\{ \psi\in X \vert d(\varphi, \psi) < r\right\} \subset X
		\end{align}
		is called an \textit{open ball} in $X$ with middle point $\varphi$ and radius $r$. 
 	\end{defn}
 
 	\begin{exmp}\label{exmp:open-balls-discrete-metric}
 		For the discrete metric $d$ according to Remark \ref{remark:discrete-metric}, the open balls can be characterized as follows:
 		\begin{align}
 			B_{r}(\varphi) = \begin{cases}
 				\{\varphi\}, &r \leq 1 
 				\\ X, &r > 1
 			\end{cases}
 		\end{align}
 	\end{exmp} 
	
	 \begin{defn}[Closed ball]
		Let $(X, d)$ be a metric space, $\varphi\in X$ and $r > 0$. Then 
		\begin{align}
			B_{r}[\varphi] := \left\{ \psi\in X \vert d(\varphi, \psi) \leq r\right\} \subset X
		\end{align}
		is called a \textit{closed ball} in $X$ with middle point $\varphi$ and radius $r$. 
	\end{defn}
	
	\begin{defn}[Open Set]
		\label{defn-open-set}
		Let $(X, d)$ be a metric space. Then a subset $U\subset X$ is called \textit{open} in $X$ if for every $\varphi \in U$ there is an open ball $B_{r}(\varphi)$ that is contained in $U$, i.e. $B_r(\varphi)\subset U$. 
	\end{defn}

	\begin{defn}[Closed Set]
		Let $(X, d)$ be a metric space. Then a subset $A\subset X$ is called \textit{closed} in $X$ if the complement $X\backslash A$ is open according to Definition \ref{defn-open-set}. 
	\end{defn}

	\begin{theorem}\label{open-balls-open}
		Open balls are open. 
	\end{theorem}
	\noindent\textit{Proof:}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.35\textwidth]{Figures/open-balls-open.png}
	\end{figure}
	An illustration is shown in Figure. Let $f\in X$ and for $r > 0$ consider the open ball $U:= B_{r}(f) \subset X$. By definition, for every $\varphi \in U$ it holds that $d\left(\varphi, f\right) < r$. Now we show that 
	\begin{align}
		B_{r-d\left(\varphi, f\right)}(\varphi) \subset U.
	\end{align}
	For this consider $\psi\in B_{r-d\left(\varphi, f\right)}(\varphi)$, i.e. $d(\psi, \varphi) < r - d(\varphi, f)$. With the triangle inequality we obtain: 
	\begin{align}
		d(f, \psi) \leq d(f, \varphi) + d(\varphi, \psi) < d(f, \varphi) + r - d(\varphi, f) = r,  		
	\end{align}
	i.e. $d(f, \psi) < r$ and thus $\psi\in U = B_{r}(f)$.  \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{theorem}\label{thrm:closed_balls_open}
		Closed balls are closed.
	\end{theorem}
	
	\begin{proof}
		Let $f\in X$ and for $r > 0$ consider the closed ball $B_{r}[f]\subset X$. By definition, for every $\varphi \in X\backslash B_{r}[f]\subset X$, it holds that $d(\varphi, f) > r$. We will now show that 
		\begin{align}
			B_{d(\varphi, f) - r}(\varphi) \subset X \backslash B_{r}[f],
		\end{align}
		which would prove that $B_{r}[f]$ is closed. Let $\psi\in B_{d(\varphi, f) - r}(\varphi)$, i.e. $d(\psi, \varphi) < d(\varphi, f) -r$. With the triangle inequality, we obtain:
		\begin{align}
			d(f, \varphi) \leq d(f, \psi) + d(\psi, \varphi) \Leftrightarrow d(f, \psi) \geq d(f, \varphi) - d(\psi, \varphi) > d(f, \phi) - d(\varphi, f) + r = r, 
		\end{align}
		i.e. $d(f, \psi) > r$, and hence $\psi\notin B_r[f]$, implying that $\psi\in X\backslash B_{r}[f]$. 
	\end{proof}
	
	\begin{theorem}
		Let $(X, d)$ be a metric space. The collection of open sets as in Definition \ref{defn-open-set} gives a topology.
	\end{theorem}
	\begin{proof}		
		According to Defn. \ref{defn:metric_space}, we need to prove that $X$ and $\emptyset$ are open, that the union of an arbitrary number of open sets is open, and that the intersection of a finite number of open sets is open. 
		
		Clearly, $X$ is open (since for any choice of $r > 0$ and $\varphi\in X$, $B_{r}(\varphi)\subset X$). $\emptyset$ is also open, since nothing needs to be shown.
		
		Now let $U_{1}$, $\dots$, $U_{n}\subset X$ be open, and consider the intersection
		\begin{align}
			U := \bigcap_{i=1}^{n}U_{i} \subset X.
		\end{align} 
		For any $\varphi\in U$, we know that $\varphi\in U_i$ for all $1\leq i \leq n$. Since the $U_{i}$ are open, there is a radius $r_{i} > 0$ such that $B_{r_i}(\varphi) \subset U_i$ for all $1\leq i \leq n$. Defining $r := \min_{1 \leq i \leq n}\{r_{i}\} > 0$, we have that $B_{r}(\varphi) \subset U_i$ for all $1\leq i\leq n$ and hence $B_{r}(\varphi)\subset U$.
		
		Finally, let $U_{i}$, $i\in I$, be open, where $I$ is in index set. Consider the union
		\begin{align}
			U := \bigcup_{i\in I}U_{i} \subset X.
		\end{align}
		For any $\varphi\in U$ there is an index $i\in I$ such that $\varphi\in U_i$. Since $U_i$ is open, there is an $r > 0$ such that $B_{r}(\varphi) \subset U_i \subset U$.
	\end{proof}

	\begin{defn}[Convergence of sequence \cite{fa2019}]\label{defn:convergence_sequence}
		Let $(X, d)$ be a metric space. A sequence $(\varphi_n)_{n\in\mathbb N}$ with elements $\varphi_n\in X$, $n\in\mathbb N$, is called \textit{convergent} if there is a $\varphi\in X$ s.t.
		\begin{align}
			\lim\limits_{n\to\infty}d(\varphi_n, \varphi) = 0,
		\end{align}
		i.e. for all $\epsilon > 0$ there exists an $N(\epsilon)\in\mathbb N$ s.t.
		\begin{align}
			d(\varphi_n, \varphi) < \epsilon \quad \forall n\geq N(\epsilon).
		\end{align}
		$\varphi$ is called the \textit{limit} of the sequence $(\varphi_n)_{n\in \mathbb N}$. For this, we write 
		\begin{align}
			\lim\limits_{n\to\infty}\varphi_n = \varphi \quad \text{or}\quad \varphi_n \overset{n \to\infty}{\longrightarrow} \varphi.
		\end{align}
		Non-convergent sequence are called \textit{divergent}.
	\end{defn}

	\begin{defn}[Accumulation point]\label{defn:accumulation_point}
		Let $U\subset X$. Then $\varphi\in X$ is called an \textit{accumulation point} of $U$ if there is a sequence $\left(\varphi_{n}\right)_{n\in \mathbb N}$ in $U$ such that $\lim\limits_{n\to\infty}\varphi_{n} = \varphi$.
	\end{defn}

	\begin{theorem}\label{thm:closed_set_acc_point}
		A subset $U \subset X$ is closed if and only if it contains all its accumulation points according to Defn. \ref{defn:accumulation_point}.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longleftarrow$}: Let $U$ contain all its accumulation points, and let $\varphi\in X\backslash U$. We will now show that there is an $n\in \mathbb N$ such that $B_{\frac{1}{n}}(\varphi) \subset X\backslash U$, proving that $X\backslash U$ is open. Assuming there is no such $n\in \mathbb N$, then $B_{\frac{1}{n}}(\varphi) \cap U = \emptyset$ for all $n\in \mathbb N$. This means that there is a sequence $(\varphi_{n})_{n\in\mathbb N}$ in $U$ such that $d(\varphi, \varphi_n) < \frac{1}{n}$ for all $n\in\mathbb N$. Hence, $\varphi$ is an accumulation point of $U$, and since $U$ contains all its accumulation points by assumption, it follows that $\varphi\in U$, contradicting the assumption that $\varphi\in X\backslash U$.
		
		\enquote{$\Longrightarrow$}: Let $U$ be closed, i.e. $X\backslash U$ is open. Further, let $\varphi\in X$ be an accumulation point of $U$, i.e. there is a sequence $(\varphi_n)_{n\in\mathbb N}$ in $U$ such that $\lim\limits_{n\to\infty}\varphi_n = \varphi$. Hence, by definition of convergence, for all $\epsilon > 0$ there exists an $n\in\mathbb N$ such that $\varphi_n\in B(\varphi, \epsilon) \cap U$, implying that $\varphi\in U$. Thus, $U$ contains all its accumulation points.
	\end{proof}
	
	\begin{defn}[Closure of set]\label{defn:closure_set}
		The set 
		\begin{align}
			\bar{U} := \{\varphi\in X\mid \varphi\ \text{is an accumulation point of}\ U\} \subset X
		\end{align}
		is called the closure of the set $U \subset X$.
	\end{defn}

	\begin{remark}\label{remark:closure_superset}
		Clearly, $U\subset \bar{U}$, since any $\varphi\in U$ is an accumulation point of $U$ (consider the constant sequence, where for any $n\in\mathbb N$, $\varphi_n := \varphi$).
	\end{remark}

	\begin{remark}
		According to Theorem \ref{thm:closed_set_acc_point}, a subset $U\subset X$ is closed if and only if it contains all its limit points, thus for a closed subset we have $\overline{U} \subset U$. With Remark \ref{remark:closure_superset}, we get $U$ is closed if and only if $U = \overline{U}$.
	\end{remark}
	
	\begin{theorem}\label{thrm:closure_closed}
		Let $U\subset X$. Then its closure, $\bar{U}$, is closed.
	\end{theorem}

	\begin{proof}
		According to Theorem \ref{thm:closed_set_acc_point}, we need to prove that $\bar{U}$ contains all its limit points. Let $\varphi\in X$ be an accumulation point of $\overline{U}$. Then for all $n\in \mathbb N$ there is a $\varphi_n\in\overline{U}$ such that $d(\varphi, \varphi_n) < 1/n$. By Def. \ref{defn:closure_set}, we also know that since $\varphi_n\in \bar{U}$, it is an accumulation point of $U$, i.e. for every $\varphi_n\in \overline{U}$ there is a $\psi_n\in U$ such that $d(\varphi_n, \psi_n) < 1/n$. Thus:
		\begin{align}
			d(\psi_n, \varphi) \leq d(\psi_n, \varphi_n) + d(\varphi_n, \varphi) < \frac{1}{n} + \frac{1}{n} \overset{n\to\infty}{\longrightarrow} 0,
		\end{align}
		i.e. $(\psi_n)_{n\in\mathbb N}$ is a sequence in $U$ that converges to $\varphi$, i.e. $\varphi$ is an accumulation point of $U$, hence $\varphi\in \bar{U}$.
	\end{proof}
	
	\begin{theorem}\label{thrm:property_metric_space}
		Let $(X, d)$ be a metric space. Then $\varphi\in \overline{U}$ for $U\subset X$ if and only if for every open set $O\subset X$ containing $\varphi$, $U\cap O\ne \emptyset$.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longrightarrow$} Since $\varphi\in\overline{U}$, there exists a sequence $\left(\varphi_n\right)_{n\in\mathbb N}$ converging to $\varphi$ wrt $d$, i.e. for all $\epsilon > 0$ there exists an $N\in\mathbb N$ s.t. for all $n\geq N$, $d(\varphi_n, \varphi) < \epsilon$. Let $O\subset X$ be open, and let $\varphi\in O$. By definition, there exists an open ball $B_{r}(\varphi)\subset O$ with $r > 0$. Choose $\epsilon = r$ and $\psi = x_{N}\in U$, then $\psi\in B_{r}(\varphi) \subset O$.
		
		\enquote{$\Longleftarrow$} By assumption, for any non-empty open set $O\subset X$, $U\cap O\ne\emptyset$, where $U\subset X$. Let $\varphi\in X$. Since open balls are open, for any $\epsilon > 0$, $U\cap B_{\epsilon}(\varphi)\ne\emptyset$. Consider the sequence of open balls $\left( B_{1/n}(\varphi)\right)_{n\in\mathbb N}$, for which $U\cap B_{\frac{1}{n}}(\varphi) \ne \emptyset$ holds. Thus for each $n\in\mathbb N$, there exists a point $\varphi_{n}\in X$ s.t. $\varphi_n \in B_{\frac{1}{n}}(\varphi)$ and $\varphi_n\in U$. Hence, by definition, $\varphi\in X$ is an accumulation point of $U$.
	\end{proof}
	
	\begin{theorem}
		Let $U\subset X$. Then its closure, $\overline{U}$, is the \textit{smallest} closed set in $X$ containing $U$.
	\end{theorem}

	\begin{proof}
		From Remark \ref{remark:closure_superset}, we know that $U\subset \overline{U}$, and from Theorem \ref{thrm:closure_closed}, we know that $\overline{U}$ is closed. To prove that $\overline{U}$ is the smallest \textit{closed} set in $X$ containing $U$, let $V\subset X$ be another closed set that contains $U$, i.e. $U\subset V$. Every sequence in $U$ is also a sequence in $V$, i.e. every accumulation point of $U$ is in $V$, i.e. $\overline{U}\subset V$.
	\end{proof}

	\begin{defn}[Topological equivalence \cite{topology-singh}]
		Two metrics $d$ and $d'$ on a set $X$ are called \textit{equivalent} if they induce the same topology on $X$, i.e. $\tau_d = \tau_{d'}$.
	\end{defn}

	\begin{theorem}\label{thrm:topological_equivalence}
		Two metrics $d$ and $d'$ on a set $X$ are equivalent if and only if for each $x\in X$ and for each $\epsilon > 0$, there exist real numbers $\delta_1$, $\delta_2 > 0$ (depending upon $x$ and $\epsilon$) such that $B^{d}_{\delta_1}(x) \subset B^{d'}_{\epsilon}(x)$ and $B^{d'}_{\delta_2}(x) \subset B^{d}_{\epsilon}(x)$. \cite{topology-singh}.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longrightarrow$} For each $x\in X$ and for each $\epsilon > 0$, the open ball $B^{d}_{\epsilon}(x)$ in the metric space $(X, d)$ is open in the topology $\tau_{d'}$, and the open ball $B^{d'}_{\epsilon}(x)$ in the metric space $(X, d')$ is open in the topology $\tau_{d}$. Hence, there exist $\delta_1$, $\delta_2$ such that $B^{d'}_{\delta_1}(x) \subset B^{d}_{\epsilon}(x)$ and $B^{d}_{\delta_2}(x) \subset B^{d'}_{\epsilon}(x)$.
		
		\enquote{$\Longleftarrow$} Suppose that $B^{d'}_{\delta_1}(x) \subset B^{d}_{\epsilon}(x)$ and $B^{d}_{\delta_2}(x) \subset B^{d'}_{\epsilon}(x)$ hold for each $x\in X$ and $\epsilon > 0$. Given an open set $U\subset X$ in the topology $\tau_d$ and a point $x\in U$, there exists an open ball $B^{d}_{r}(x) \subset U$ with $r > 0$. By assumption, there exists an open ball $B^{d'}_{\delta_1}(x) \subset B^{d}_{r}(x) \subset U$, proving that $U$ is an open set in the topology $\tau_{d'}$. Similarly, one can show that every open set in the topology $\tau_{d'}$ is also open in the topology $\tau_{d}$. Hence, $\tau_{d'} = \tau_{d}$.
	\end{proof}

	\begin{theorem}
		If two metrics $d$ and $d'$ on a set $X$ are strongly equivalent, cf. Def. \ref{defn:strong_equivalence}, then they are also (topologically) equivalent.
	\end{theorem}

	\begin{proof}\cite{1379634}
		By definition of strong equivalence, we know that 
		\begin{align}
			\alpha d(x, y) \leq d'(x, y) \leq \beta d(x, y) \quad\forall x, y\in X, \alpha, \beta > 0.
		\end{align}
		According to Theorem \ref{thrm:topological_equivalence}, we need to show that for all $x\in X$ and $\epsilon > 0$, there exist $\delta_1$, $\delta_2 > 0$ such that 
		\begin{align}
			\left(B_{\delta_1}^{d}(x) \subset B_{\epsilon}^{d'}(x)\right) \wedge \left(B^{d'}_{\delta_2}(x) \subset B^{d}_{\epsilon}(x)\right)
		\end{align}
		To show the first inclusion, let $\delta_1 \leq \epsilon/\beta$, then we have for all $x$, $y\in X$:
		\begin{align}
			\frac{d'(x, y)}{\beta} \leq d(x, y) < \delta_1 \leq \frac{\epsilon}{\beta} \Rightarrow d'(x, y) < \epsilon.
		\end{align}
		To show the second inclusion, let $\delta_2 \leq \epsilon\alpha$, then we have for all $x$, $y\in X$:
		\begin{align}
			\alpha d(x, y)\leq d'(x, y) < \delta_2 \leq \epsilon\alpha \Rightarrow d(x, y) < \epsilon.
		\end{align}
	\end{proof}

	\begin{remark}
		There exist metrics $d$ and $d'$ on a set $X$ that are (topologically) equivalent, but not strongly equivalent. \cite{5065146}
	\end{remark}

	\begin{exmp}
		Let $X := (0, 1]$, and for any $x$, $y\in X$, define $d(x, y) := \abs{x - y}$ and \newline $d'(x, y) := \abs{x^{-1} - y^{-1}}$. Clearly, $d$ and $d'$ define metrics on $X$, and they are not strongly equivalent, since there is no $\beta > 0$ s.t. $d'(x, y) \leq d(x, y)$ for all $x$, $y\in X$. 
		
		However, $d$ and $d'$ are (topologically) equivalent. To see this, remember that acc. to \mbox{Theorem \ref{thrm:topological_equivalence}}, we need to prove that for any $x\in X$ and for any $\epsilon > 0$, there exist $\delta_{1}(x, \epsilon)$ and $\delta_2(x, \epsilon)$ s.t. $B_{\delta_1}^{d}(x) \subset B_{\epsilon}^{d'}(x)$ and $B_{\delta_2}^{d'}(x) \subset B_{\epsilon}^{d}(x)$. 
		
		To show the first inclusion, choose any
		\begin{align}\label{eq:top-eq-not-imply-strong-eq-delta_1}
			\delta_{1} < \frac{\epsilon x^2}{\epsilon x + 1}.
		\end{align}
		Now we have 
		\begin{align}\label{eq:top-eq-not-imply-strong-eq-delta_1-2}
			y \in B_{\delta_1}^{d}(x) \Rightarrow \abs{y - x} < \delta_1 \Rightarrow \frac{1}{xy}\abs{x - y} = \abs{\frac{1}{x} - \frac{1}{y}} < \frac{\delta_1}{xy}
		\end{align}
		Since $z \leq \abs{z}$ for any $z\in\mathbb R$, 
		\begin{align}\label{eq:top-eq-not-imply-strong-eq-delta_1-3}
			y \in B_{\delta_1}^{d}(x) \Rightarrow x - y \leq \abs{x-y} < \delta_1 \Rightarrow x - \delta_1 < y\Rightarrow \frac{1}{y} < \frac{1}{x - \delta_1},
		\end{align}
		and hence 
		\begin{align}
			y \in B_{\delta_1}^{d}(x) \overset{\tiny \eqref{eq:top-eq-not-imply-strong-eq-delta_1-2}}{\Rightarrow} \abs{\frac{1}{x} - \frac{1}{y}} &< \frac{\delta_1}{xy} \overset{\tiny \eqref{eq:top-eq-not-imply-strong-eq-delta_1-3}}{<} \frac{\delta_1}{x(x - \delta_1)} = \frac{\delta_1}{x^2 - x\delta_1} \overset{\tiny \eqref{eq:top-eq-not-imply-strong-eq-delta_1}}{<} \frac{\epsilon x^2}{\left(\epsilon x + 1\right)\left(x^2 - \frac{\epsilon x^3}{\epsilon x + 1}\right)} 
			\\ &= \frac{\epsilon x^2}{\epsilon x^3 - \frac{\epsilon^2 x^4}{\epsilon x + 1} + x^2 - \frac{\epsilon x^3}{\epsilon x + 1}} = \frac{\epsilon x^2}{\frac{\epsilon^2 x^4 + \epsilon x^3}{\epsilon x + 1} - \frac{\epsilon^2 x^4}{\epsilon x + 1} + x^2 - \frac{\epsilon x^3}{\epsilon x + 1}} = \epsilon.
		\end{align}
		
		To show the second inclusion, i.e. $B_{\delta_2}^{d'}(x) \subset B_{\epsilon}^{d}(x)$, choose $\delta_2 := \epsilon$, then we have:
		\begin{align}
			y \in B_{\delta_2}^{d'}(x) \Rightarrow \abs{\frac{1}{x} - \frac{1}{y}} < \delta_2 \Rightarrow xy\abs{\frac{1}{x} - \frac{1}{y}} = \abs{y - x} < \delta_2 xy \overset{x, y\in X = (0, 1]}{\leq} \delta_2 = \epsilon.
		\end{align}
	\end{exmp}

	\begin{theorem}
		Two metrics $d$ and $d'$ on a set $X$ are (topologically) equivalent iff each convergent sequence in $X$ wrt $d$ is also convergent in $X$ wrt $d'$. In that case, their limits coincide.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longrightarrow$} Let $d$ and $d'$ be equivalent metrics on $X$, and let $(\varphi_n)_{n\in\mathbb N}$ be a sequence in $X$, which converges to $\varphi\in X$ wrt $d$, i.e.
		\begin{align}
			d(\varphi, \varphi_n) \overset{n\to\infty}{\longrightarrow} 0.
		\end{align}
		According to Theorem \ref{thrm:topological_equivalence}, for any $\epsilon > 0$, there is a $\delta > 0$ s.t. $B_{\delta}^{d}(\varphi) \subset B_{\epsilon}^{d'}(\varphi)$. To this $\delta$, there exists an $N\in\mathbb N$ s.t. $d(\varphi, \varphi_n) < \delta$ for all $n\geq N$, cf. Def. \ref{defn:convergence_sequence}. Hence:
		\begin{align}
			\varphi_n\in B^{d}_{\delta}(\varphi) \subset B^{d'}_{\epsilon}(\varphi), \quad \forall n\geq N
		\end{align}
		i.e. $d'(\varphi, \varphi_n) < \epsilon$ for any $\epsilon > 0$.
		
		\enquote{$\Longleftarrow$} Let $U\subset X$ be $d$-open in $X$, i.e. open wrt $d$. Then $X\backslash U$ is $d$-closed in $X$. Let $\varphi\in X$ be a $d'$-accumulation point of $X\backslash U$, i.e. there exists a sequence $(\varphi_n)_{n\in\mathbb N}$ in $X\backslash U$ s.t. $\varphi_n \overset{d'}{\longrightarrow} \varphi$ for $n\to\infty$. By assumption, $(\varphi_n)_{n\in\mathbb N}$ converges to a $\psi\in X$ wrt $d$. Since $X\backslash U$ is closed, by Theorem \ref{thm:closed_set_acc_point}, $\psi\in X\backslash U$ holds. 
		
		Let us now show that $\varphi = \psi$. For this, consider the extended sequence 
		\begin{align}
			\left(\varphi_1, \varphi, \varphi_2, \varphi, \varphi_3, \varphi, \dots\right).
		\end{align}
		The extended sequence converges to $\varphi$ wrt $d'$, and hence it also converges wrt $d$. The subsequence $(\varphi_n)_{n\in\mathbb N}$ of the extended sequence converges to $\psi$ wrt $d$. Hence, $\varphi = \psi$, and the $d'$-accumulation point $\varphi=\psi$ of $X\backslash U$ lies in $X\backslash U$, i.e. $X\backslash U$ is also $d'$-closed (by Theorem \ref{thm:closed_set_acc_point}). This in turn implies that $U$ is $d'$-open.
		
		We have shown that $d$ and $d'$ induce the same topology on $X$, and are hence equivalent.
	\end{proof}

	\begin{defn}[Bounded set]\label{defn:bounded_set}
		A subset $U\subset X$ is called \textit{bounded} in $X$ if it is contained in a closed ball in $X$, i.e. there exists $r > 0$ and $\varphi \in X$ s.t. $U\subset B_{r}[\varphi]$.
	\end{defn}

	\begin{theorem}
		Convergent sequences in $X$ are fully contained in a bounded subset.
	\end{theorem}
	
	\begin{proof}
		Let $(\varphi_n)_{n\in\mathbb N}$ be a convergent sequence in $X$ with limit point $\varphi\in X$. Then there exists an $N\in\mathbb N$ s.t. $d(\varphi_n, \varphi) < \epsilon$ for all $n\geq N$ for a fixed $\epsilon > 0$. Defining
		\begin{align}
			r := \max \left\{ \epsilon, \max_{n \leq N}\{d(\varphi, \varphi_n)\} \right\},
		\end{align}
		we have $d(\varphi, \varphi_n) \leq r$ for all $n\in\mathbb N$. Hence, for any $n\in\mathbb N$, 
		$\varphi_n\in B_{r}[\varphi]$.¸
	\end{proof}
	
	\begin{defn}[Isometry]
		Let $(X, d)$ and $(X', d')$ be metric spaces. Then $f: X \rightarrow Y$ is called a distance-preserving map (or \textit{isometry}) if
		\begin{align}
			d'(f(\varphi), f(\psi)) = d(\varphi, \psi) \quad\forall \varphi, \psi\in X
		\end{align}
		$(X, d)$ and $(X', d')$ are called \textit{isometric} if there is a surjective isometry $f:X\rightarrow X'$.
	\end{defn}

	\begin{theorem}\label{thrm:isometries_injective}
		Let $(X, d)$ and $(X', d')$ be metric spaces and $f: X\rightarrow X'$  be an isometry. Then it is injective.
	\end{theorem}

	\begin{proof}
		If $f(\varphi) = f(\psi)$ for any $\varphi$, $\psi\in X$, then 
		\begin{align}
			d'(f(\varphi), f(\psi)) = d'(f(\varphi), f(\varphi)) = d(\varphi, \psi) = 0, 
		\end{align}
		i.e. $\varphi = \psi$.
	\end{proof}

	\begin{remark}
		Two isometric spaces $(X, d)$ and $(X', d')$ do not differ in their properties wrt their metrics, but instead in the properties of their individual elements. Any metric property holding in one metric space also holds in the other. Hence, in this sense, we say that $(X, d)$ and $(X', d')$ are \textit{identical}.
	\end{remark}
	
	\begin{remark}
		Surjective isometries are hence bijective.
	\end{remark}
	
	\subsection{Complete metric spaces}
	
	\begin{defn}
		A sequence $(\varphi_n)_{n\in\mathbb N}$ in a metric space $(X, d)$ is a \textit{Cauchy sequence} if 
		\begin{align}
			\lim\limits_{n, m\to\infty} d(\varphi_n, \varphi_m) = 0,
		\end{align}
		i.e. for all $\epsilon > 0$ there exists an $N(\epsilon)\in \mathbb N$ s.t. 
		\begin{align}
			d(\varphi_n, \varphi_m) < \epsilon \quad\forall n, m\geq N.
		\end{align}
	\end{defn}

	\begin{theorem}\label{thrm:convergent_sequence_cauchy_sequence}
		Every convergent sequence in $X$ is a cauchy sequence in $X$.
	\end{theorem}

	\begin{proof}
		Let $(\varphi_n)_{n\in\mathbb N}$ be a convergent sequence in $X$ with limit point $\varphi\in X$. Hence, for all $\epsilon > 0$, there is an $N(\epsilon)\in\mathbb N$ with $d(\varphi_n, \varphi) < \epsilon$ for all $n\geq N(\epsilon)$. Hence
		\begin{align}
			d(\varphi_n, \varphi_m) \leq d(\varphi_n, \varphi) + d(\varphi, \varphi_m) < 2\epsilon \quad\forall n,m\geq N.
		\end{align}
	\end{proof}

	\begin{remark}\label{remark:cauchy_not_convergent_necess}
		The converse of Theorem \ref{thrm:convergent_sequence_cauchy_sequence} is in general not true: Take $X=\mathbb Q$ and let $d(x, y) := \abs{x - y}$, for any $x$, $y\in\mathbb Q$. Then we have a metric space $(\mathbb Q, d)$, but the sequence $(a_n)_{n\in\mathbb N}$ with
		\begin{align}\label{eq:seq_euler}
			a_n := \left(1 + \frac{1}{n}\right)^n,
		\end{align}
		which is a Cauchy sequence and where all elements are in $\mathbb Q$, does not converge in $\mathbb Q$ (the limit point is Euler's number $e$). To prove this, we will make use of the following two Theorems.
	\end{remark}
	
	\begin{theorem}\label{thrm:mono_inc_seq_Cauchy}
		Any sequence $(a_n)_{n\in\mathbb N}$ with $a_n\in \mathbb R$ for any $n\in \mathbb N$ that is monotonically increasing and upper bounded for any $n\in\mathbb N$ is a Cauchy sequence in ($\mathbb R$, $d$) with $d(x, y):= \abs{x - y}$ for any \newline $x$, $y\in \mathbb R$.		
	\end{theorem}

	\begin{proof}(\cite{2169936})
		The completeness axiom states that any non-empty subset of $\mathbb R$ that has an upper bound has a least upper bound in $\mathbb R$. Let $M$ be the least upper bound for the sequence $(a_n)_{n\in\mathbb N}$, i.e. $a_n \leq M$ for all $n\in\mathbb N$. Since $M$ is the least upper bound, for all $\epsilon > 0$ there is an $N\in\mathbb N$ s.t. $M - \epsilon < a_N \leq M$.
		
		By assumption, the sequence is monotonically increasing, i.e. $a_m \geq a_n$ for any $m > n$. Without loss of generality, let $m > n$, then we have
		\begin{align}\label{eq:mono_inc_seq_Cauchy}
			M - \epsilon < a_N \leq a_n \leq a_m \leq M, \quad \forall (m, n\geq N) \wedge (m > n)
		\end{align}
		from which $a_m - a_n < \epsilon$ follows. Similarly, we can show that $a_n - a_m < \epsilon$ for $n > m$. Hence, $d(a_m, a_n) < \epsilon$ for any $m$, $n\geq N$.
	\end{proof}
	
	\begin{theorem}\label{thrm:AM-GM}
		Let $x_1$, \dots, $x_n\in\mathbb R_{\geq 0}$, $n\geq 1$, then the arithmetic mean is bigger than or equal to the geometric mean, i.e.
		\begin{align}\label{eq:AM-GM}
			\frac{\sum_{i=1}^{n} x_i}{n} \geq \left(\prod_{i=1}^{n}x_i\right)^{1/n}
		\end{align}
	\end{theorem}
	
	\begin{proof}(\cite{Wiki:AM-GM})
		Via induction. Let $n=1$, then $x_1 \geq x_1$, which is true. 
		
		Assuming that Eq. \eqref{eq:AM-GM} holds for any $n\in\mathbb N$, we will make the transition $n\mapsto n + 1$ and denote the arithmetic mean of $x_1$, \dots, $x_{n + 1}$ by $\alpha$:
		\begin{align}\label{eq:AM-GM-proof-1}
			(n + 1) \alpha = \sum_{i=1}^{n + 1} x_i \Leftrightarrow n\alpha = \sum_{i=1}^{n + 1}x_i - \alpha
		\end{align}
		If $x_i = \alpha$ for all $1\leq i\leq n + 1$, then we are done, since Eq. \eqref{eq:AM-GM} would be an equality. If all the $x_i$ are not equal to $\alpha$, then WLOG, let there be exactly two $x_i$ that are not equal to $\alpha$. Place these two elements at the end, and noting that one needs to be bigger than $\alpha$ and the other smaller than $\alpha$, WLOG we have 
		\begin{align}\label{eq:AM-GM-proof-2}
			\left(x_n - \alpha > 0\right) \wedge \left( \alpha - x_{n+1} > 0 \right) \Rightarrow (x_n - \alpha)(\alpha - x_{n+1}) > 0.
		\end{align}
		Now define
		\begin{align}
			y := x_{n} + x_{n + 1} - \alpha \geq x_{n} - \alpha \overset{\tiny\eqref{eq:AM-GM-proof-2}}{>} 0.
		\end{align}
		and substitute it into Eq. \eqref{eq:AM-GM-proof-1}:
		\begin{align}
			n\alpha = \sum_{i=1}^{n + 1}x_i - \alpha = \sum_{i=1}^{n-1}x_i + x_n + x_{n+1} - \alpha = \sum_{i=1}^{n-1}x_i + y
		\end{align}
		Thus, $\alpha$ is also the arithmetic mean of the $n$ numbers $x_1$, \dots, $x_{n-1}$, $y$. Making use of the induction hypothesis, we get
		\begin{align}\label{eq:AM-GM-proof-3}
			\alpha^{n+1} = \alpha\alpha^n \geq \alpha y\prod_{i=1}^{n-1} x_i.
		\end{align}
		From Eq.\eqref{eq:AM-GM-proof-2} we know that 
		\begin{align}\label{eq:AM-GM-proof-4}
			y\alpha - x_{n}x_{n+1} = (x_{n} + x_{n + 1} - \alpha)\alpha - x_{n}x_{n+1} = (x_n - \alpha)(\alpha - x_{n+1}) > 0 \Leftrightarrow y\alpha > x_{n}x_{n+1}
		\end{align}
		Substituting Eq. \eqref{eq:AM-GM-proof-4} into \eqref{eq:AM-GM-proof-3}, we get
		\begin{align}
			\alpha^{n+1} \geq \alpha y\prod_{i=1}^{n-1} x_i > x_nx_{n+1}\prod_{i=1}^{n-1} x_i = \prod_{i=1}^{n + 1} x_i \Leftrightarrow \alpha \geq \left(\prod_{i=1}^{n+1}x_i\right)^{\frac{1}{n+1}},
		\end{align}
		which proves the induction hypothesis.
	\end{proof}
	
	\begin{proof}(of Remark \ref{remark:cauchy_not_convergent_necess}) 
		Looking at the proof of Theorem \ref{thrm:mono_inc_seq_Cauchy}, we will see that that the statement of Theorem \ref{thrm:mono_inc_seq_Cauchy} also holds for sequences in $\mathbb Q$ that are upper bounded and monotonically increasing, since any non-empty subset of $\mathbb Q\subset \mathbb R$ that is upper bounded has a least upper bound in $\mathbb R$ (though not necessarily in $\mathbb Q$). 
		
		Hence, to show that the sequence $(a_n)_{n\in\mathbb N}$ with $a_n = \left(1 + n^{-1}\right)^n$ is a Cauchy sequence in $\mathbb Q$, we only need to show that it is monotonically increasing and derive an upper bound.
			
		For the monotonicity, we will make use of Theorem \ref{thrm:AM-GM} (\cite{64864}). For this, define the $x_i$ as follows: 
		\begin{align}
			x_i := \begin{cases}
				1, & i = 1
				\\ 1 + \frac{1}{n}, & 2 \leq i \leq n + 1
			\end{cases}
		\end{align}
		Then we have
		\begin{align}
			\frac{\sum_{i=1}^{n + 1}x_i}{n + 1} &\geq \left(\prod_{i = 1}^{n + 1}x_i\right)^{\frac{1}{n+1}} 
			\Leftrightarrow \left(\frac{\sum_{i=1}^{n + 1}x_i}{n + 1}\right)^{n+1} \geq \prod_{i = 1}^{n + 1}x_i
			\\ \Rightarrow \left(\frac{1 + n\left(1 + \frac{1}{n}\right)}{n+1}\right)^{n + 1} &= \left(1 + \frac{1}{n + 1}\right)^{n + 1} \geq \left(1 + \frac{1}{n}\right)^n \quad\forall n\in\mathbb N,
		\end{align}
		i.e. $a_{n+1} > a_n$.
		
		To prove that the sequence $(a_n)_{n\in\mathbb N}$ is upper bounded (\cite{5065619}), note that
		\begin{align}
			a_n &= \left(1 + \frac{1}{n}\right)^n = \sum_{k=0}^{n}\begin{pmatrix}
				n \\ k
			\end{pmatrix}\frac{1}{n^k} = \sum_{k = 0}^{n}\frac{n!}{k!(n-k)!}\frac{1}{n^k} = 1 + \sum_{k = 1}^{n}\frac{n!}{k!(n-k)!}\frac{1}{n^k}
			\\ &= 1 + \sum_{k = 1}^{n}\frac{n(n-1)\dots (n-k+1)}{k!}\frac{1}{n^k} \leq 1 + \sum_{k=1}^{n}\frac{1}{k!} \leq 1 + \sum_{k=1}^{n}\frac{1}{2^{k-1}}
			\\ &\leq 1 + \sum_{k=0}^{\infty}\frac{1}{2^k} = 3.
		\end{align}
	\end{proof}

	\begin{exmp}
		We can instruct infinitely many counter-examples to Theorem \ref{thrm:convergent_sequence_cauchy_sequence} in $\mathbb Q$. Let 
		\begin{align}
			\left(a_n\right)_{n\in\mathbb N} := \floor{10^{n} \cdot a}\cdot 10^{-n}, 
		\end{align}
		where $a\in \mathbb R\backslash \mathbb Q$. $(a_n)_{n\in\mathbb N}$ is a Cauchy sequence in $\mathbb Q$, which gives $a$ to its $n^{\text{th}}$ decimal point, converging to $a$. To see that it is a Cauchy sequence in $\mathbb Q$, note that all sequence elements are rational, that the sequence is bounded by $a$ and that it is monotonically increasing. Thus, by the proof of Theorem \ref{thrm:mono_inc_seq_Cauchy}, which also holds for $\mathbb Q$, $\left(a_n\right)_{n\in\mathbb N}$ is Cauchy in $\mathbb Q$.
	\end{exmp}

	\begin{theorem}\label{thrm:mono_inc_seq_converges}
		Any sequence $(a_n)_{n\in\mathbb N}$ with $a_n\in \mathbb R$ for any $n\in \mathbb N$ that is monotonically increasing and upper bounded for any $n\in\mathbb N$ converges in ($\mathbb R$, $d$) with $d(x, y):= \abs{x - y}$ for any $x$, $y\in \mathbb R$.
	\end{theorem}

	\begin{proof}
		The proof is identical to that of Theorem \ref{thrm:mono_inc_seq_Cauchy}. With the same notation as there, we have, cf. Eq. \eqref{eq:mono_inc_seq_Cauchy}:
		\begin{align}
			M - \epsilon &< a_n \leq M, \quad \forall n\geq N
			\\ \Rightarrow \abs{a_n - M} &< \epsilon,
		\end{align}
		i.e. the limit point of $(a_n)_{n\in\mathbb N}$ is $M$ (the least upper bound).
	\end{proof}

	\begin{remark}
		Note that Theorem \ref{thrm:mono_inc_seq_converges} does not hold in $\mathbb Q$, since the completeness axiom only guarantess the existence of a least upper bound in $\mathbb R$.
	\end{remark}

	\begin{defn}
		Let $(X, d)$ be a metric space. Then $(X, d)$ is called \textit{complete} if every Cauchy sequence in $X$ converges to a $\varphi\in X$ wrt $d$.
	\end{defn}
	
	\begin{theorem}
		Let $(X, d)$ be a metric space and consider the set $Y$. If $f: X\to Y$ is a bijection, then we can endow $Y$ with the following metric $d'$:
		\begin{align}\label{eq:metrics_completion}
			d'(y_1, y_2) := d\left(f^{-1}(y_1), f^{-1}(y_2)\right). \quad \forall y_1, y_2\in Y
		\end{align}
		$(X, d)$ is complete iff $(Y, d')$ is complete \cite{3789234}.
	\end{theorem}
	
	\begin{proof}
		Let $(X, d)$ be complete, i.e. every Cauchy sequence in $X$ converges in $X$. Now let $(x_n)_{n\in\mathbb N}$ be a Cauchy sequence in $X$ with limit point $\varphi\in X$. Consider the sequence $(y_n)_{n\in\mathbb N} := \left(f(x_n)\right)_{n\in\mathbb N}$ in $Y$. Since $X$ is Cauchy, we have $ d(x_m, x_n) = d'(y_m, y_n) < \epsilon$ for all $\epsilon > 0$ and $m$, $n\geq N$. Thus, we have that $(y_{n})_{n\in\mathbb N}$ is also a Cauchy sequence in $Y$. The limit point of $\left(y_n\right)_{n\in\mathbb N}$ is $f(\varphi)$, since $d'(y_n, f(\varphi)) = d(x_n, \varphi) < \epsilon'$ for all $\epsilon' > 0$ and $n\geq N'$.
	\end{proof}

	\begin{remark}
		To construct incomplete metric spaces, we can thus take an incomplete metric space $(X, d)$, construct a surjection $f:X\to Y$ to another set $Y$ and endow $Y$ with the metric $d'$ from Eq. \eqref{eq:metrics_completion}, then $(Y, d')$ is also incomplete.
	\end{remark}
	
	\begin{exmp}
		Let $X = \left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$, and consider the metric $d(x_1, x_2) := \abs{x_1 - x_2}$. Define $Y = \mathbb R$, and consider the map $f: X\to Y$, $x\mapsto \tan(x)$, which is surjective. On $Y$, consider the metric $d'(y_1, y_2) := d\left( \arctan(y_1), \arctan(y_2)\right) = \abs{\arctan y_1 - \arctan y_2}$, cf. Eq. \eqref{eq:metrics_completion}. Since $(X, d)$ is incomplete, $(Y, d')$ is incomplete as well. To see that $(X, d)$ is incomplete, note that $\left(\frac{\pi}{2} - \frac{1}{n}\right)_{n\in\mathbb N}$ or $\left( -\frac{\pi}{2} + \frac{1}{n} \right)_{n\in\mathbb N}$ are Cauchy in $X$, yet do not converge in $X$.
	\end{exmp}
	
	\begin{exmp}[Complete metric spaces]
		Any set $X$ equipped with the discrete metric from Def. \ref{remark:discrete-metric} is complete.
	\end{exmp}
	
	\begin{exmp}
		The metric space $(\mathbb R, d)$ with $d(x, y) := \abs{x - y}$ is \mbox{complete}. Also, $(\mathbb R^{d}, \norm{x}{p})$, cf. Example \ref{exmp:lp-norm-vectors}, for $1\leq p\leq \infty$ is complete.
	\end{exmp}

	\begin{exmp}\label{exmp:space-continuous-functions-complete}
		Let $\mathcal C[a, b]$ be the space of all continuous functions $\chi: [a, b]\to\mathbb{R}$ with the metric 
		\begin{align}
			d_{\infty}(\varphi, \psi) = \norm{\varphi - \psi}{\infty} :=  \sup_{x\in [a, b]}\left\{ \abs{\varphi(x) - \psi(y)} \right\}\quad \forall \varphi, \psi\in\mathcal C[a, b].
		\end{align}
		The metric space $(\mathcal C[a, b], d_{\infty})$ is complete.
	\end{exmp}

	\begin{proof}
		Let $(\varphi_n)_{n\in\mathbb N}$ be a Cauchy sequence in $\mathcal C[a, b]$, then for any $x\in [a, b]$:
		\begin{align}
			\abs{\varphi_n(x) - \varphi_m(x)} \leq \sup_{x\in [a, b]}\left\{ \abs{\varphi_{n}(x) - \varphi_{m}(y)} \right\} < \epsilon. \quad\forall m, n\geq N
		\end{align} 
		Hence, $(\varphi_n)_{n\in\mathbb N}(x)$ is a Cauchy sequence in $\mathbb R$ for any $x\in[a, b]$ wrt the metric $d(x, y) := \abs{x - y}$. Since $\left(\mathbb R, d\right)$ is complete, we know that $\varphi(x) := \lim\limits_{n\to\infty}\varphi_n(x)$ exists. 
		
		Finally, let us show that $\varphi\in\mathcal C[a, b]$. Since $\left(\varphi_n(x)\right)_{n\in\mathbb N}$ converges in $(\mathbb R, d)$, we can choose an $N\in\mathbb N$ s.t. $\abs{\varphi_n(x) - \varphi(x)} < \epsilon/3$ for all $n\geq N$ and all $x\in[a, b]$. And since $\left(\varphi_n\right)_{n\in\mathbb N}$ is a Cauchy sequence in $(\mathcal C[a, b], d_{\infty})$ (by assumption), i.e. the $\varphi_n$ are continuous, there exists a $\delta > 0$ s.t. $\abs{\varphi_N(x) - \varphi_{N}(y)} < \epsilon/3$ with $\abs{x - y} < \delta$. If $\abs{x-y} < \delta$, we get by applying the triangle inequality
		\begin{align}
			\abs{\varphi(x) - \varphi(y)} \leq \abs{\varphi(x) - \varphi_N(x)} + \abs{\varphi_N(x) - \varphi_N(y)} + \abs{\varphi_N(y) - \varphi(y)} < \epsilon,
		\end{align}
		which shows the continuity of $\varphi$, cf. Def. \ref{defn:continuity}.
		
		Thus, we have shown that $\varphi\in\mathcal C[a, b]$.
	\end{proof}

	\begin{theorem}\label{thrm:complete_subsets_properties}
		Let $(X, d)$ be a metric space. Then the following statements are true.
		\begin{enumerate}[label=(\alph*)]
			\item Complete subsets of $(X, d)$ are closed. \vspace{-0.3cm}
			\item If $(X, d)$ is complete, then every closed subset of $(X, d)$ is complete.
		\end{enumerate}
	\end{theorem}

	\begin{proof}
		(a): Let $U\subset X$ be closed wrt $d$, and let $\varphi\in X$ be a limit point of $\overline{U}$, i.e. there exists a sequence $\left(\varphi_{n}\right)_{n\in\mathbb N}$ in $U$ s.t. $\lim\limits_{n\to\infty}\varphi_n = \varphi$. Hence, $\left(\varphi_{n}\right)_{n\in\mathbb N}$ converges in $X$, and is thus a Cauchy sequence in $X$. Since $U$ is closed, the limit of the sequence lies in $U$, i.e. $U$ is closed.
		
		(b): Let $(X, d)$ be complete, $U\subset X$ closed and $(\varphi_n)_{n\in\mathbb N}$ a Cauchy sequence in $U$, which converges to a $\varphi\in X$, since $X$ is closed. Since $\varphi$ is the limit of $(\varphi_n)_{n\in\mathbb N}$ and $U$ is closed, it follows that $\varphi\in U$.
	\end{proof}

	\begin{defn}
		Let $(X, d)$ be a metric space. Then $U\subset X$ is called \textit{dense} in $X$ if $\overline{U} = X$, i.e. to every $\varphi\in X$ there exists a sequence $\left(\varphi_n\right)_{n\in\mathbb N}$ in $U$ that converges to $\varphi$ wrt $d$. Alternatively, we can say that to every $\varphi\in X$ and $\epsilon > 0$ there is a $\psi\in U$ s.t. $d(\varphi, \psi) < \epsilon$ (set e.g. $\psi := \varphi_N$, where $N$ is s.t. for all $n\geq N$, $d(\varphi_n, \varphi) < \epsilon$).
	\end{defn}

	\begin{exmp}
		$\mathbb Q$ lies dense in $\mathbb R$ wrt $d(x, y) := \abs{x - y}$ for all $x$, $y\in\mathbb R$.
	\end{exmp}

	\begin{theorem}\label{remark:sum-rational-irrational}
		If $\varphi\in\mathbb R\backslash \mathbb Q$ and $\psi\in\mathbb Q$, then $\varphi + \psi$ is irrational. This is because any $\psi \in \mathbb Q$ can be written as $p/q$, where $p$, $q\in\mathbb Z$. Thus: If $\varphi + \psi$ was rational, we would have
		\begin{align}
			\varphi + \psi = \varphi + \frac{p}{q} = \frac{r}{s} \Rightarrow \varphi = \frac{r}{s} - \frac{p}{q} = \frac{rq - ps}{qs} \in \mathbb Q,
		\end{align}
		which is a contradiction to the assumption $\varphi\in\mathbb R\backslash\mathbb Q$. This proves that the sum of a rational and irrational number is always irrational.
	\end{theorem}

	\begin{exmp}[Dense subsets]
		$\mathbb R\backslash \mathbb Q$ lies dense in $\mathbb R$ wrt $d(x, y) := \abs{x - y}$ for all $x$, $y\in\mathbb R$.
	\end{exmp}

	\begin{proof}
		We need to show that to every $\varphi\in \mathbb R$ there exists a sequence $\left(\varphi_n\right)_{n\in\mathbb N}\in \mathbb R\backslash \mathbb Q$ s.t. $\varphi_n\overset{n\to\infty}{\longrightarrow}\varphi$ wrt $d$. We already know that $\mathbb Q$ lies dense in $\mathbb R$ wrt $d$, i.e. there exists a sequence $\left(\psi_n\right)_{n\in\mathbb N}$ s.t. $\psi_n\overset{n\to\infty}{\longrightarrow}\varphi$. Now define the sequence $\seq[\varphi_n] := \seq[\psi_n + \frac{a}{n}]$, where $a\in\mathbb R\backslash \mathbb Q$. Each sequence element lies in $\mathbb R\backslash \mathbb Q$, since the sum of a rational and an irrational number is irrational, cf. Remark \ref{remark:sum-rational-irrational}, and converges to $\varphi$ wrt $d$.
	\end{proof}

	\begin{exmp}[Weierstrass Approximation Theorem]\label{exmp:weierstrass-approx-thrm}
		The algebraic polynomials $\mathcal P$ are dense on an interval $[a, b]\subset \mathbb R$ with $-\infty < a < b < \infty$ in $\mathcal C[a, b]$ wrt the metric $d_{\infty}$ from Example \ref{exmp:space-continuous-functions-complete}. \cite[Corollary 6.12]{iske:approximation}
	\end{exmp}

	\begin{proof}
		We deletegate the proof of this celebrated Theorem to appendix \ref{app:weierstrass-app-theorem}. 
	\end{proof}

	\begin{theorem}\label{thrm:characterization-dense-subsets}
		Let $(X, d)$ be a metric space. $U\subset X$ dense iff every non-empty open set $O\subset X$ intersects $U$, i.e. $U\cap O\ne\emptyset$.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longrightarrow$} Use Theorem \ref{thrm:property_metric_space} and note that $U\subset X$ is dense, i.e. $\overline{U} = X$.
		\newline\newline\enquote{$\Longleftarrow$} We want to prove $U\subset X$ dense $\Rightarrow U\cap O\ne\emptyset$ for all non-empty open sets $O\subset X$. We will prove this via its contraposition, i.e. if there exists a non-empty open set $O\subset X$ with $U\cap O=\emptyset$, then $U\subset X$ is not complete.
		
		Note that according to Theorem \ref{thrm:property_metric_space}, if $\varphi\in\overline{U}$ for $U\subset X$, then for every open set $O\subset X$ containing $\varphi$, $U\cap O\ne\emptyset$. The contraposition of this statement is what we want to prove.
	\end{proof}

	\begin{remark}
		Let $d$ and $d'$ be two metrics on $X$. If $U\subset X$ is dense wrt $d$, it is not necessarily dense wrt $d'$.
	\end{remark}

	\begin{exmp}
		Consider $X=\mathbb R$ and let $d(x, y) := \abs{x - y}$ for all $x$, $y\in\mathbb R$. While $U = \mathbb Q \subset X = \mathbb R$ is dense wrt $d$, it is not dense wrt the discrete metric from Remark \ref{remark:discrete-metric}, since for $r \leq 1$, the open balls are given by the singletons $\{\varphi\}$ for any $\varphi\in X$, cf. Example \ref{exmp:open-balls-discrete-metric}. Let $\varphi = \pi$, then $B_{r}(\pi) = \{\pi\}$ for any $r\leq 1$, but clearly, $\mathbb Q \cap \{\pi\} = \emptyset$, and thus by Theorem \ref{thrm:characterization-dense-subsets} $\mathbb Q$ is not dense in $\mathbb R$ wrt the discrete metric.
	\end{exmp}

	\begin{remark}
		Let $(X, d)$ be a metric space. Then there are examples of dense subsets $U\subset X$ wrt $d$ that are not complete wrt $d$, and vice versa.
	\end{remark}

	\begin{exmp}
		Consider the cantor set $\mathcal C$ on $[0, 1]$, which we consider as a subset of the metric space $\left([0, 1], d\right)$ with $d(x, y) := \abs{x - y}$ for all $x$, $y\in[0, 1]$. Formally, it is defined as follows:
		\begin{align}
			C_{0} &:= [0, 1],
			\\ C_{n} &:= \frac{C_{n-1}}{3} \cup \left(\frac{2}{3} + \frac{C_{n-1}}{3}\right) = \frac{1}{3}\left(C_{n-1} \cup \left( 2 + C_{n-1} \right)\right),
			\\ \mathcal C &:= \bigcap_{n=0}^{\infty}C_n.
		\end{align} 
		Intuitively, the Cantor set $\mathcal C$ is created by iteratively removing the open middle third of each line segment.
	
		Since for all $n\in\mathbb N$, $C_n$ is closed (as the finite union of closed sets) -- for a formal proof, use induction -- and the arbitrary intersection of closed sets is closed, we have that $\mathcal C\subset [0, 1]$ is closed wrt $d$. Thus, by Theorem \ref{thrm:complete_subsets_properties} b), $\mathcal C$ is complete wrt $d$. Now consider the open set $O := \left(\frac{1}{2}, \frac{2}{3}\right) \subset [0, 1]$, which is open wrt $d$. Clearly, $O\cap \mathcal C = \emptyset$, and thus by Theorem \ref{thrm:characterization-dense-subsets} $\mathcal C$ is not dense wrt $d$.
	\end{exmp}

	\begin{exmp}
		$\mathbb Q$ is a dense subset of $X$, where $\left(X, d\right)$ with $d(x, y) := \abs{x-y}$ for all $x$, $y\in\mathbb R$. However, we have already established that $\mathbb Q$ is not complete wrt $d$, e.g. cf. Remark \ref{remark:cauchy_not_convergent_necess}.
	\end{exmp}
	
	
	\newpage 
	\subsection{Continuity}
	\begin{defn}[$\epsilon$-$\delta$ definition of continuity \cite{MfPI}]\label{defn:continuity}
		Let $(X, d)$ and $(Y, d')$ be metric spaces. A function $f: X\rightarrow Y$ is called \textit{continuous} at $p\in X$ if for all $\epsilon > 0$ there exists a $\delta = \delta(\epsilon, p) > 0$ s.t. 
		\begin{align}
			\forall z\in D: d\left(z, p\right) < \delta \Rightarrow d'\left( f\left(z\right), f\left(p\right) \right) < \epsilon.  
		\end{align}
		$f$ is called continuous on $X$ if it is continuous for every $p\in X$.
	\end{defn}

	\begin{defn}[Uniform continuity \cite{src:uniform_continuity}]
		Let $(X, d)$ and $(Y, d')$ be metric spaces. A function $f: X\rightarrow Y$ is called \textit{uniformly continuous} on $X$ if for all $\epsilon > 0$ there exists a $\delta = \delta(\epsilon)$ s.t. for all $p\in X$,
		\begin{align}
			\forall z\in D: d\left(z, p\right) < \delta \Rightarrow d'\left( f\left(z\right), f\left(p\right) \right) < \epsilon.
		\end{align}
	\end{defn}

	\begin{remark}
		While uniform continuity implies continuity, the opposite is in general not true.
	\end{remark}

	\begin{theorem}
		Def. \ref{defn:continuity} is equivalent to the following (with the same notation): For every sequence $\seq[p_n]$ in $D$ that converges to $p$, i.e. $p_n \overset{n\to\infty}{\longrightarrow} p$ wrt $d$, the sequence $\seq[f(p_n)]$ converges to $f(p)$ wrt $d'$.
	\end{theorem}

	\begin{theorem}
		Any function $f: [a, b]\to\mathbb R$ defined on the compact interval \\ $[a, b] \subset \mathbb R$, where on $\mathbb R$ we consider the usual metric, that is continuous on $[a, b]$, is uniformly continuous on  $[a, b]$.
	\end{theorem}

	\begin{proof}
		\cite[p. 285]{MfPI}.
	\end{proof}

	\begin{theorem}
		Any function $f: [a, b]\to\mathbb R$ defined on the compact interval \\ $[a, b] \subset \mathbb R$ (where on $\mathbb R$ we consider the usual metric) that is continuous is bounded, in the sense that $\{ f(x) \mid x\in [a, b] \}$ is bounded, cf. Defn. \ref{defn:bounded_set}, 
	\end{theorem}

	\begin{proof}
		\cite[p. 161]{MfPI}.
	\end{proof}

	\begin{lemma}\label{continuity_vector_components}
		Consider the map $f: \mathbb R^m\rightarrow\mathbb R^n$. Then $f$ is continuous if all the $f_i: \mathbb R^m\rightarrow\mathbb R$, $i\in\{1, \dots, n\}$ are continuous. 
	\end{lemma}
	\begin{proof}
		Since all metrics are strongly equivalent, it does not matter which metric we equip $\mathbb R$, $\mathbb R^m$ and $\mathbb R^n$ with, cf. Remark \ref{equivalence_metrics_finite_dimensional}; for the following, consider the Euclidean metric. Suppose each $f_i$ is continuous for all $i\in \{1, \dots, n\}$, i.e.: 
		\begin{align} 
			\forall p\in\mathbb R^m: \forall \epsilon_i > 0 \ \exists 
			\delta_i>0: d_{\mathbb R}(f_i(p), f_i(z)) < \frac{\epsilon_i}{\sqrt{n}} \ \forall z\in D \quad \text{with}\ d_{\mathbb R^m}(p, z) < \delta_i. 
		\end{align} 
		Now define $\epsilon:= \max\{\epsilon_1, \dots, \epsilon_n\}$. Thus: 
		\begin{align}
			d_{\mathbb R^n}(f(p), f(z)) = \sqrt{\sum_{i=1}^{n}\left(f_i(p)-f_i(z)\right)^2} 
			= \sqrt{\sum_{i=1}^{n}d^2_{\mathbb R}\left(f_i(p), f_i(z)\right)} < \sqrt{\sum_{i=1}^{n}\left(\frac{\epsilon}{\sqrt{n}}\right)^2} = \epsilon. 
		\end{align}
	\end{proof}
	
	\begin{theorem}\label{defn:preimages_continuous_functions}
		For a map $f: X\rightarrow Y$ between two metric spaces $\left(X, d_{X} \right)$ and $\left(Y, d_{Y} \right)$ the following statements are equivalent: 
		\begin{enumerate}[label = (\roman*)]
			\item $f$ is continuous,
			\item preimages $f^{-1}(V) := \left\{ x\in X\mid f\left(x\right) \in V \right\}$ of \underline{all} open sets $V\subset Y$ are open,  
			\item preimages $f^{-1}(A)$ of \underline{all} closed sets $A\subset Y$ are closed. 	
		\end{enumerate}
	\end{theorem} 
	\noindent\textit{Proof:} \\ (i) $\Rightarrow$ (ii) Assume that $f$ is continuous and that $V\subset Y$ is open and let $a\in f^{-1}\left( V\right)$. Since $V$ is an open set, $\exists \epsilon > 0$ s.t. $B_{\epsilon}\left( f\left(a\right) \right)\subset V$, cf. Def. \ref{defn-open-set}. By assumption, $f$ is continuous at $a\in f^{-1}\left(V\right)\subset X$ and therefore $\forall \epsilon > 0 \ \exists \delta > 0$ s.t. $d_{X}\left(x, a\right) < \delta\Rightarrow d_{Y}\left(f\left(x\right), f\left(a\right)\right) < \epsilon \ \forall x\in X$. Put differently, $x\in B_{\delta}\left( a\right)$ implies $f\left(x\right) \in B_{\epsilon}\left( f\left(a\right) \right) \subset V$. Thus, $B_{\delta}(a) \subset f^{-1}(V) \Rightarrow f^{-1}(V) \subset X$ is an open set, cf. Def. \ref{defn-open-set}. 
	\\ 
	\\ 
	(ii) $\Rightarrow$ (i) Assume that $f^{-1}\left(Y\right) \subset X$ is open for $V\subset Y$ open and let $a\in X$, $\epsilon > 0$. From Theorem \ref{open-balls-open} we know that $B_{\epsilon}\left(f\left(a\right)\right)\subset Y$ is open. Thus, by assumption, $f^{-1}\left( B_{\epsilon}\left( f\left(a\right) \right) \right) = \left\{ x\in X \mid f\left( x \right) \in B_{\epsilon}\left( f\left(a\right) \right) \right\} = \left\{ x\in X \mid f\left(x\right) \in \left\{ y\in Y\mid d_Y\left( y, f\left(a\right) \right) < \epsilon \right\}\right\}$ is open as well. Clearly, $a\in f^{-1}\left( B_{\epsilon}\left( f\left(a\right) \right) \right)$. Therefore, it follows from Def. \ref{defn-open-set} that $\exists\delta > 0$ s.t. $B_{\delta}(a) \subset f^{-1}\left( B_{\epsilon}\left( f\left(a\right) \right) \right)$. Thus, $\forall x\in B_{\delta}(a): d_{X}\left( x, a \right) < \delta \Rightarrow d_Y\left(f\left(x\right), f\left(a\right)\right) < \epsilon$. This proves that $f: X\rightarrow Y$ is continuous in every point $a\in X$. \cite{cont-functions-open-sets}   
	\\ 
	\\
	(ii) $\Rightarrow$ (iii) Assume that the preimages $f^{-1}(V)$ of all open sets $V\subset Y$ are open. Since $f^{-1}(Y\backslash V) = f^{-1}(Y)\backslash f^{-1}(V)$, as can be easily shown by using the definition of the complement of a set, we have for all open sets $V \subset Y$: 
	\begin{align}
		f^{-1}(Y\backslash V) = f^{-1}(Y)\backslash f^{-1}(V) = X\backslash f^{-1}(V). 
	\end{align}
	Since $f^{-1}(V)$ is open by assumption, $f^{-1}(Y\backslash V) = X\backslash f^{-1}(V)$ is closed. 
	\\ 
	\\
	(iii) $\Rightarrow$ (ii) Assume that the preimages $f^{-1}(A)$
	of all closed sets $A\subset Y$ are closed. Then: 
	\begin{align}
		f^{-1}(Y\backslash A) = f^{-1}(Y)\backslash f^{-1}(A) = X\backslash f^{-1}(A). 
	\end{align}
	Since $f^{-1}(A)$ is closed by assumption, $f^{-1}(Y\backslash A) = X\backslash f^{-1}(A)$ is open. \cite{preimage-of-closed-sets} \qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{corollary}
		Let $(X, \tau_1)$ and $(Y, \tau_2)$ be topological spaces coming from metric spaces. Then $f: X\rightarrow Y$ is continuous if and only if $f^{-1}(G)$ for every open set $G\in Y$. 
	\end{corollary}
	
	\begin{defn}[Basis]
		A \textit{basis} of a topology $\left(M, \tau\right)$ is a collection of open sets $\mathcal B$ such that for all $U\in \tau$ there exists an index set $I$ and corresponding $B_i\in \mathcal B$ s. t. 
		\begin{align}
			\bigcup_{i\in I}B_i = U. 
		\end{align}
	\end{defn}

	\begin{exmp}\label{basis_discrete_metric}
		Let $(X, d)$ be a metric space, where the metric is the discrete metric from Remark \ref{remark:discrete-metric}. Then the collection of all singletons $\{\varphi\in X\}$ is basis of $X$. 
	\end{exmp}
	
	\begin{exmp}\label{basis_metric_space}
		In a metric space $X$, the collection $$\{ B_{r}(x) \mid x\in X, r > 0 \}$$ of open balls is a basis for the topology given by the metric on $X$. 
	\end{exmp}

	\begin{defn}[Hausdorff]
		A topology $\left(M, \tau\right)$ is called \textbf{Hausdorff} if $\forall p\ne q \in M \ \exists U$, $V\in \tau$ open with $p\in U$, $q\in V$ such that $U\cap V = \emptyset$.  
	\end{defn} 

	\begin{lemma}
		All metric spaces are Hausdorff spaces. 
	\end{lemma}
	\noindent\textit{Proof:} 
	\begin{figure}[h!]		
		\centering 
		\includegraphics[trim = {3.8cm 5.8cm 9.7cm 2.8cm}, width=0.5\textwidth, clip]{Figures/metric-spaces-Hausdorff-spaces-v2.png}
		\caption{Visualization for why a metric space is a Hausdorff space.}
		\label{metric-space-Hausdorff-space}
	\end{figure} 
A visualization is shown in Fig. \ref{metric-space-Hausdorff-space}. To prove this more rigorously, define $U := B_{r}(p)$ and $V:=B_{r}(q)$ with radius $r := \frac{d(p, q)}{2}$, where from Theorem \ref{open-balls-open} we know that $U$ and $V$ are open. Suppose $U\cap V = \emptyset$ would not hold. Then there exists a $z\in U\cap V$ with 
	\begin{align}
		d(p, z) < \frac{d(p, q)}{2}
	\end{align}
	and 
	\begin{align}
		d(q, z) < \frac{d(p, q)}{2}. 
	\end{align}
	Therefore, by the triangle inequality for metric spaces, we have: 
	\begin{align}
		d(p, q) \leq d(p, z) + d(q, z) < d(p, q), 
	\end{align}
	which is clearly a contradiction. 
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}
		Let $(X, \tau)$ be a topological space. Then it is said to be \textit{second countable} if $\tau$ has a countable basis. 
	\end{defn}
	
	\begin{exmp}
		For any $n > 0$, the topological spaces $\mathbb R^n$ are second countable. 
	\end{exmp}
	
	\begin{remark}
		Metric spaces are not automatically second countable. For example, take an uncountable set $X$, endow it with the discrete metric and because of Example \ref{basis_discrete_metric}, \ref{basis_metric_space},  the topological space $X$ is not second countable, since $X$ is uncountable. 
	\end{remark}
	
	\begin{defn}[Homeomorphism] A \textit{homeomorphism} between two topological spaces $X$ and $Y$ is an invertible function $f: X\rightarrow Y$ such that $f$ and $f^{-1}$ are continuous \cite[p. 33]{topology-singh}. 
	\end{defn}

	\begin{exmp}
		The Euclidean space $\mathbb R^n$, equipped with the usual topology, is homeomorphic to the open ball $B_{r}(\varphi) =  \{x\in\mathbb R^n \mid \lvert\lvert x-\varphi \rvert\rvert < r \} \subset \mathbb R^n$ (consider the open ball as a metric space with the \textit{induced metric} from the whole space of $\mathbb R^n$ and then equip it with the usual topology on a metric space given by the open sets). 
	\end{exmp}
	\noindent\textit{Proof:} Consider the map $$f: \mathbb R^n\rightarrow B_r(\varphi), \ x\mapsto \frac{r\cdot (x-\varphi)}{1+\lvert\lvert x-\varphi\rvert\rvert}.$$ Obviously, $f$ is continuous with inverse $$f^{-1}: B_r(\varphi)\rightarrow \mathbb R^n, \ x \mapsto \frac{x}{r-\lvert\lvert x\rvert\rvert}+\varphi,$$ which is also continuous. One can easily show that $f$ and $f^{-1}$ are inverses to each other. Thus, $f$ \\ describes a homeomorphism. 	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad$\blacksquare$
	
	\begin{exmp}[Stereographic projection]
		The unit sphere $S^n$, embedded in $\mathbb R^{n+1}$, without the north pole, i.e. $S^n\backslash \{p\} \subset \mathbb R^{n+1}$, where $S^n := \{ x\in\mathbb R^{n+1} \mid \norm{x}{2} = 1 \}$ and $p := \{ x\in\mathbb R^{n+1} \mid x_i = 0 \ \forall i\in[1, n], x_{n+1} = 1 \}$, is homeomorphic to $\mathbb R^n$ (consider the unit sphere as a metric space with the \textit{induced metric} from the whole space of $\mathbb R^{n+1}$ and then equip it with the usual topology on a metric space given by the open sets). 
	\end{exmp}
	\begin{proof}
			Consider the map 
		\begin{align}
			f: S^n\backslash\{p\}\rightarrow \mathbb R^n, \ x\mapsto \frac{1}{1-x_{n+1}}\begin{pmatrix} x_1, \dots, x_n \end{pmatrix}^T. 
		\end{align}
		Obviously, $f$ is continuous. Its inverse is given by 
		\begin{align}\label{stereographic_map_inverse}
			f^{-1}: \mathbb R^n \rightarrow S^n\backslash\{p\}, \ x\mapsto \begin{pmatrix} 2x_1/ \left(1+\norm{x}{2}^2\right)\\ \vdots\\ 2x_n/\left(1+\norm{x}{2}^2\right) \\[4pt] 1-2/\left(1+\norm{x}{2}^2\right) \end{pmatrix}.  
		\end{align}
		It is trivial to show that $f$ and $f^{-1}$ are inverses to each other. One can also easily show that $\norm{f^{-1}(x)}{2} = 1$; thus, $f^{-1}$ does indeed bring us to the unit ball $S^n$. 
		To see that $f^{-1}$ is continuous, note that all components are continuous and thus, according to Lemma \ref{continuity_vector_components}, the function itself is continuous. 
	\end{proof} 
	
	\begin{defn}[Homotopy mapping \cite{ding2023sides}]
		The function $H(z, \lambda) = \lambda r(z) + (1 - \lambda) g(z)$ is said to be a homotopy mapping from $g(z)$ to $r(z)$ if $\lambda\in [0, 1]$ and $g$ is a smooth function. The equation $H(z, \lambda) = 0$ is the so-called \enquote{zero path} of this homotopy mapping.
	\end{defn}
	
	\newpage 
	\section{Linear Functional Analysis}

	\subsection{Bounded Linear Operators}

	Let us start by defining the notions of \textit{linearity} and \textit{boundedness} of operators (\mbox{functionals}). 
	
	\begin{defn}\label{defn:linearity_operator}
		Let $X$, $Y$ be normed linear spaces. Then an operator 
		$A:X\to Y$ is called \textit{linear} if 
		\begin{align}
			A(\alpha \varphi + \beta \psi) = \alpha A(\varphi) + \beta A(\psi) \quad \forall \varphi, \psi\in X, \forall \alpha, \beta\in \mathbb K.
		\end{align}
	\end{defn}

	\begin{theorem}
		A linear operator $A: X\to Y$ from a normed linear space $(X, \norm{\cdot}{X})$ into another normed linear space $(Y, \norm{\cdot}{Y})$ is continuous on $X$ iff it is continuous at a single point in $X$.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longrightarrow$} By definition. 
		
		\enquote{$\Longleftarrow$} Let $A$ be continuous at $\varphi_0\in X$, and let $\seq[\varphi_n]\subset X$ converge to $\varphi\in X$ wrt $\norm{\cdot}{X}$, then
		\begin{align}
			A(\varphi_n) = A(\varphi_n + \varphi_0 - \varphi) - A(\varphi_0 - \varphi) \overset{n\to\infty}{\longrightarrow} A(\varphi)
		\end{align} 
		wrt $\norm{\cdot}{Y}$.
	\end{proof}
	
	\begin{defn}\label{defn:boundedness_operator}
		Let $\left(X, \norm{\cdot}{X}\right)$ and $(Y, \norm{\cdot}{Y})$ be normed linear spaces. Then an operator $A: X\to Y$ is called \textit{bounded} if there is a constant $C(A) > 0$ s.t. 
		\begin{align}
			\norm{A\varphi}{Y} \leq C \norm{\varphi}{X} \quad\forall \varphi\in X.
		\end{align}
		Any such constant $C$ is called an \textit{upper bound} for $A$.
	\end{defn}

	\begin{theorem}
		A linear operator $A: X\to Y$ between normed spaces $(X, \norm{\cdot}{X})$ and $(Y, \norm{\cdot}{Y})$ is bounded iff the operator norm
		\begin{align}
			\norm{A}{} := \sup_{\norm{\varphi}{X} = 1}\norm{A\varphi}{Y} = \sup_{\varphi\in X\backslash \{0\}}\frac{\norm{A\varphi}{Y}}{\norm{\varphi}{X}}
		\end{align}
		on the linear space $\mathcal L(X, Y)$ of all bounded linear operators between the two normed spaces $(X, \norm{\cdot}{X})$ and $(Y, \norm{\cdot}{Y})$ is finite, i.e. 
		\begin{align}
			\norm{A}{} < \infty. 
		\end{align} 
		The operator norm is the smallest upper bound for $A$.
	\end{theorem}
	
	\begin{proof}
		First, let us prove that the operator norm does indeed define a norm on $\mathcal L(X, Y)$. Note that the sum of two operators $A, B\in\mathcal L(X, Y)$ is defined pointwise, i.e. for $\varphi\in X$: $\left(A + B\right)(\varphi) := A\varphi + B\varphi$. Similarly, scalar multiplication is defined for $\alpha \in \mathbb K$ as $\left(\alpha A\right)(\varphi) := \alpha A\varphi$.
		
		The positivity, definiteness and homogeneity of $\norm{A}{}$ are trivial to show. For the triangle equality, note that for any $A, B\in\mathcal L(X, Y)$, we have
		\begin{align}
			\norm{A + B}{} &= \sup_{\norm{\varphi}{X} = 1}\norm{\left(A + B\right)(\varphi)}{Y} = \sup_{\norm{\varphi}{X} = 1}\norm{A\varphi + B\varphi}{Y}
			\\ &\leq \sup_{\norm{\varphi}{X} = 1} \left\{\norm{A\varphi}{Y} + \norm{B\varphi}{Y}\right\} 
			\\ &\leq \sup_{\norm{\varphi}{X} = 1}\{ \norm{A\varphi}{Y} \} + \sup_{\norm{\varphi}{X} = 1}\{ \norm{B\varphi}{Y} \} 
			\\ &= \norm{A}{} + \norm{B}{}.
		\end{align}
	
		Now to the statement $A$ bounded iff $\norm{A}{} < \infty$.
		
		\enquote{$\Longrightarrow$} Let $A$ be upper bounded with bound $C > 0$, then
		\begin{align}
			\norm{A}{} = \sup_{\norm{\varphi}{X} = 1}\norm{A\varphi}{Y} \leq C < \infty.
		\end{align}
		This also shows that $\norm{A}{}$ is the smallest upper bound for $A$.
		\\
		
		\enquote{$\Longleftarrow$} We have 
		\begin{align}
			\norm{A}{} \norm{\varphi}{X} = \sup_{\varphi\in X\backslash \{0\}}\frac{\norm{A\varphi}{Y}}{\norm{\varphi}{X}} \norm{\varphi}{X} = \sup_{\varphi\in X\backslash \{0\}}\norm{A\varphi}{Y} \geq \norm{A\varphi}{Y},
		\end{align}
		which shows that $A$ is bounded with upper bound $C = \norm{A}{} < \infty$ (note that for $v = 0$, there is nothing to show).
	\end{proof}

	\begin{theorem}\label{thrm:continuous-operator-bounded}
		A linear operator $A:X\to Y$ for normed linear spaces $\left(X, \norm{\cdot}{X}\right)$ and $\left( Y, \norm{\cdot}{Y}\right)$ is continuous iff it is bounded.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longleftarrow$} Since $A$ is bounded, by definition, we know that for all $\varphi\in X$, there is an upper bound $C > 0$ s.t. 
		\begin{align}
			\norm{A\varphi}{Y} \leq C\cdot \norm{\varphi}{X}.
		\end{align}
		For an arbitrary $\epsilon > 0$, choose $\delta := \epsilon/C$, then for $\norm{\varphi - \psi}{X} < \delta = \epsilon/C$ (where $\psi\in X$ is arbitary) we have because of the linearity of $A$
		\begin{align}
			\norm{A\varphi - A\psi}{Y} = \norm{A(\varphi - \psi)}{Y} \leq C\norm{\varphi - \psi}{X} < \epsilon,
		\end{align}
		which proves the continuity of $A$, cf. Def. \ref{defn:continuity} \cite{556667}.
		\\
		
		\enquote{$\Longrightarrow$} Let $A$ be continuous on $X$. Then by Theorem \ref{defn:preimages_continuous_functions}, we know that $A^{-1}\left(B^{Y}_1(0)\right)$ is open, since $B^{Y}_{1}(0)$ is open. By the linearity of $A$, $0\in A^{-1}\left(B^{Y}_1(0)\right)$, since \\ $A(0) = 0 \in B_1^{Y}(0)$. This means that there is an $r > 0$ s.t. $B^{V}_{r}(0) \subset A^{-1}\left(B^{Y}_1(0)\right)$, i.e. the image of $B^{V}_{r}(0)$ is contained in $B^{Y}_{1}(0)$.
		
		Now, for any $a > 1$, choose $C := a/r$, and we will show that $C$ is an upper bound for $A$. For $\varphi\in X\backslash \{0\}$ (for $\varphi =0$, there is nothing to show), note that $$\norm{\frac{r}{a\norm{\varphi}{X}}\varphi}{X} = \frac{r}{a} < r,$$ i.e. 
		\begin{align}
			\frac{r}{a\norm{\varphi}{X}}\varphi \in B_{r}^{V}(0) &\Rightarrow A\left(\frac{r}{a\norm{\varphi}{X}}\varphi\right) = \frac{r}{a\norm{\varphi}{X}}A\varphi\in B^{Y}_{1}(0) 
			\\[6pt] &\Rightarrow \norm{\frac{r}{a\norm{\varphi}{X}}A\varphi}{Y} < 1 \Rightarrow \norm{A\varphi}{Y} \leq \frac{a}{r}\norm{\varphi}{X} = C\norm{\varphi}{X},
		\end{align}
		which completes the proof \cite[p. 2]{src:mit_lec}.
	\end{proof}
	
	\newpage 
	\section{Set Theory}
	\begin{defn}[Binary Relation \cite{binary_relations}]
		A \textit{binary relation} over a set $X$ is some relation $R$ where \\ $\forall x$, $y\in X$ the statement $xRy$ is either true or false. 
	\end{defn}
	
	\begin{defn}[Equivalence Relation and Class \cite{equivalence_relation}]
		An \textit{equivalence relation} on a set $X$ is a binary relation $\sim$ with the following properties $\forall x$, $y$, $z \in X$: 
		\begin{itemize}
			\item \textbf{reflexivity}: $x\sim x$, 
			\item \textbf{symmetry}: $x \sim y\Leftrightarrow y\sim x$, 
			\item  \textbf{transitivity}: $\left(x\sim y\right) \wedge \left(y\sim z\right)\Rightarrow x\sim z$.  
		\end{itemize}
		The \textit{equivalence class} of an element $x\in X$ is defined as 
		\begin{align}
			\left[x\right] := \left\{y\in X \mid x\sim y \right\} = \left\{ y\in X \mid y\sim x \right\}. 
		\end{align}
	\end{defn}

	\begin{defn}[Partially Ordered Set \cite{kuratowski_zorn_lemma}]\label{partially_ordered_set}
		A \textit{partially ordered set}  $\left(X, \leq\right)$ is a set $X$, equipped with a binary relation $\leq$, that satisfies the following properties $\forall x$, $y$, $z\in X$:
		\begin{itemize}
			\item \textbf{reflexivity}: $x\leq x$, 
			\item \textbf{antisymmetry}: $\left(x\leq y\right) \wedge \left(y\leq x\right) \Rightarrow x = y$, 
			\item \textbf{transitivity}: $\left(x\leq y\right) \wedge \left(y\leq z\right)\Rightarrow x\leq z$. 
		\end{itemize}
	\end{defn}

	\begin{defn}[Incomparability]
		In Definition \ref{partially_ordered_set}, the phrasing \enquote{partially ordered} is used to emphasize that there might exist elements $x$, $y\in X$ s.t. both $x\leq y$ and $y\leq x$ are wrong. These pairs are called \textit{incomparable}. If either $x\leq y$ or $y \leq x$ is true, then we say that the pair is \textit{comparable}. 
	\end{defn}
	
	\begin{exmp}
		Consider $X := \left\{\{1\}, \{2\}, \{1, 2\}\right\}$ with $\subset$ as partial ordering. Obviously, the elements $\{1\}$ and $\{2\}$ are incomparable. 
	\end{exmp}

	\begin{defn}[Chain, Upper Bound, Maximal Element]
	For preparing the Kuratowski-Zorn lemma, the following definitions come in handy: 
		\begin{enumerate}[label=\alph*)]
			\item A \textit{chain} $C$ is a partially ordered set where every pair of elements in $C$ is comparable.\ One might also say that $C$ is a \textit{totally ordered set}. 
			\item An \textit{upper bound} (if existent)  of a subset $S\subset X$, where $X$ is a partially ordered set, is an element $u\in X$ such that 
			\begin{align}
				s \leq u \ \forall s\in S. 
			\end{align}
			Since $S\subset X$, $S$ itself is a partially ordered set. 
			\item A \textit{maximal element} (if existent) of a partially ordered set $X$ is an element $m\in X$ such that 
			\begin{align}
				\text{if}\ m\leq x \ \text{for some}\ x\in X,\ \text{then}\ x=m.
			\end{align}
			This is equivalent to saying that there is no $x\in X$ such that $m\leq x$ and $x\ne m$. 
		\end{enumerate}
	\end{defn}

	\begin{remark}
		For an arbitrary partially ordered set $X$, a maximal element (if existent) does not have to be unique.\ For example, consider $X := \left\{ \{1\}, \{2\}, \{3\}, \{1, 2\} \right\}$ with $\subset$ as partial ordering. Both $\{3\}$ and $\{1, 2\}$ are maximal elements.\ However, if we consider chains, then maximal elements are indeed unique by definition. 
	\end{remark}

	\begin{theorem}[Kuratowski-Zorn Lemma]
		Let $\left(M, \leq\right)$ be a non-empty partially ordered set.\ If every chain $C\subset M$ has an upper bound, then $M$ has a maximal element. 
	\end{theorem}

	\begin{remark}
		The upper bound of every chain $C\subset M$ need not be in $C$, by definition of a chain, but it must be in $M$. 
	\end{remark}
	
	\newpage 
	\section{Differential Geometry}
	\begin{samepage}
		\begin{defn}[Smooth Atlas \cite{Lindemann-lec1}]
			Let $M$ be a second countable Hausdorff topological space. An \textit{$n$-dimensional smooth atlas on $M$}  is a collection of maps
			\begin{align*}
				\mathcal A = \left\{\left(\varphi_i, U_i\right)\mid i\in I\right\}, \quad \varphi_i: U_i \rightarrow \varphi_i(U_i) \subset \mathbb R^n
			\end{align*}
			such that all $U_i \subset M$ are open, all $\varphi_i$ are homeomorphisms, $I$ is an index set and 
			\begin{itemize}
				\item $\{U_i \mid i\in I\}$ is an open covering of $M$, 
				\item $\varphi_i \circ \varphi_j^{-1}: \varphi_j(U_i \cap U_j) \rightarrow \varphi_i(U_i \cap U_j)$ are smooth $\forall i$, $j \in I$. 
			\end{itemize}
			The tuples $(\varphi_i, U_i)$, $i\in I$, are so-called \textit{charts} on $M$, the maps $\varphi_i\circ \varphi_j^{-1}$ are called \textit{transition maps} or \textit{changes of coordinates} and $n$ is the \textit{dimension} of $M$.  \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\begin{tikzpicture}
				\draw (0,0) circle (0.15cm);
			\end{tikzpicture}	
		\end{defn}
	\end{samepage}

	\begin{remark}
		To see why the domain of the transition maps $\varphi_i \circ \varphi_j^{-1}$ is $\varphi_j\left(U_i \cap U_j\right)$, note that the expression $\varphi_i\left(\varphi_j^{-1}\left(x\right)\right)$ only makes sense if 
		\begin{align*}
			\left(x\in \varphi_j(U_j)\right) \wedge  \left(\varphi_j^{-1}\left(x\right)\in U_i\right) \Rightarrow \left(x\in \varphi_j(U_j)\right) \wedge \left(x\in \varphi_j\left(U_i\right)\right) \Rightarrow x\in \varphi_j(U_j \cap U_i). 
		\end{align*}
		Similarly, we can convince ourselves that the codomain of the transition maps $\varphi_i\circ\varphi_j^{-1}$ is given by $\varphi_i(U_i\cap U_j)$. Since $x\in \varphi_j(U_j)$, it follows that $\varphi_j^{-1}(x)\in U_j$. In addition, due to the domain of the homeomorphism $\varphi_i$, it must hold that $\varphi_j^{-1}(x) \in U_i$. Thus: 
		\begin{align*}
			\left(\varphi_j^{-1}(x)\in U_j\right) &\wedge \left(\varphi_j^{-1}(x) \in U_i\right) 
			\\
			\Rightarrow 			\left(\varphi_i(\varphi_j^{-1}(x))\in \varphi_i(U_j)\right) &\wedge \left(\varphi_i(\varphi_j^{-1}(x))\in \varphi_i(U_i)\right) 
			\\ 
			\Rightarrow \varphi_i\left(\varphi_j^{-1}(x)\right) &\in \varphi_i\left(U_i\cap U_j\right). 
		\end{align*}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	\end{remark}
	
	\begin{defn}[Equivalence of Atlases]
		Let $M$ be a second countable Hausdorff topological space. Two atlases $\mathcal A$ and $\mathcal B$ on $M$ are called \textit{equivalent} if $\mathcal A\cup \mathcal B$ is an atlas on $M$.  
	\end{defn}

	\begin{remark}
		To see that not all atlases are equivalent to each other, consider $M = \mathbb R$ (which is a second countable Hausdorff topological space). Consider the atlases $\mathcal A = \{ (\varphi, M) \}$ with $\varphi: M\rightarrow M$, $x\mapsto x$ and $\mathcal B = \{(\psi, M)\}$ with $\psi: M\rightarrow M$, $x\mapsto x^3$. The atlases are not equivalent, since $\varphi\circ \psi^{-1}: M \rightarrow M$, $x\mapsto \sqrt[3]{x}$ is not smooth (the derivative is not continuous). 
	\end{remark}
	
	\section{Normalizing Flows}
	\begin{defn}\label{defn-diffeomorphism}
		Let $M$, $N$ be two manifolds, $g: M\rightarrow N$ a differentiable map. If $g$ is a bijection and its inverse $g^{-1}: N\rightarrow M$ is differentiable as well, then we call $f$ a \textit{diffeomorphism}. We talk of a $C^k$ \textit{diffeomorphism} if both $g$ and $g^{-1}$ are $k$-times continuously differentiable. 
	\end{defn}
	
	\begin{theorem}[Change of variables \cite{MfPIII}]\label{change-of-variables}
		Let $U$, $V\subset\mathbb{R}^n$ be open subsets and $T: U\rightarrow V$ a diffeomorphism, cf. Def.  \ref{defn-diffeomorphism}. Then the function $f: V\rightarrow \mathbb{C} \ \cup \{\infty\}$ is integrable over $V$ if and only if the function 
		\begin{align}
			\left(f\circ T\right) \cdot \left\vert \det \left( \frac{\partial T_{\mu}}{\partial x_{\nu}} \right)_{\mu\nu} \right\vert 
		\end{align}
		is integrable over $U$. In this case, it holds that 
		\begin{align}\label{change-of-variables-formula}
			\int_{U} \left(f\circ T\right)(x) \cdot \left\vert \det\left(\frac{\partial T_{\mu}}{\partial x_{\nu}}(x)\right)_{\mu, \nu} \right\vert dx = \int_{V} f(y)dy. 
		\end{align}
	\end{theorem}
	\begin{remark}\label{diffeomorphism-inverse}
		If $T$ is a diffeomorphism, then also $T^{-1}$ is a diffeomorphism, thus we could also have chosen $T^{-1}$ in the formulation of Theorem \ref{change-of-variables}.
	\end{remark}
	
	\begin{theorem}[Inverse function theorem \cite{IFT}] Let $g: \mathbb R^n \rightarrow \mathbb R^n$ be continuously differentiable on some open set $V\subset \mathbb R^n$ containing $\mathbf{a}$ and suppose $\det Jg(\mathbf{a}) \ne 0$, where $J$ shall be the Jacobi matrix of $g$. Then there is some open set containing $\mathbf{a}$ and an open set $W\subset \mathbb{R}^n$ containing $g(\mathbf{a})$ such that $g: V\rightarrow W$ has a continuous inverse $g^{-1}: W\rightarrow V$ which is differentiable for all $\mathbf{y}\in W$. 
		\\ \\	
		As matrices, we can write this as 
		\begin{align}
			J(g^{-1})(\mathbf{y}) = \left[\left(Jg\right)\left(g^{-1}(\mathbf{y})\right)\right] ^{-1}
		\end{align} 
	\end{theorem}
	
	\begin{remark} An example for a function that is invertible and continuously differentiable but not a diffeomorphism is $g: \mathbb{R}\rightarrow\mathbb{R}$, $x\mapsto x^3$. Its inverse is obviously $g^{-1}: \mathbb{R}\rightarrow\mathbb{R}, x\mapsto \sqrt[3]{x}$, cf. \cite{cube-root} for a nice plot. However, $\frac{dg^{-1}}{dx}|_{x = 0}$ does not exist. The reason is that $\det Jg(0) = 0$ and hence the inverse function theorem does not apply. 
	\end{remark}
	\begin{itemize}
		\item Let $\mathbf{U}$ be a random variable and let $p(\mathbf{U})$ describe the probability distribution of it, e.g. a uniform distribution between $0$ and $1$. We now make a simple transformation and obtain a new random variable $\mathbf{X}$, where we again denote by $p(\mathbf{X})$ the probability distribution of $\mathbf{X}$. We obtain $\mathbf{X}$ in the following way: 
		\begin{align}\label{normalizing-flow-forward}
			p(\mathbf{X}) = p(\mathbf{U})\left\vert \det\left(\frac{\partial\mathbf{f}}{\partial \mathbf{U}}\right)\right\vert^{-1}, 
		\end{align}
		where $\mathbf{f}$ denotes an invertible (and hence bijective) mapping. 
		
		\item Without proof, it holds that 
		\begin{align}
			\left\vert \det\left(\frac{\partial\mathbf{f}}{\partial\mathbf{U}}\right)\right\vert^{-1} = \left\vert \text{det}\left(\frac{\partial\mathbf{f}^{-1}}{\partial\mathbf{U}}\right) \right\vert 
		\end{align}
		and thus we can rewrite Eq. \eqref{normalizing-flow-forward} as 
		\begin{align}\label{normalizing-flow-backward}
			p(\mathbf{U}) = p(\mathbf{X})\left\vert \det\left(\frac{\partial\mathbf{f^{-1}}}{\partial \mathbf{U}}\right)\right\vert^{-1}. 
		\end{align}
		Since we assumed $\mathbf{f}$ to be invertible, $\mathbf{f^{-1}}$ is well-defined. 
		
		\item In practice, we will want $\mathbf{f}$ to be both invertible \underline{and} to have a \textbf{tractable} Jacobian, i.e. a Jacobian that we can easily calculate. For $\mathbf{f}$ to have a Jacobian at all, each of its first-order partial derivatives must exist \cite{jacobi-matrix}.
		So-called \textit{autoregressive flows} have the property that their Jacobian is an \underline{upper triangular matrix}. For an upper triangular matrix, it holds that its determinant is given by the product of its diagonal elements \cite{triangular-matrices}.
	\end{itemize}

	\begin{defn}[Determinant]
		Let $D$ be an $n\times n$ matrix and let $S_n$ denote the symmetric group over $n$. Then the determinant of $D$ is defined as:  
		\begin{align}
			\det(D) := \sum_{\sigma\in S_n} \left( \text{sgn}(\sigma) \prod_{i = 1}^{n}a_{i, \sigma(i)} \right),  
		\end{align}
		cf. \cite{leibniz-formula}. 
	\end{defn}

	\begin{lemma}
		Let $A$ be a $k\times k$, $0$ an $k\times n$, $C$ an $n\times k$ and $D$ an $n\times n$ matrix; then 
		\begin{align}
			\det\left( \begin{pmatrix}	A & 0 \\ C & D \end{pmatrix} \right) = \det(A)\det(D). 
		\end{align}
	\end{lemma}
	\noindent \textit{Proof.} Define 
	\begin{align}
		B := \begin{pmatrix} A & 0 \\ C & D
		\end{pmatrix}. 
	\end{align}
	Clearly, 
	\begin{align}\label{case-diff}
		b_{i,j} = 
		\begin{cases}
			a_{i, j} \qquad &i, j\leq k, \\
			0 		 \qquad &i\leq k, j \geq k+1, \\
			c_{i-k, j} \qquad &i\geq k+1, j \leq k, \\
			d_{i-k, j-k} \qquad &i, j\geq k+1. 
		\end{cases}
	\end{align}
	We can write the determinant of $B$ as 
	\begin{align}
		\det(B) = \sum_{\sigma\in S_{n+k}} \text{sgn}(\sigma)\prod_{i = 1}^{n+k}b_{i, \sigma(i)}. 
	\end{align}
	From Eq. \eqref{case-diff} we know that all summands of the form $\sigma(i) = j$ with $i\leq k$, $j\geq k+1$ are $0$. Therefore, we can consider all permutations of the form $\sigma(i) = j$ with $i, j \leq k$ or $\sigma(i) = j$ with $i\geq k+1$, $j\leq k$. We can also write this in the form $\sigma(i) = \pi(i)$ for $i\leq k$ and $\sigma(k + i) = k + \tau(i)$ for $1\leq i \leq n$, where $\pi\in S_k$ and $\tau\in S_n$. Denote the set of all such permutations by $\tilde{S}_{k+n}$.  Thus: 
	\begin{align}
		\det(B) &= \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{n+k}b_{i, \sigma(i)}
		\\ &= \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{k}b_{i, \sigma(i)}\prod_{i = k+1}^{n+k}b_{i, \sigma(i)}
		\\ &\overset{\tiny\eqref{case-diff}}{=} \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{k}a_{i, \sigma(i)}\prod_{i=k+1}^{n+k}d_{i-k, \sigma(i)-k}
		\\ &= \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{k}a_{i, \sigma(i)}\prod_{i = 1}^{n}d_{i, \sigma(i+k)-k}
		\\ &= \sum_{\pi\in S_k, \tau\in S_n}\text{sgn}(\pi)\text{sgn}(\tau)\prod_{i = 1}^{k}a_{i, \pi(i)}\prod_{i = 1}^{n}d_{i, \tau(i)}
		\\ &= \sum_{\pi\in S_k}\text{sgn}(\pi)\prod_{i = 1}^{k}a_{i, \pi(i)}\sum_{\tau\in S_n}\text{sgn}(\tau)\prod_{i = 1}^{n}d_{i, \tau(i)}
		\\ &= \det(A)\det(D)
	\end{align}
	\cite{block-triangular-matrix}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}
		Let $h(\bm{\cdot}\ ; \theta): \mathbb{R}\rightarrow\mathbb{R}$ be a bijection parametrized by $\theta$. Then an \textit{autoregressive model} is a function \begin{align}
			g: \mathbb{R}^D\rightarrow\mathbb{R}^D, \begin{pmatrix}
				x_1 \\ \dots \\ x_D 
			\end{pmatrix} \mapsto 
			\begin{pmatrix}
				h\left(x_1; \Theta_1\right) \\ h\left(x_2; 	\Theta_2\left(x_1\right)\right) \\ \dots \\ h\left(x_D; \Theta_D\left(x_1, \dots, x_{D-1}\right)\right)
			\end{pmatrix}
		\end{align}
		The functions $\Theta_t$ for $t = 2$, $\dots$, $D$ are arbitrary functions whose domain is $\mathbb{R}^{t-1}$, $\Theta_1$ is a constant. 
	\end{defn}

	\begin{remark}
		The Jacobian matrix of an autoregressive flow is given as follows: 
		\begin{align}
			Dg &= \begin{pmatrix}
				\partial g_1/\partial x_1 & \partial g_1/\partial x_2 & \dots & \partial g_1/\partial x_D 
								\\ 
				\partial g_2/\partial x_1 & \partial g_2/\partial x_2 & \dots & \partial g_2/\partial x_D
								\\
				\vdots & \vdots & \ddots & \vdots 
								\\ 
				\partial g_D/\partial x_1 & \partial g_D / \partial x_2 & \dots & \partial g_D / \partial x_D
			\end{pmatrix} 
		\end{align}
	One can easily convince oneself that $Dg$ is a lower triangular matrix. 
	\end{remark}

	\begin{theorem}
		Normalizing flows in $\mathbb{R}^D$ come from the push-forward of a measure. 
	\end{theorem}
	\noindent \textit{Proof.} Let $\mathcal Y$, $\mathcal Z\subset\mathbb{R}^D$ be open,  $\Sigma_{\mathcal Y} = \mathcal B(\mathcal Y)$, $\Sigma_{\mathcal Z} = \mathcal B(\mathcal Z)$ and $g: \mathcal Z\rightarrow\mathcal Y$ be a diffeomorphism. The function $g: \mathcal Z \rightarrow \mathcal Y$ is measurable if and only if $g^{-1}(G)\in \Sigma_{\mathcal Z}$ for every set $G$ that is open in $\mathcal Y$, cf. Theorem \ref{generator-and-measurable-function} (Borel $\sigma$-algebras are generated by the open sets). Since for functions between two topological spaces it holds that they are continuous if and only if the inverse image of an open set is again open, we have that $g$ is indeed measurable. 
	\\ \\ Now define the probability measure $\mu$ on the measurable space $(\mathcal Z, \Sigma_{\mathcal Z})$ as 
	\begin{align}\label{prob-measure-proof}
		\mu(I) := \int_{I} p_{\mathbf{Z}}(z)d\lambda(z) \ \forall I\in \Sigma_{\mathcal Z}, 
	\end{align}
	where $p_{\mathbf{Z}}: \mathcal Z \rightarrow\mathbb{R}$ shall be a PDF and $\lambda$ the Lebesgue measure. (The Lebesgue measure is defined on the completion of $\mathcal B(\mathbb R^D)$, which is \enquote{larger} than $\mathcal B(\mathbb R^D)$.) By considering the push-forward of $\mu$ under the measurable map $g$, we have $\forall J \in \Sigma_{\mathcal Y}$: 
	\begin{align}
		g_{\star}\mu\left(J\right) = \mu(g^{-1}(J))  \overset{\tiny\eqref{prob-measure-proof}}{=} \int_{g^{-1}(J)} p_{\mathbf{Z}}(z)d\lambda(z)
	\end{align}
	Since by assumption $g:\mathcal Z\rightarrow \mathcal Y$ and therefore also the inverse $g^{-1}: \mathcal Y \rightarrow \mathcal Z$ are a diffeomorphism, we can use the change of variables formula from Theorem \ref{change-of-variables}, since we assume $p_{\mathbf{Z}}$ to be integrable over $\mathcal Z$. We apply Theorem \ref{change-of-variables} to $g^{-1}$ instead of $g$, cf. Remark \ref{diffeomorphism-inverse}: 
	\begin{align}
		g_{\star}\mu(J) = \int_{g^{-1}(J)}p_{\mathbf{Z}}(z)d\lambda(z) = \int_{J} \underbrace{\left(p_{\mathbf{Z}}\circ g^{-1}\right)(y) \cdot \left\vert \det Dg^{-1}(y) \right\vert }_{= p_{\mathbf{Y}}(y)} d\lambda(y) 						
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}
		A rational-quadratic function takes the form of a quotient of two quadratic polynomials. 
		\begin{align}
			\frac{\alpha^{(k)}\left(\xi\right)}{\beta^{(k)}(\xi)} = \frac{a_0\xi^2 + a_1\xi + a_2}{b_0\xi^2 + b_1\xi + b_2} 
		\end{align}
	\end{defn}	

\newpage 

\section{Diffusion-Based Models}
	
	\begin{remark}[ELBO VAEs]
		One can easily show that the (marginal) log-likelihood of the data is given as
		\cite{cs_231n_lec_13}
		\begin{align}\label{eq:elbo__marginal_LL}
			\log p(x) = \text{ELBO} + D_{\text{KL}}\left[ q_{\phi}(z\vert x) \vert\vert p(z\vert x)\right], 
		\end{align}
		
		\noindent where $p(z\vert x)$ is the true posterior and ELBO is the usual evidence lower bound, i.e.
		\begin{align}
			\text{ELBO} = \mathbb E_{q_{\phi}(z\vert x)}\left[ \log p_{\psi}(x\vert z) \right] - D_{\text{KL}}\left[ q_{\phi}(z\vert x) \vert\vert p(z) \right]
		\end{align}
		
		\noindent Eq.~\eqref{eq:elbo__marginal_LL} shows that since the (marginal) log-likelihood is not parameterized by any NN parameters, maximizing the ELBO necessary leads to a lower $D_{\text{KL}}\left[ q_{\phi}(z\vert x) \vert\vert p(z\vert x)\right]$, which is the reverse KLD. 
		
	\end{remark}

\begin{lemma}[ELBO]
	Let $q(x_{0})$ denote the true (unknown) distribution of a real image $x_{0}$, and let $p_{\theta}(x_{0})$ be the model's approximation to $q(x_{0})$, then we have the following ELBO-like loss: 
	\begin{align}\label{elbo_diffusion_models}
		\mathbb E_{q(x_{0})}\left[\log p_{\theta}(x_{0})\right] \geq -\mathbb E_{q(x_{0}, \dots, x_{T})}\left[\log \frac{q(x_{1}, \dots, x_{T} \mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right].    
	\end{align}
\end{lemma}

\begin{proof}
\cite{lilian_weng}
	\begin{align}
		\log p_{\theta}(x_{0}) &\geq \log p_{\theta}(x_{0}) - D_{\text{KL}}\left[ q(x_{1}, \dots, x_{T}\mid x_{0})\mid\mid p_{\theta}(x_{1}, \dots, x_{T}\mid x_{0}) \right]
		\\[4pt] &= \log p_{\theta}(x_{0}) - \mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{1}, \dots, x_{T} \mid x_{0})}\right]
		\\[4pt] &= \log p_{\theta}(x_{0}) - \mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})/ p_{\theta}(x_{0})}\right]
		\\[4pt] &= -\mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right]
		\\[4pt] \Rightarrow \mathbb E_{q(x_{0})}\left[\log p_{\theta}(x_{0})\right] &\geq -\mathbb E_{q(x_{0})}\mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right]
	\end{align}
	Assuming that the assumptions of Fubini's theorem hold and from the monotonicity of the expectation, we have: 
	\begin{align}\label{diff_elbo_to_be_proved}
		\mathbb E_{q(x_{0})}\left[ \log p_{\theta}(x_{0}) \right] \geq \mathbb E_{q(x_{1}, \dots,  x_{T} \mid x_{0})q(x_{0})}\left[\log \frac{q(x_{1}, \dots, x_{T} \mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right] = E_{q(x_{0}, \dots,  x_{T} \mid x_{0})}\left[\log \frac{q(x_{1}, \dots, x_{T} \mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right].
	\end{align}
\end{proof}

\begin{defn}[Wiener process]
	Let $W_{t}$ be a real-valued continuous-time stochastic process.\ It is said to be a \textit{Wiener process} if the following properties hold: 
	\begin{itemize}
		\item $W_{0} = 0$, 
		\item $W$ has independent increments, i.e.\ $\forall t > 0:$, the terms $W_{t+u} - W_{t}$, $u\geq 0$ are independent of past values $W_{s}$, $s\leq t$, 
		\item $W$ has Gaussian increments:\ $W_{t+u} - W_{t} \sim \mathcal N(0, u)$, 
		\item $W$ has continuous paths, i.e.\ $\forall t$, $W_{t}$ is continuous in $t$. 
	\end{itemize}
	source:\ \url{https://en.wikipedia.org/wiki/Wiener_process}
\end{defn}

\begin{remark}
	If $\xi_{1}$, $\xi_{2}$, $\dots$ be i.i.d random variables with a mean of $0$ and standard deviation of $1$.\ For every $n$, define a continuous time stochastic process 
	\begin{align}\label{random_walk_Wiener}
		W_{n}(t) := \frac{1}{\sqrt{n}}\sum_{1\leq k\leq\lfloor nt \rfloor}\xi_{k} 
	\end{align}
	This is what makes Wiener processes so powerful (and explains the ubiquity of Brownian motion).\ According to Donsker's theorem, the  above expression becomes a Wiener process.
\end{remark}

\begin{defn}[Globally Lipschitz continuous]
	
	
\end{defn}

\newpage 

\section{Miscellaneous}

	\begin{lemma}[Chain rule for KL-divergences]
		Let $p(x, y)$ and $q(x, y)$ be two arbitrary PDF's. Then the following hods:
		
		\begin{align}
			D_{\text{KL}}(p(x, y) \vert\vert q(x, y)) = D_{\text{KL}}(p(x)\vert\vert q(x)) + D_{\text{KL}}(p(y\vert x) \vert\vert q(y\vert x))
		\end{align}
		
		\begin{proof}
			Brute-force calculation yields:
			\begin{align}
				D_{\text{KL}}(p(x, y) \vert\vert q(x, y)) &= \int\int p(x, y)\log\frac{p(x, y)}{q(x, y)}d\lambda(x)d\lambda(y)
				\\[4pt] &= \int\int p(x, y)\log\frac{p(x)p(y\vert x)}{q(x)q(y\vert x)}d\lambda(x)d\lambda(y)
				\\[4pt] &= \int\int p(x, y)\log\frac{p(x)}{q(x)}d\lambda(x)d\lambda(y) + \int\int p(x, y)\log\frac{p(y\vert x)}{q(y\vert x)}d\lambda(x)d\lambda(y)
				\\[4pt] &= D_{\text{KL}}(p(x) \vert\vert q(x)) + \int p(x)d\lambda(x)\int p(y\vert x)\log\frac{p(y\vert x)}{q(y\vert x)}d\lambda(y)
				\\[4pt] &= D_{\text{KL}}(p(x) \vert\vert q(x)) + D_{\text{KL}}(p(y\vert x) \vert\vert q(y\vert x))
			\end{align}
		\end{proof}
	\end{lemma}
	
	\begin{defn}[Mutual information]
		Let $(X, Y)\sim P_{\left(X, Y\right)}\in \mathcal P(\mathcal X\times \mathcal Y)$, where $\mathcal P(\mathcal X\times \mathcal Y)$ is the space of all probability measures over the space $\mathcal X\times \mathcal Y$. The mutual information between the random variables $X$ and $Y$ is now defined as 
		\cite[Def.~10.1]{ece_ece_5630_lectures10}:
		
		\begin{align}
			I(X; Y) := D_{\text{KL}}\left(P_{(X, Y)} \vert\vert P_{X} \otimes P_{Y}\right),
		\end{align}
	
		\noindent where $P_{X}$ and $P_{Y}$ are the marginal measures of the coupling measure $P_{(X, Y)}$ and $P_{X}\otimes P_{Y}$ is the induced \textbf{product measure}.
	\end{defn}

	\begin{defn}[Divergence]
		Let $p$ and $q$ be two probability distributions, then a divergence $D$ must satisfy:~$D(p\vert\vert q) \geq 0$ $\forall p,q$ and $D(p\vert\vert q) = 0 \Leftrightarrow p = q$ a.e. Note that the triangle inequality and the symmetry property need not be satisfied in general.
	\end{defn}

	\begin{exmp}[$f$-Divergence]
		Let $P$ and $Q$ be two probability measures defined on the $\sigma$-algebra over a space $\Omega$ such that $P \ll Q$, i.e.~$P$ is absolutely continuous wrt $Q$. Then, for a convex function $f:[0, \infty)\rightarrow \mathbb R$ s.t.
		
		\begin{enumerate}[label=(\roman*)]
			\item $f(1) = 0$, 
			\item $f$ is strictly convex at $x = 1$,
		\end{enumerate}
	
		\noindent the $f$-divergence is now defined as \cite{ece_ece_5630_lectures6}, 
		\begin{align}\label{eq:def__f_divergence}
			D_{f}(P\vert\vert Q) := \int_{\Omega} f\left( \frac{dP}{dQ} \right)dQ, 
		\end{align}
		where $dP/dQ$ is the Radon-Nikodym derivative.
	\end{exmp}

	\begin{proof}
		\begin{enumerate}
			\item \textbf{Non-negativity}:~We know that 
			\begin{align}
				\mathbb E_{Q}\left[ \frac{dP}{dQ} \right] = \int_{\Omega} \frac{dP}{dQ} dQ = 1.
			\end{align}
			
			Since $f$ is a convex function, by Jensen's inequality,
			\begin{align}
				0 = f(1) = f\left(\mathbb E_{Q}\left[ \frac{dP}{dQ} \right]\right) \leq \mathbb E_{Q}\left[ f\left( \frac{dP}{dQ} \right) \right] = \int_{\Omega} f\left( \frac{dP}{dQ} \right) dQ \overset{\scriptsize\eqref{eq:def__f_divergence}}{=} D_{f}(P\vert\vert Q).
			\end{align}
			
			\item \textbf{Zero iff equal}:~If $P = Q$ a.e., then $dP / dQ = 1$ a.e., and hence
			\begin{align}
				D_{f}(P\vert\vert Q) = \int_{\Omega} f(1)dQ = \int_{\Omega} 0dQ = 0.
			\end{align}
			
			Conversely, if $D_{f}(P\vert\vert Q) = 0$, since $f$ is strictly convex at $1$ and convex everywhere else, this implies that the only minimum point of $f$ is at $f(1) = 0$. Therefore, the integrand of Eq.~\eqref{eq:def__f_divergence}, i.e.~$f\left(\frac{dP}{dQ}\right)$, must be $0$ a.e., implying that 
			\begin{align}
				\frac{dP}{dQ} = 1\Rightarrow P = Q.
			\end{align}
			
		\end{enumerate}
	\end{proof}


\appendix 
\include{appendix_A.tex}




\newpage 
\printbibliography

\end{document} 