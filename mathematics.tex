\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{comment, listings, centernot, amssymb, hyperref, graphicx, amsmath, textcomp, breqn, mathtools, csquotes, cancel, enumitem, amsthm, caption, bm, tikz}
\usepackage[backend=biber]{biblatex}
\usepackage[english]{babel}
\addbibresource{references.bib}

% NEW (16-04-21, 19p49)
\usepackage[margin = 0.6in]{geometry}
\numberwithin{equation}{section}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section] % reset theorem numbering for each chapter

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers
\newtheorem{lemma}[thm]{Lemma} % same for example numbers
\newtheorem{remark}[thm]{Remark} % same for example numbers
\newtheorem{theorem}[thm]{Theorem} 
\newtheorem{corollary}[thm]{Corollary}
\newcommand{\norm}[2]{\left\vert\left\vert #1 \right\vert\right\vert_{#2}}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand{\inner}[1]{\left\langle #1 \right\rangle}
\newcommand{\epi}[1]{\text{epi($ #1 $) } }
\renewcommand{\qedsymbol}{$\blacksquare$}

\title{Mathematics}
% \subtitle{Questions}
\author{Imahn Shekhzadeh}
\date{\today}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage 
	
	\section{Algebra}
	
	\begin{defn}[Action]\label{defn:algebra__group_action}
		Let $G$ be a group (where $e$ is the neutral element), and $X$ a set. Then a (left) group action $\alpha$ of $G$ on $X$ is a function
		\begin{align}
			\alpha: G\times X \rightarrow X
		\end{align}
		satisfying the following two axioms $(\forall g, h\in G) \wedge (\forall x\in X)$:
		\begin{align*}
			\alpha(e, x) &= x, 
			\\ \alpha(g, \alpha(h, x)) &= \alpha(gh, x)
		\end{align*}
		The group action is commonly written as $g\cdot x$ s.t.~the axioms become
		\begin{align}\label{eqn:left__group_action}
			e\cdot x &= x, 
			\\ g\cdot \left(h\cdot x\right) &= \left(gh\right)\cdot x
		\end{align}
		A right group action of $G$ on $X$ is a function
		\begin{align}
			\alpha: X\times G \rightarrow X
		\end{align}
		satisfying the following two axioms $(\forall g, h\in G) \wedge (\forall x\in X)$:
		\begin{align}
			\alpha(x, e) &= x, 
			\\ \alpha(\alpha(x, g), h) &= \alpha(x, gh)
		\end{align}
	\end{defn}
	
	\begin{exmp}
		Consider $G = \text{SO}(2)$, i.e.~the group of all rotations in $\mathbb R^2$, with the group operation given by the composition of rotations, and let $X = S^1$ be the unit circle in $\mathbb C$. Now consider the group action
		\begin{align}
			\Phi:\text{SO}(2) \times S^{1} \rightarrow S^{1}, (R_{\theta}, p) \mapsto R_{\theta}(p)
		\end{align}
		This is a group action on $X = S^{1}$, since 
		\begin{align*}
			\Phi(R_{\theta_1}, \Phi(R_{\theta_2}, p)) = \Phi(R_{\theta_1 + \theta_2}, p).
		\end{align*}
		In the short notation, one would write this as
		\begin{align*}
			R_{\theta_1}\cdot \left( R_{\theta_2} \cdot p \right) = R_{\theta_1 + \theta_2} \cdot p.
		\end{align*}
	\end{exmp}
	
	\begin{defn}[Signals\footnote{\cite[p.~11]{bronstein2021geometric}}] 
		Let $V$ be a vector space\footnote{In computer vision, its dimensions would be called \textit{channels}.} and $\Omega$ a set, possibly with additional structure. Then the space of $V$-valued signals on $\Omega$,
		\begin{align}
			\mathcal X(\Omega, V) = \{ x: \Omega \rightarrow V \}
		\end{align}
		is a function space with a vector space structure. (Note that each $x\in \mathcal X(\Omega, V)$ is a signal.)
		
		Addition and scalar multiplication of signals are defined as 
		\begin{align}
			(\alpha x + \beta y)(u) := \alpha x(u) + \beta y(u) \quad \forall u\in\Omega\ \forall \alpha, \beta\in \mathbb R.
		\end{align}
	\end{defn}

	\begin{lemma}
		If $V$ is endowed with an inner product $\langle v, w\rangle_{V}$ and a measure $\mu$ on a $\sigma$-algebra defined on the set $\Omega$ (wrt which an integral can be defined), we have the following induced inner product on $\mathcal X(\Omega, V)$
		\begin{align}
			\langle x, y\rangle_{\mathcal X(\Omega, V)} := \int_{\Omega}\langle \underbrace{x(u)}_{\in V}, \underbrace{y(u)}_{\in V}\rangle_{V}d\mu(u)
		\end{align}
	\end{lemma}

	\begin{proof}
		Trivial.
	\end{proof}

	\begin{remark}
		The domain (set) $\Omega$ can be discrete, in which case $\mu$ can be chosen to be the counting measure, in which case the integral becomes a sum.
	\end{remark}

	\begin{exmp}
		Let $\Omega = \mathbb Z_{n}\times Z_{n}$, i.e.~a two-dimensional grid, and $x$ an RGB image, i.e.~a signal $x:\Omega\rightarrow \mathbb R^3$, and $f$ a function (such as a single-layer perceptron) operating on $3n^2$-dimensional inputs.
	\end{exmp}
	
	\begin{defn}[Homomorphism]
		Let $f:A\rightarrow B$ be a map between two sets $A$ and $B$ that are equipped with the same structure. Also assume that $\bm{\cdot}$ is an operation of the structure. Then $f$ is said to be a homomorphism if $\forall x, y\in A$,
		\begin{align}
			f(x\bm{\cdot} y) = f(x) \bm{\cdot} f(y).
		\end{align}
	\end{defn}
	
	\begin{defn}[Group representation]
		An $n$-dimensional real \textbf{representation} of a group $G$ is a map $\rho:G\rightarrow \text{GL}(n, \mathbb R)$ (correspondingly, one defines an $n$-dimensional complex representation) \cite[p.~15]{bronstein2021geometric}, where each $g\in G$ is mapped to an invertible matrix, under the \textit{condition} that the map is a homomorphism, i.e.~$\rho(gh) = \rho(g)\rho(h)$.
	\end{defn}

	\begin{exmp}[Group representation]
		Let $G = \mathbb Z_{n}$\footnote{Cyclic group of order $n$, $\mathbb Z_n = \{ 0, 1, \dots, n - 1 \}$.}, where the group operation is addition. The group representation 
		\begin{align}
			\rho: \mathbb Z_{n} \rightarrow \text{GL}(1, \mathbb C), g\mapsto \exp\left(\frac{2\pi ig}{n}\right)
		\end{align}
		To see that this is indeed a group representation, note that
		\begin{align}
			\rho(gh) = \rho(g + h) = \exp\left( \frac{2\pi igh}{n} \right) = \exp\left( \frac{2\pi i\left(g + h\right)}{n} \right) = \exp\left( \frac{2\pi ig}{n} \right)\exp\left( \frac{2\pi ih}{n} \right),
		\end{align}
		i.e.~the group representation is a homomorphism.
	\end{exmp}
	
	\begin{defn}[Equivariance]
		Let $\rho$ be a representation of a group $G$. A function $f:\mathcal X(\Omega, V) \rightarrow \mathcal X(\Omega, V)$ is $G$-equivariant if 
		\begin{align}\label{eqn:group_action}
			f(\rho(g)x) = \rho(g)f(x) \quad \forall g\in G.
		\end{align}
	\end{defn}
	
	\begin{remark}
		Note that $\rho(g)\in \text{GL}(n, \mathbb K)$, $x\in \mathcal X(\Omega, V)$, and hence $\rho(g)x$ is a group action mediated by the representation $\rho$ on the group element $g$, i.e.~$\rho(g)x\in \mathcal X(\Omega, V)$. Also note that $f(x)\in \mathcal X(\Omega, V)$, i.e.~$\rho(g)f(x)$ is a group action as well, where the group element $g$ -- mediated by the representation $\rho$ -- and hence $\rho(g)f(x) \in \mathcal X(\Omega, V)$.
		
		The fact that we have a group action implies according to Def.~\ref{defn:algebra__group_action} and Eq.~\eqref{eqn:group_action},
		\begin{align}
			f\left(\rho(g)(\rho(h)x)\right) \overset{\scriptsize \eqref{eqn:left__group_action}}{=} f\left( (\rho(g)\rho(h))x \right) = \left(\rho(g)\rho(h)\right)f(x).
		\end{align}
	\end{remark}
	
	\newpage
	
	\section{Measure Theory}
	\begin{defn}[Generated $\sigma$-algebra \cite{generated-sigma-algebras}]
		Let $X$ be a set and $\mathcal E\subset \mathcal P(X)$ a non-empty collection of subsets of $X$. The \textit{smallest} $\sigma$-algebra containing all the sets of $\mathcal E$ is denoted by $\sigma(\mathcal E)$. 
	\end{defn}

	\begin{corollary}
		Let $\mathcal E_1$, $\mathcal E_2\subset \mathcal P(X)$ be such that $\mathcal E_1 \subset \mathcal E_2$. Then $\sigma(\mathcal E_1) \subset \sigma(\mathcal E_2)$.    
	\end{corollary}
	
	\begin{defn}[Measurable function]
		Let $(X, \mathcal E)$ and $(Y, \mathcal F)$ be measurable spaces. A map $f: X\rightarrow Y$ is said to be \textit{$\mathcal E$-measurable} if 
		\begin{align} \label{measurable-mapping-eq}
			f^{-1}(\mathcal F) := \left\{ f^{-1}(A) \vert A\in \mathcal F \right\} := \left\{ \left\{ x\in X \vert f(x) \in A \right\} \vert A\in \mathcal F \right\} \subset \mathcal E.
		\end{align} 
	\end{defn}

	\begin{theorem}[Generator and measurable function \cite{measurable-functions}] \label{generator-and-measurable-function} Let $(X, \mathcal E)$ and $(Y, \mathcal F)$ be measurable spaces and $\mathcal F = \sigma(\mathcal G)$, i.e. $\mathcal F$ is the $\sigma$-algebra generated by a family $\mathcal G \subset \mathcal P(Y)$, where $\mathcal P(Y)$ denotes the power set of $Y$. Then $f: X\rightarrow Y$ is measurable if and only if 
		\begin{align}
			f^{-1}\left(G\right) \in \mathcal E \ \forall G\in\mathcal G. 
		\end{align}
	\end{theorem}
	\noindent \textit{Proof}. \enquote{$\Rightarrow$} Since $\mathcal F = \sigma(\mathcal G)$, it obviously holds that $\mathcal G\subset \mathcal F$ and therefore $f^{-1}(G)\in \mathcal E \ \forall G\in\mathcal G$ is ensured by $f$ being measurable. 
	\\ \\ 
	\enquote{$\Leftarrow$} Define the set $\mathcal M:= \left\{ B\subset Y \mid f^{-1}(B)\in \mathcal A \right\}$. First we want to convince ourselves that $\mathcal M$ is a $\sigma$-algebra on $Y$. 
	\begin{enumerate}
		\item $\emptyset \in \mathcal M$, since $f^{-1}(\emptyset) = \left\{ x\in X \mid f(x)\in \emptyset \right\} = \emptyset$. 
		
		\item Let $B\in \mathcal M$, then also $Y\backslash B\in \mathcal M$, since $f^{-1}\left(Y\backslash B\right) = f^{-1}(Y)\backslash f^{-1}(B)$, as can be easily shown by using the definition of the complement of a set. Since $f^{-1}(Y) = X$, it follows that $f^{-1}(Y\backslash B) = X\backslash f^{-1}(B)$. Since by assumption $B\in \mathcal M$ (and therefore $f^{-1}(B)\in \mathcal A$) and $\mathcal A$ itself is a $\sigma$-algebra, it follows that $X\backslash f^{-1}(B)\in\mathcal A$. 
		
		\item  Let $B_i \in \mathcal M$ for $i\in \mathbb N$, then also $\cup_{i\in\mathbb N}B_i\in \mathcal M$, since $$f^{-1}\left(\bigcup_{i\in\mathbb N}B_i\right) = \bigcup_{i\in\mathbb N}f^{-1}\left(B_i\right).$$ 
	\end{enumerate}
	Since $\mathcal M$ is a $\sigma$-algebra and since $\mathcal G\subset \mathcal M \Rightarrow \mathcal F = \sigma(\mathcal G)\subset \mathcal M = \sigma(\mathcal M) $, it follows that $f$ is measurable. 
	 \newline \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$

	\begin{lemma}[Push-forward measure]\label{push-forward-measure}
		Let $(X, \mathcal E)$ and $(Y, \mathcal F)$ be measurable spaces. Given a measurable map $f: X\rightarrow Y$ and a measure $\mu$ on $\mathcal E$, let $f_{\#}\mu$ be defined by 
		\begin{align}
			f_{\#}\mu(A) := \mu(f^{-1}(A)) \quad \forall A\in \mathcal F. 
		\end{align}
		$f_{\#}\mu$ is a measure on $\mathcal F$ and called the \textit{push-forward} of $\mu$ under $f$. 
	\end{lemma}

	\noindent 
	\textit{Proof}. Obviously, $f_{\#}\mu(\emptyset) = \mu(f^{-1}(\emptyset)) \overset{\footnotesize\eqref{measurable-mapping-eq}}{=} \mu(\emptyset) = 0$. Also, no matter what kind of set $A\in \mathcal F$ we take, since $\mu(A) \geq 0$, the same holds for $f_{\#}\mu(A)$. Finally, let $(A_n)_{n\in\mathbb{N}} \subset \mathcal F$ be a sequence of mutually disjoint sets, then: 
	\begin{align}
		&f_{\#}\mu\left(\bigcup_{n\in\mathbb N}A_n \right) 
					\\ &= 
		\mu\left(f^{-1}\left(\bigcup_{n\in\mathbb N}A_n\right) \right) 
					\\ &\overset{\eqref{measurable-mapping-eq}}{=}
		\mu\left( \left\{ x\in X \bigg\vert f(x) \in  \bigcup_{n\in\mathbb N}A_n \right\}\right) 
					\\ &= 		
		\mu\left( \bigcup_{n\in\mathbb N}\left\{ x\in X \vert f(x)\in A_n \right\} \right) 
					\\ &=
		\mu\left( \bigcup_{n\in\mathbb{N}}f^{-1}(A_n) \right)
					\\ &=
		\sum_{n\in\mathbb N}\mu\left(f^{-1}(A_n)\right)
					\\ &=
		\sum_{n\in\mathbb{N}}f_{\#}\mu(A_n) 
	\end{align} 
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{corollary}[Push-forward of a probability measure]
		Let $(X, \mathcal E)$ and $(Y, \mathcal F)$	be measurable spaces. Given a measurable map $f: X\rightarrow Y$ and a \underline{probability} measure on $\mathcal E$, the push-forward of $\mu$ under $f$, denoted by $f_{\#}\mu$, is also a probability measure. 
	\end{corollary}
	\noindent\textit{Proof}. Since $\mu$ is in particular a measure and thus $f_{\#}\mu$ is also a measure, we only need to show that  
	\begin{align}
		f_{\#}\mu(Y) = \mu\left(f^{-1}\left(\mu\right)\right) = \mu\left(\left\{ x\in X \mid f(x)\in Y  \right\}\right) = \mu(X) = 1. 
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}[$\sigma$-finite measure] Let $(X, \mathcal A)$ be a measurable space and $\mu$ a measure on it. If there are sets $A_1$, $A_2$, $\dots \in \mathcal A$ with $\mu(A_n) < \infty \ \forall n\in \mathbb N$ that satisfy 
	\begin{align}
		\bigcup_{n\in\mathbb N}A_n = X
	\end{align}  
	then we say $\mu$ is \textit{$\sigma$-finite}. 
	\end{defn}

	\begin{remark}
		Obviously, every finite measure is $\sigma$-finite; however, the converse does not necessarily hold \cite{sigma-finite}. 
	\end{remark}
	
	
	\begin{defn}[Absolutely continuous measures.] 
	Let $\mu$ and $\nu$ be two measures on a $\sigma$-algebra $\mathcal A$. $\nu$ is called \textit{absolutely continuous} w.r.t. $\mu$, written as 
		\begin{align}
			\nu \ll \mu, 
		\end{align}
	if for each $A\in \mathcal A$, $\mu(A) = 0$ implies $\nu(A) = 0$. If $\mu$ and $\nu$ are both absolutely continuous w.r.t each other $\mu$ and $\nu$ are called \textit{equivalent}. 
	\end{defn}

	\begin{theorem}[Radon-Nikodym Theorem \cite{measure-integration}]
	Let $\mu$ be a $\sigma$-finite measure on a measurable space ($S$, $\mathcal{A}$). Then it is equivalent: 
		\begin{enumerate}
			\item $\nu \ll \mu$, 
			\item $d\nu = hd\mu$ for some measurable function $h: S\rightarrow \mathbb R_{+}$. 
		\end{enumerate}
	The density $h$ then is $\mu$-a.e. finite and $\mu$-a.e. unique. 
	\end{theorem}
	
	\begin{lemma}[Frechét Inception Distance]
		For two multivariate Gaussian distributions $\mathcal G(\mu_{x}, \Sigma_{x})$, $\mathcal G(\mu_{y}, \Sigma_{y})$, the \textit{Frechét Inception Distance} (FID) is defined as [\url{https://arxiv.org/pdf/1706.08500.pdf}]: 
		\begin{align}
			d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) := \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \text{Tr}\left(\Sigma_{x} + \Sigma_{y} - 2\left(\Sigma_{x}\Sigma_{y}\right)^{\frac{1}{2}}\right)}. 
		\end{align}
		It is a metric.\
	\end{lemma}
	\begin{proof}
		Clearly, $d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{x}, \Sigma_{x})\right) = 0$.\ Also, $d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) = d\left(\mathcal G(\mu_{y}, \Sigma_{y}), \mathcal G(\mu_{x}, \Sigma_{x})\right)$, which holds because $\text{Tr}(AB) = \text{Tr}(BA)$ for any matrices $A$ and $B$.\ To see that $d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) \geq 0$, note that $\text{Tr}(\Sigma_{x} + \Sigma_{y} - 2\left(\Sigma_{x}\Sigma_{y}\right)^{1/2}) = \text{Tr}\left(\left(\Sigma_{x}^{1/2} - \Sigma_{y}^{1/2}\right)^{2}\right) \geq 0$, since the covariance matrices contain the variances on the diagonal, which are obviously non-negative.\ It now remains to be shown that also the triangle inequality is fulfilled.\ For this, note that 
		\begin{align}
			d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \text{Tr}\left(\Sigma_{x} + \Sigma_{y} - 2\left(\Sigma_{x}\Sigma_{y}\right)^{\frac{1}{2}}\right)} 
			\\ &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \text{Tr}\left(\left(\Sigma_{x}^{1/2} - \Sigma_{y}^{1/2}\right)^{2}\right)}
			\\ &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y} \right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{y}\right\vert\right\vert_{2}^{2}}, 
		\end{align}
		where $\sigma_{x}$ and $\sigma_{y}$ denote vectors containing the standard deviations of the two Gaussian distributions.\ Clearly, 
		\begin{align}
			d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{z}, \Sigma_{z})\right) &= \sqrt{\left\vert\left\vert \mu_{x} - \mu_{z} \right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{z}\right\vert\right\vert_{2}^{2}} 
			\\ &\leq \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \mu_{y} - \mu_{z}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{y}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{y} - \sigma_{z}\right\vert\right\vert_{2}^{2}} 
			\\ &\leq \sqrt{\left\vert\left\vert \mu_{x} - \mu_{y}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{x} - \sigma_{y}\right\vert\right\vert_{2}^{2}} \ + \ \sqrt{\left\vert\left\vert \mu_{y} - \mu_{z}\right\vert\right\vert_{2}^{2} + \left\vert\left\vert \sigma_{y} - \sigma_{z}\right\vert\right\vert_{2}^{2}} 
			\\ &= d\left(\mathcal G(\mu_{x}, \Sigma_{x}), \mathcal G(\mu_{y}, \Sigma_{y})\right) + d\left(\mathcal G(\mu_{y}, \Sigma_{y}), \mathcal G(\mu_{z}, \Sigma_{z})\right), 
		\end{align}
		since $\sqrt{x+y} \leq \sqrt{x} + \sqrt{y}$, as one can directly show by squaring for non-negative $x$, $y\in\mathbb R$.\ 
	\end{proof}

	\begin{defn}[Convergence in probability]
		Assume we have a sequence of random variables $(X_{n})_{n\in \mathbb N}$, defined on a probability space $(\Omega, \mathcal F, \mathbb P)$.\ We say this sequence converges to another random variable $X$ if 
		\begin{align}
			\forall \epsilon > 0: \lim\limits_{n\rightarrow\infty}\mathbb P\left\{ \left\vert X_{n} - X\right\vert > \epsilon \right\} = 0. 
		\end{align}
	\end{defn}
	
	\newpage 
	\section{Functional Analysis}
	\begin{defn}[Inner Product Space]
		Let $\mathbb K$ be a field ($\mathbb K = \mathbb R$ or $\mathbb K = \mathbb C$). An inner product space is a vector space $V$ over $\mathbb K$ that allows for an \textbf{inner product} 
		\begin{align}
			\left\langle \bm{\cdot}, \bm{\cdot}\right\rangle: V\times V\rightarrow \mathbb K
		\end{align}
		satisfying the following properties $\forall \alpha, \beta\in \mathbb K; x, y, z\in V$:
		\begin{enumerate}
			\item \textbf{Conjugate symmetry}:
			\begin{align}
				\langle x, y\rangle = \overline{\langle y, x\rangle},
			\end{align}
			which implies that $\langle x, x\rangle\in \mathbb R$, even if $\mathbb K = \mathbb C$.\footnote{Set $y = x$.} 
			
			For $\mathbb K = \mathbb R$, conjugate symmetry is exact symmetry.
			
			\item \textbf{Linearity} (in the first argument): 
			\begin{align}
				\langle \alpha x + \beta y, z\rangle = \alpha\langle x, z\rangle + \beta\langle y, z\rangle
			\end{align}
			From the conjugate symmetry property, this means that we have semi-linearity in the second argument:
			\begin{align}
				\langle x, \alpha y + \beta z\rangle = \overline{\langle \alpha y + \beta z, x \rangle} = \overline{\alpha\langle y, x \rangle + \beta\langle z, x \rangle} = \overline{\alpha}\langle x, y\rangle + \overline{\beta}\langle x, z\rangle.
			\end{align}
		
			\item \textbf{Positive-definiteness}:
			\begin{align}
				\langle x, x\rangle \geq 0
			\end{align}
			and 
			\begin{align}
				\langle x, x\rangle = 0 \Leftrightarrow x = 0.
			\end{align}
		\end{enumerate}
	\end{defn}

	\begin{defn}[Normed Space]\label{defn:normed_space}
		Let $\mathbb K$ be a space ($\mathbb{K} = \mathbb{R}$ or $\mathbb{K} = \mathbb{C}$). Then a mapping $\norm{\cdot}{}: X\to\mathbb{R}$ is called a \textbf{norm} if it satisfies the following properties for all $\varphi$, $\psi\in X$ and $\alpha\in \mathbb{K}$:
		
		\begin{enumerate}
			\item \textbf{Positivity}:
			\begin{align}
				\norm{\varphi}{} \geq 0
			\end{align}

			\item \textbf{Definiteness}:
			\begin{align}
				\norm{\varphi}{} = 0 \Leftrightarrow \varphi = 0
			\end{align}
		
			\item \textbf{Homogeneity}:
			\begin{align}
				\norm{\alpha\varphi}{} = \abs{\alpha}\norm{\varphi}{}
			\end{align}
		
			\item \textbf{Triangle inequality}:
			\begin{align}
				\norm{\varphi + \psi}{} \leq \norm{\varphi}{} + \norm{\psi}{}
			\end{align}
			
			A linear space $X$ with a norm $\norm{\cdot}{}$ is called a \textbf{normed linear space} or \textbf{normed space} for short. For a normed space, we shall use the notation $\left(X, \norm{\cdot}{}\right)$.
			
		\end{enumerate}
	\end{defn}

	\begin{exmp}
		Let $X = \mathbb{R}^d$. Then for $1 \leq p < \infty$, the $L^p$ norm of a vector $x\in X$ is defined as:
		
		\begin{align}\label{eq:L^p_norm}
			\norm{x}{p} := \left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}}
		\end{align}
		In the limit $p\to\infty$, we obain the so-called \textit{supremum norm}:
		\begin{align}\label{eq:sup_norm}
			\norm{x}{\infty} := \max_{1\leq j\leq d}{\abs{x_j}}.
		\end{align} 
		In the special case of $p = 2$, we recover the Euclidean norm.
	\end{exmp}
	
	\noindent\textit{Proof}: First, we show that Eq. \eqref{eq:sup_norm} is indeed the limit of Eq. \eqref{eq:L^p_norm}:
	
	\begin{align}
		\norm{x}{\infty} &= \max_{1\leq j\leq d}{\abs{x_j}} \leq \sum_{j=1}^{d}\abs{x_j} \leq d \cdot \norm{x}{\infty}
		\\ \Rightarrow \norm{x}{\infty}^p &= \left(\max_{1\leq j\leq d}{\abs{x_j}}\right)^p = \max_{1\leq j\leq d}{\abs{x_j}^p} \leq \sum_{j=1}^{d}\abs{x_j}^p \leq d \cdot \max_{1\leq j\leq d}{\abs{x_j}^p} = d\norm{x}{\infty}^p
		\\ \Rightarrow \norm{x}{\infty} &\leq \left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}} \leq d^{\frac{1}{p}}\norm{x}{\infty}
		\\ \Rightarrow \lim\limits_{p\to\infty}\norm{x}{\infty} &= \norm{x}{\infty} \leq \lim\limits_{p\to\infty}\left\{\left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}}\right\} \leq \lim\limits_{p\to\infty}\left\{d^{\frac{1}{p}}\norm{x}{\infty}\right\} = \norm{x}{\infty}
		\\ \Rightarrow \lim\limits_{p\to\infty}\left\{\left(\sum_{j=1}^{d}\abs{x_j}^p\right)^{\frac{1}{p}}\right\} &= \norm{x}{\infty}
	\end{align}

	To show the norm property of the $L^p$ norm for $1 \leq p < \infty$, we will explicitly show the fulfilling properties of a norm, cf. Defn. \eqref{defn:normed_space}, for all $x$, $y$, $z\in X$ and $\alpha\in \mathbb{R}$:

	\begin{enumerate}
		\item Positivity: $\norm{x}{p} \geq 0 \ \forall x\in X$,
		\item Definiteness: $\norm{x}{p} = 0\Leftrightarrow x = 0$,
		\item Homogeneity: $$\norm{\alpha x}{p} = \left(\sum_{j=1}^{d}\abs{\alpha x_j}^p\right)^{\frac{1}{p}} = \left(\sum_{j=1}^{d}\abs{\alpha}^p\abs{x_j}^p\right)^{\frac{1}{p}} = \abs{\alpha}\norm{x}{p},$$
		\item Triangle inequality: $$\norm{x + y}{p} = \left(\sum_{j=1}^{d}\abs{x_j + y_j}^p\right)^{\frac{1}{p}} \leq \left(\sum_{j=1}^{d}\abs{x_j}^p + \abs{y_j}^p\right)^{\frac{1}{p}} = \norm{x}{p} + \norm{y}{p}.$$	
	\end{enumerate}
	
	In case of $p = \infty$, we will only show the triangle inequality, since the other properties are trivial to prove:
	
	$$\norm{x + y}{\infty} = \max_{1\leq j\leq d}{\abs{x_j + y_j}} \leq \max_{1\leq j\leq d}\left\{\abs{x_j} + \abs{y_j}\right\} \leq \max_{1\leq j\leq d}\abs{x_j} + \max_{1\leq j\leq d}\abs{y_j} = \norm{x}{\infty} + \norm{y}{\infty},$$
	
	where the last inequality holds since for any $1\leq j \leq d$, it holds that
	\begin{align*}
		&\left(\abs{x_j} \leq \max_{1\leq k\leq d}\abs{x_k}\right) \wedge \left(\abs{y_j} \leq \max_{1\leq k\leq d}\abs{y_k}\right) 
		\\ &\Rightarrow \abs{x_j} + \abs{y_j} \leq \max_{1\leq k\leq d}\abs{x_k} + \max_{1\leq k\leq d}\abs{y_k} 
		\\ &\Rightarrow \max_{1\leq j\leq d}\left\{\abs{x_j} + \abs{y_j}\right\} \leq \max_{1\leq k\leq d}\abs{x_k} + \max_{1\leq k\leq d}\abs{y_k}
	\end{align*}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{theorem}\label{second-triangle-inequality}
		Let $(X, \norm{\cdot}{})$ be a normed space. Then the \enquote{second triangle inequality} holds: 
			\begin{align}
				\abs{\norm{\varphi}{} - \norm{\psi}{}} \leq \norm{\varphi - \psi}{} \quad \forall \varphi, \psi \in X. 
			\end{align}
	\end{theorem}
	\noindent\textit{Proof}: For $\varphi$, $\psi \in X$ we have 
	\begin{align}
		\norm{\varphi}{} = \norm{\varphi - \psi + \psi}{} \leq \norm{\varphi - \psi}{} + \norm{\psi}{} \Leftrightarrow \norm{\varphi}{} - \norm{\psi}{} \leq \norm{\varphi - \psi}{}. 
	\end{align}
	By exchanging the roles of $\varphi$ and $\psi$ we obtain 
	\begin{align}
		\norm{\psi}{} - \norm{\varphi}{} \leq \norm{\varphi - \psi}{}
	\end{align}
	and thus 
	\begin{align}
		\abs{\norm{\varphi}{} - \norm{\psi}{}} \leq \norm{\varphi - \psi}{}. 
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{theorem}\label{norms_continuities}
		Let $(X, \norm{\cdot}{})$ be a normed space. Then the addition, scalar multiplication and the norm itself are continuous. 
	\end{theorem}
	\noindent\textit{Proof}: 
	\begin{itemize}
		\item ad continuity of the addition: Let $(\varphi_n)_{n\in\mathbb{N}}$ and $(\psi_n)_{n\in\mathbb{N}}$ be convergent sequences in $X$ with limit elements $\varphi$, $\psi\in X$, i.e. $\varphi_n \longrightarrow \varphi$ and $\psi_n \longrightarrow \psi$ for $n\to\infty$. Thus 
		\begin{align}
			0\leq \norm{(\varphi_n + \psi_n) - (\varphi + \psi)}{} \leq \norm{\varphi_n - \varphi}{} + \norm{\psi_n - \psi}{} \longrightarrow 0 \quad\text{for } n\to \infty
		\end{align}
		and hence $\varphi_n + \psi_n \rightarrow \varphi + \psi$ for $n\to\infty$. 
		\item ad continuity of the scalar multiplication: Let $\left(\alpha_n\right)_{n\in\mathbb N} \in \mathbb K$ converge to $\alpha\in \mathbb K$ and \\ $\left(\varphi_n\right)_{n\in\mathbb N}\in X \rightarrow \varphi\in X$ for $n\to\infty$. Then 	
		\begin{align}
			0&\leq\norm{\alpha_n\varphi_n-\alpha\varphi}{} = \norm{\alpha_n\left(\varphi_n-\varphi\right) + \left(\alpha_n-\alpha\right)\varphi}{} \leq \norm{\alpha_n(\varphi_n-\varphi)}{} + \norm{\left(\alpha_n - \alpha\right)\varphi}{}
			\\ &\leq \abs{\alpha_n}\norm{\varphi_n-\varphi}{} + \abs{\alpha_n-\alpha}\norm{\varphi}{} \overset{n\to\infty}{\longrightarrow} 0. 
		\end{align}
		This implies $\alpha_n\varphi_n  \rightarrow \alpha\varphi$ for $n\to\infty$. 
		\item ad continuity of the norm: Let $\varphi_n \rightarrow\varphi$. With Theorem \ref{second-triangle-inequality} we have: 
		\begin{align}
			0\leq \abs{\ \norm{\varphi_n}{}-\norm{\varphi}{}\ } \leq \norm{\varphi_n - \varphi}{} \overset{n\to\infty}{\longrightarrow}  
		\end{align}
		and hence $\norm{\varphi_n}{} \rightarrow \norm{\varphi}{}$ for $n\to\infty$. 
	\end{itemize}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	\begin{defn}
		Two norms $\left\vert\left\vert \cdot \right\vert\right\vert_{a}$ and $\left\vert\left\vert \cdot \right\vert\right\vert_{b}$ on a linear space $X$ are said to be equivalent if and only if there exist positive constants $0 < c \leq C < \infty$ such that 
		\begin{align}\label{equivalence_norms}
			c\norm{\varphi}{b} \leq \norm{\varphi}{a} \leq C\norm{\varphi}{b} \quad \forall \varphi \in X. 
		\end{align}	
		(It is also possible to write this as $\tilde{c}\norm{\varphi}{a} \leq \norm{\varphi}{b} \leq \tilde{C}\norm{\varphi}{a}$ with $\tilde{c} := C^{-1}$ and $\tilde{C} := c^{-1}$, where $0 < \tilde{c} \leq \tilde{C}<\infty$.)
	\end{defn}

	\begin{lemma}
		Let $X$ be a linear space and the pairs $\left(\norm{\cdot}{a}, \norm{\cdot}{c}\right)$ and $\left(\norm{\cdot}{b}, \norm{\cdot}{c}\right)$ be equivalent. Then also the pair $\left(\norm{\cdot}{a}, \norm{\cdot}{b}\right)$ is equivalent. 
	\end{lemma}
	\noindent\textit{Proof}: By assumption, we know that 
	\begin{align}
		c\norm{\varphi}{c} &\leq \norm{\varphi}{a} \leq C\norm{\varphi}{c} \quad \forall \varphi\in X \label{equivalent-norms-lemma-proof}
	\end{align}
	and 
	\begin{align}
		d\norm{\varphi}{c} &\leq \norm{\varphi}{b} \leq D\norm{\varphi}{c} \quad \forall \varphi\in X
							\\ 
		\Leftrightarrow \norm{\varphi}{c} &\leq \frac{1}{d}\norm{\varphi}{b} \leq \frac{D}{d}\norm{\varphi}{c}. 
							\\
		\overset{\tiny\eqref{equivalent-norms-lemma-proof}}{\Leftrightarrow} \frac{1}{C}\norm{\varphi}{a} &\leq \norm{\varphi}{c} \leq \frac{1}{d}\norm{\varphi}{b} \leq \frac{D}{d}\norm{\varphi}{c} \leq \frac{D}{d\cdot c}\norm{\varphi}{a}  
							\\ 
		\Leftrightarrow \frac{1}{C}\norm{\varphi}{a} &\leq \frac{1}{d}\norm{\varphi}{b} \leq \frac{D}{d\cdot c}\norm{\varphi}{a}
							\\ 
		\Leftrightarrow \frac{d}{C}\norm{\varphi}{a} &\leq \norm{\varphi}{b} \leq \frac{D}{c}\norm{\varphi}{a}. 
	\end{align}
	It is clear that $0 < dC^{-1} \leq Dc^{-1} < \infty$ holds. \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$

	\begin{theorem}\label{finite_dimensional_norm_equivalence}
		On a \textit{finite-dimensional} space $X$ over a field $\mathbb{K}$ all norms are equivalent. 
	\end{theorem}
	\noindent\textit{Proof \cite[p. 27f.]{werner-fa}}: Let $\dim(X) = n$, $\{e_1, \dots, e_n\}$ be a basis of $X$ and $\norm{\cdot}{}$ a norm on $X$. We can now show that $\norm{\cdot}{}$ is equivalent to the Euclidean norm $\norm{\sum_{i = 1}^n\alpha_ie_i}{2} = \left( \sum_{i = 1}^{n}\left\vert \alpha_i \right\vert^2 \right)^{1/2}$ as follows: \\[6pt] Set $K:= \max\left\{ \norm{e_1}{}, \dots, \norm{e_n}{}\right\} > 0$. Then from the triangle inequality for $\norm{\cdot}{}$ we have: 
	\begin{align}\label{equivalence-of-norms}
		\norm{x}{} = \norm{\sum_{i= 1}^{n}\alpha_ie_i}{} \leq \sum_{i= 1}^n \norm{\alpha_ie_i}{} = \sum_{i= 1}^n \abs{\alpha_i} \norm{e_i}{}
	\end{align}
	Since $(\abs{\alpha_1}, \dots, \abs{\alpha_n})^T$, $(\norm{e_1}, \dots, \norm{e_n}{})^T\in \mathbb R^n$ and 
	\begin{align}
		\left\langle \left(\abs{\alpha_1}, \dots, \abs{\alpha_n}\right), (\norm{e_1}, \dots, \norm{e_n}{} ) \right\rangle = \sum_{i = 1}^n \abs{\alpha_i}\norm{e_i}{}
	\end{align}
	we can use the Cauchy-Schwarz inequality: 
	\begin{align}
		\langle (\abs{\alpha_1}, \dots, \abs{\alpha_n}), (\norm{e_1}, \dots, \norm{e_n}{} ) \rangle &\leq \norm{\sum_{i=1}^n \alpha_ie_i}{2}\cdot \norm{\sum_{i= 1}^n e_i}{2} = \sqrt{\sum_{i = 1}^n\abs{\alpha_i}^2} \cdot \sqrt{\sum_{i = 1}^n\norm{e_i}{}^2}
		\\[8pt] &= \norm{x}{2}\cdot \sqrt{\sum_{i=1}^n K^2} = K\sqrt{n}\norm{x}{2} \quad \forall x\in X, 
	\end{align}
	where in the last line we used that $K = \max\{\norm{e_1}, \dots, \norm{e_n}{}\}$ and $x = \sum_{i = 1}^n \alpha_i e_i$. Putting the last Eq. into Eq. \eqref{equivalence-of-norms}, we have: 
	\begin{align}\label{fa_equiv_norms_intermed_1}
		\norm{x}{} \leq \sum_{i=1}^{n} \abs{\alpha_i}\norm{e_i}{} \leq K\sqrt{n}\norm{x}{2} \quad \forall x\in X.
	\end{align}
	Now define the set 
	\begin{align}
		S:= \{ x\in X \mid \norm{x}{2} = 1 \}. 
	\end{align}
	This set is closed since it is the preimage of the closed set $\{1\}\subset \mathbb R$ under the continuous function $\norm{\cdot}{2}$, cf. Theorem \ref{norms_continuities}, \ref{preimages_continuous_functions}. $S$ is also closed since $S\subset B_{r}(0) = \{\psi\in X\mid \norm{\psi}{2} < r\}$ for $r>0$ (here, we take into account that every norm induces a metric). Thus, $S$ is compact according to Heine-Borel (which applies to every finite-dimensional normed vector space). Since every continuous function takes its minimum on a compact set, we know that $\norm{\cdot}{}$ has a minimum $m > 0$ on $S$. Since $x\cdot \norm{x}{2}^{-1}\in S$ for $x\ne 0$, we have ($m$ is the minimum of the function $\norm{\cdot}{}$): 
	\begin{align}
		m\norm{x}{2} \leq \norm{x}{} \quad \forall x\in X.
	\end{align}
	All in all, we proved: 
	\begin{align}
		m\norm{x}{2} \leq \norm{x}{} \leq K\sqrt{n}\norm{x}{2} \quad \forall x\in X. 
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}
		Let $X$ be a linear space equipped with two metrics $d$ and $d'$. Then the metrics are \textit{strongly equivalent} if and only if there exist positive constants $0 < \alpha \leq \beta < \infty$ such that 
		\begin{align}
			\alpha d(x, y) \leq d'(x, y) \leq \beta d(x, y). 
		\end{align}
		\cite{equivalence-metrics}
	\end{defn}
	
	\begin{remark}\label{equivalence_metrics_finite_dimensional}
		Obviously, Eq. \eqref{equivalence_norms} can also be written as 
		\begin{align}
			c\norm{\varphi-\psi}{b} \leq \norm{\varphi-\psi}{a} \leq C\norm{\varphi-\psi}{b} \quad \forall \varphi, \psi\in X. 
		\end{align} 
		Thus, also Theorem \ref{finite_dimensional_norm_equivalence} holds for metrics as well: In a finite-dimensional space $X$, all metrics are strongly equivalent. 
	\end{remark}
	
	\newpage 
	\section{Topology}
	\begin{defn}[Metric Space \cite{fa2019}]\label{defn:metric_space}
		Let $X$ be a non-empty set. Then the map 
		\begin{align*}
			d: X\times X\rightarrow \mathbb R
		\end{align*}
		is called a \textit{metric} on $X$ if for all $\varphi$, $\psi$, $\chi\in X$ the following properties are given: 
		\begin{itemize}
			\item $d(\varphi, \psi) \geq 0$, 
			\item $d(\varphi, \psi) = d(\psi, \varphi)$, 
			\item $d(\varphi, \varphi) = 0$, 
			\item $d(\varphi, \psi) \leq d(\varphi, \chi) + d(\chi, \psi)$. 
		\end{itemize}
	\end{defn}

	\begin{defn}[Topological Space \cite{topology-singh}]
		A topology on a set $X$ is a collection $\tau$ of subsets of $X$ such that 
		\begin{itemize}
			\item the intersection of two members of $\tau$ is in $\tau$, 
			\item the union of any collection of members of $\tau$ is in $\tau$, 
			\item the empty set $\{\}$ and $X$ itself are in $\tau$.
		\end{itemize}
		A set $X$ endowed with a topological structure $\tau$ on it is called a \textit{topological space}. The elements of $X$ are called \textit{points}, and the members of $\tau$ are called the \textit{open sets}. 
	\end{defn} 
	
	\begin{remark}[Discrete Metric]\label{discrete-metric}
		Let $X$ be an arbitrary set and $d(\varphi, \psi) = 1 \ \forall \varphi$, $\psi\in X$ and $d(\varphi, \varphi) = 0$. Then $d$ is a metric, called the \textit{discrete metric} on $X$. 
	\end{remark}
	
	\begin{defn}[Open Ball]
		Let $(X, d)$ be a metric space, $\varphi\in X$ and $r > 0$. Then 
		\begin{align}
			B_{r}(\varphi) := \left\{ \psi\in X \vert d(\varphi, \psi) < r\right\} \subset X
		\end{align}
		is called an \textit{open ball} in $X$ with middle point $\varphi$ and radius $r$. 
 	\end{defn}
 
 	\begin{exmp}
 		For the discrete metric $d$ according to Remark \ref{discrete-metric}, the open balls can be characterized as follows:
 		\begin{align}
 			B_{r}(\varphi) = \begin{cases}
 				\{\varphi\}, &r \leq 1 
 				\\ X, &r > 1
 			\end{cases}
 		\end{align}
 	\end{exmp} 
	
	 \begin{defn}[Closed ball]
		Let $(X, d)$ be a metric space, $\varphi\in X$ and $r > 0$. Then 
		\begin{align}
			B_{r}[\varphi] := \left\{ \psi\in X \vert d(\varphi, \psi) \leq r\right\} \subset X
		\end{align}
		is called a \textit{closed ball} in $X$ with middle point $\varphi$ and radius $r$. 
	\end{defn}
	
	\begin{defn}[Open Set]
		\label{defn-open-set}
		Let $(X, d)$ be a metric space. Then a subset $U\subset X$ is called \textit{open} in $X$ if for every $\varphi \in U$ there is an open ball $B_{r}(\varphi)$ that is contained in $U$, i.e. $B_r(\varphi)\subset U$. 
	\end{defn}

	\begin{defn}[Closed Set]
		Let $(X, d)$ be a metric space. Then a subset $A\subset X$ is called \textit{closed} in $X$ if the complement $X\backslash A$ is open according to Definition \ref{defn-open-set}. 
	\end{defn}

	\begin{theorem}\label{open-balls-open}
		Open balls are open. 
	\end{theorem}
	\noindent\textit{Proof:}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.35\textwidth]{Figures/open-balls-open.png}
	\end{figure}
	An illustration is shown in Figure. Let $f\in X$ and for $r > 0$ consider the open ball $U:= B_{r}(f) \subset X$. By definition, for every $\varphi \in U$ it holds that $d\left(\varphi, f\right) < r$. Now we show that 
	\begin{align}
		B_{r-d\left(\varphi, f\right)}(\varphi) \subset U.
	\end{align}
	For this consider $\psi\in B_{r-d\left(\varphi, f\right)}(\varphi)$, i.e. $d(\psi, \varphi) < r - d(\varphi, f)$. With the triangle inequality we obtain: 
	\begin{align}
		d(f, \psi) \leq d(f, \varphi) + d(\varphi, \psi) < d(f, \varphi) + r - d(\varphi, f) = r,  		
	\end{align}
	i.e. $d(f, \psi) < r$ and thus $\psi\in U = B_{r}(f)$.  \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{theorem}\label{thrm:closed_balls_open}
		Closed balls are closed.
	\end{theorem}
	
	\begin{proof}
		Let $f\in X$ and for $r > 0$ consider the closed ball $B_{r}[f]\subset X$. By definition, for every $\varphi \in X\backslash B_{r}[f]\subset X$, it holds that $d(\varphi, f) > r$. We will now show that 
		\begin{align}
			B_{d(\varphi, f) - r}(\varphi) \subset X \backslash B_{r}[f],
		\end{align}
		which would prove that $B_{r}[f]$ is closed. Let $\psi\in B_{d(\varphi, f) - r}(\varphi)$, i.e. $d(\psi, \varphi) < d(\varphi, f) -r$. With the triangle inequality, we obtain:
		\begin{align}
			d(f, \varphi) \leq d(f, \psi) + d(\psi, \varphi) \Leftrightarrow d(f, \psi) \geq d(f, \varphi) - d(\psi, \varphi) > d(f, \phi) - d(\varphi, f) + r = r, 
		\end{align}
		i.e. $d(f, \psi) > r$, and hence $\psi\notin B_r[f]$, implying that $\psi\in X\backslash B_{r}[f]$. 
	\end{proof}
	
	\begin{theorem}
		Let $(X, d)$ be a metric space. The collection of open sets as in Definition \ref{defn-open-set} gives a topology.
	\end{theorem}
	\begin{proof}		
		According to Defn. \ref{defn:metric_space}, we need to prove that $X$ and $\emptyset$ are open, that the union of an arbitrary number of open sets is open, and that the intersection of a finite number of open sets is open. 
		
		Clearly, $X$ is open (since for any choice of $r > 0$ and $\varphi\in X$, $B_{r}(\varphi)\subset X$). $\emptyset$ is also open, since nothing needs to be shown.
		
		Now let $U_{1}$, $\dots$, $U_{n}\subset X$ be open, and consider the intersection
		\begin{align}
			U := \bigcap_{i=1}^{n}U_{i} \subset X.
		\end{align} 
		For any $\varphi\in U$, we know that $\varphi\in U_i$ for all $1\leq i \leq n$. Since the $U_{i}$ are open, there is a radius $r_{i} > 0$ such that $B_{r_i}(\varphi) \subset U_i$ for all $1\leq i \leq n$. Defining $r := \min_{1 \leq i \leq n}\{r_{i}\} > 0$, we have that $B_{r}(\varphi) \subset U_i$ for all $1\leq i\leq n$ and hence $B_{r}(\varphi)\subset U$.
		
		Finally, let $U_{i}$, $i\in I$, be open, where $I$ is in index set. Consider the union
		\begin{align}
			U := \bigcup_{i\in I}U_{i} \subset X.
		\end{align}
		For any $\varphi\in U$ there is an index $i\in I$ such that $\varphi\in U_i$. Since $U_i$ is open, there is an $r > 0$ such that $B_{r}(\varphi) \subset U_i \subset U$.
	\end{proof}
	
	\begin{defn}[Bounded Set]
		Let $(X, d)$ be a metric space. Then $A\subset X$ is called \textit{bounded} if there exists $r>0$ and $\varphi\in X$ such that $A\subset B_{r}(\varphi)\subset X$. \cite[p. 567]{MfPII}
	\end{defn}

	\begin{defn}\label{defn:accumulation_point}
		Let $U\subset X$. Then $\varphi\in X$ is called an \textit{accumulation point} of $U$ if there is a sequence $\left(\varphi_{n}\right)_{n\in \mathbb N}$ in $U$ such that $\lim\limits_{n\to\infty}\varphi_{n} = \varphi$.
	\end{defn}

	\begin{theorem}\label{thm:closed_set_acc_point}
		A subset $U \subset X$ is closed if and only if it contains all its accumulation points according to Defn. \ref{defn:accumulation_point}.
	\end{theorem}

	\begin{proof}
		\enquote{$\Longleftarrow$}: Let $U$ contain all its accumulation points, and let $\varphi\in X\backslash U$. We will now show that there is an $n\in \mathbb N$ such that $B_{\frac{1}{n}}(\varphi) \subset X\backslash U$, proving that $X\backslash U$ is open. Assuming there is no such $n\in \mathbb N$, then $B_{\frac{1}{n}}(\varphi) \cap U = \emptyset$ for all $n\in \mathbb N$. This means that there is a sequence $(\varphi_{n})_{n\in\mathbb N}$ in $U$ such that $d(\varphi, \varphi_n) < \frac{1}{n}$ for all $n\in\mathbb N$. Hence, $\varphi$ is an accumulation point of $U$, and since $U$ contains all its accumulation points by assumption, it follows that $\varphi\in U$, contradicting the assumption that $\varphi\in X\backslash U$.
		
		\enquote{$\Longrightarrow$}: Let $U$ be closed, i.e. $X\backslash U$ is open. Further, let $\varphi\in X$ be an accumulation point of $U$, i.e. there is a sequence $(\varphi_n)_{n\in\mathbb N}$ in $U$ such that $\lim\limits_{n\to\infty}\varphi_n = \varphi$. Hence, by definition of convergence, for all $\epsilon > 0$ there exists an $n\in\mathbb N$ such that $\varphi_n\in B(\varphi, \epsilon) \cap U$, implying that $\varphi\in U$. Thus, $U$ contains all its accumulation points.
	\end{proof}
	
	\begin{defn}[Closure of set]
		The set 
		\begin{align}
			\bar{U} := \{\varphi\in X\mid \varphi\ \text{is an accumulation point of}\ U\} \subset X
		\end{align}
		is called the closure of the set $U \subset X$.
	\end{defn}

	\begin{remark}
		Clearly, $U\subset \bar{U}$, since any $\varphi\in U$ is an accumulation point of $U$ (consider the constant sequence, where for any $n\in\mathbb N$, $\varphi_n := \varphi$).
	\end{remark}

	\begin{remark}
		According to Theorem \ref{thm:closed_set_acc_point}, a subset $U\subset X$ is closed if and only if coincides with its closure, i.e. $U = \bar{U}$.
	\end{remark}
	
	\begin{defn}[$\epsilon$-$\delta$ definition of continuity]
		Let $D$ be a subset of a metric space $(X, d)$ and let $p\in D$. Let $(Y, d')$ be another metric space. A function $f: D\rightarrow Y$ is called \textit{continuous} at $p$ if for all $\epsilon > 0$ there exists a $\delta > 0$ s.t. 
		\begin{align}
			d'\left( f\left(z\right), f\left(p\right) \right) < \epsilon \ \forall z\in D \quad \text{with} \ d\left(z, p\right) < \delta.  
		\end{align}
		 \cite{MfPI}
	\end{defn}

	\begin{lemma}\label{continuity_vector_components}
		Consider the map $f: \mathbb R^m\rightarrow\mathbb R^n$. Then $f$ is continuous if all the $f_i: \mathbb R^m\rightarrow\mathbb R$, $i\in\{1, \dots, n\}$ are continuous. 
	\end{lemma}
	\begin{proof}
		Since all metrics are strongly equivalent, it does not matter which metric we equip $\mathbb R$, $\mathbb R^m$ and $\mathbb R^n$ with, cf. Remark \ref{equivalence_metrics_finite_dimensional}; for the following, consider the Euclidean metric. Suppose each $f_i$ is continuous for all $i\in \{1, \dots, n\}$, i.e.: 
		\begin{align} 
			\forall p\in\mathbb R^m: \forall \epsilon_i > 0 \ \exists 
			\delta_i>0: d_{\mathbb R}(f_i(p), f_i(z)) < \frac{\epsilon_i}{\sqrt{n}} \ \forall z\in D \quad \text{with}\ d_{\mathbb R^m}(p, z) < \delta_i. 
		\end{align} 
		Now define $\epsilon:= \max\{\epsilon_1, \dots, \epsilon_n\}$. Thus: 
		\begin{align}
			d_{\mathbb R^n}(f(p), f(z)) = \sqrt{\sum_{i=1}^{n}\left(f_i(p)-f_i(z)\right)^2} 
			= \sqrt{\sum_{i=1}^{n}d^2_{\mathbb R}\left(f_i(p), f_i(z)\right)} < \sqrt{\sum_{i=1}^{n}\left(\frac{\epsilon}{\sqrt{n}}\right)^2} = \epsilon. 
		\end{align}
	\end{proof}
	
	\begin{theorem}\label{preimages_continuous_functions}
		For a map $f: X\rightarrow Y$ between two metric spaces $\left(X, d_{X} \right)$ and $\left(Y, d_{Y} \right)$ the following statements are equivalent: 
		\begin{enumerate}[label = (\roman*)]
			\item $f$ is continuous,
			\item preimages $f^{-1}(V) := \left\{ x\in X\mid f\left(x\right) \in V \right\}$ of \underline{all} open sets $V\subset Y$ are open,  
			\item preimages $f^{-1}(A)$ of \underline{all} closed sets $A\subset Y$ are closed. 	
		\end{enumerate}
	\end{theorem} 
	\noindent\textit{Proof:} \\ (i) $\Rightarrow$ (ii) Assume that $f$ is continuous and that $V\subset Y$ is open and let $a\in f^{-1}\left( V\right)$. Since $V$ is an open set, $\exists \epsilon > 0$ s.t. $B_{\epsilon}\left( f\left(a\right) \right)\subset V$, cf. Def. \ref{defn-open-set}. By assumption, $f$ is continuous at $a\in f^{-1}\left(V\right)\subset X$ and therefore $\forall \epsilon > 0 \ \exists \delta > 0$ s.t. $d_{X}\left(x, a\right) < \delta\Rightarrow d_{Y}\left(f\left(x\right), f\left(a\right)\right) < \epsilon \ \forall x\in X$. Put differently, $x\in B_{\delta}\left( a\right)$ implies $f\left(x\right) \in B_{\epsilon}\left( f\left(a\right) \right) \subset V$. Thus, $B_{\delta}(a) \subset f^{-1}(V) \Rightarrow f^{-1}(V) \subset X$ is an open set, cf. Def. \ref{defn-open-set}. 
	\\ 
	\\ 
	(ii) $\Rightarrow$ (i) Assume that $f^{-1}\left(Y\right) \subset X$ is open for $V\subset Y$ open and let $a\in X$, $\epsilon > 0$. From Theorem \ref{open-balls-open} we know that $B_{\epsilon}\left(f\left(a\right)\right)\subset Y$ is open. Thus, by assumption, $f^{-1}\left( B_{\epsilon}\left( f\left(a\right) \right) \right) = \left\{ x\in X \mid f\left( x \right) \in B_{\epsilon}\left( f\left(a\right) \right) \right\} = \left\{ x\in X \mid f\left(x\right) \in \left\{ y\in Y\mid d_Y\left( y, f\left(a\right) \right) < \epsilon \right\}\right\}$ is open as well. Clearly, $a\in f^{-1}\left( B_{\epsilon}\left( f\left(a\right) \right) \right)$. Therefore, it follows from Def. \ref{defn-open-set} that $\exists\delta > 0$ s.t. $B_{\delta}(a) \subset f^{-1}\left( B_{\epsilon}\left( f\left(a\right) \right) \right)$. Thus, $\forall x\in B_{\delta}(a): d_{X}\left( x, a \right) < \delta \Rightarrow d_Y\left(f\left(x\right), f\left(a\right)\right) < \epsilon$. This proves that $f: X\rightarrow Y$ is continuous in every point $a\in X$. \cite{cont-functions-open-sets}   
	\\ 
	\\
	(ii) $\Rightarrow$ (iii) Assume that the preimages $f^{-1}(V)$ of all open sets $V\subset Y$ are open. Since $f^{-1}(Y\backslash V) = f^{-1}(Y)\backslash f^{-1}(V)$, as can be easily shown by using the definition of the complement of a set, we have for all open sets $V \subset Y$: 
	\begin{align}
		f^{-1}(Y\backslash V) = f^{-1}(Y)\backslash f^{-1}(V) = X\backslash f^{-1}(V). 
	\end{align}
	Since $f^{-1}(V)$ is open by assumption, $f^{-1}(Y\backslash V) = X\backslash f^{-1}(V)$ is closed. 
	\\ 
	\\
	(iii) $\Rightarrow$ (ii) Assume that the preimages $f^{-1}(A)$
	of all closed sets $A\subset Y$ are closed. Then: 
	\begin{align}
		f^{-1}(Y\backslash A) = f^{-1}(Y)\backslash f^{-1}(A) = X\backslash f^{-1}(A). 
	\end{align}
	Since $f^{-1}(A)$ is closed by assumption, $f^{-1}(Y\backslash A) = X\backslash f^{-1}(A)$ is open. \cite{preimage-of-closed-sets} \qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{corollary}
		Let $(X, \tau_1)$ and $(Y, \tau_2)$ be topological spaces coming from metric spaces. Then $f: X\rightarrow Y$ is continuous if and only if $f^{-1}(G)$ for every open set $G\in Y$. 
	\end{corollary}
	
	\begin{defn}[Basis]
		A \textit{basis} of a topology $\left(M, \tau\right)$ is a collection of open sets $\mathcal B$ such that for all $U\in \tau$ there exists an index set $I$ and corresponding $B_i\in \mathcal B$ s. t. 
		\begin{align}
			\bigcup_{i\in I}B_i = U. 
		\end{align}
	\end{defn}

	\begin{exmp}\label{basis_discrete_metric}
		Let $(X, d)$ be a metric space, where the metric is the discrete metric from Remark \ref{discrete-metric}. Then the collection of all singletons $\{\varphi\in X\}$ is basis of $X$. 
	\end{exmp}
	
	\begin{exmp}\label{basis_metric_space}
		In a metric space $X$, the collection $$\{ B_{r}(x) \mid x\in X, r > 0 \}$$ of open balls is a basis for the topology given by the metric on $X$. 
	\end{exmp}

	\begin{defn}[Hausdorff]
		A topology $\left(M, \tau\right)$ is called \textbf{Hausdorff} if $\forall p\ne q \in M \ \exists U$, $V\in \tau$ open with $p\in U$, $q\in V$ such that $U\cap V = \emptyset$.  
	\end{defn} 

	\begin{lemma}
		All metric spaces are Hausdorff spaces. 
	\end{lemma}
	\noindent\textit{Proof:} 
	\begin{figure}[h!]		
		\centering 
		\includegraphics[trim = {3.8cm 5.8cm 9.7cm 2.8cm}, width=0.5\textwidth, clip]{Figures/metric-spaces-Hausdorff-spaces-v2.png}
		\caption{Visualization for why a metric space is a Hausdorff space.}
		\label{metric-space-Hausdorff-space}
	\end{figure} 
A visualization is shown in Fig. \ref{metric-space-Hausdorff-space}. To prove this more rigorously, define $U := B_{r}(p)$ and $V:=B_{r}(q)$ with radius $r := \frac{d(p, q)}{2}$, where from Theorem \ref{open-balls-open} we know that $U$ and $V$ are open. Suppose $U\cap V = \emptyset$ would not hold. Then there exists a $z\in U\cap V$ with 
	\begin{align}
		d(p, z) < \frac{d(p, q)}{2}
	\end{align}
	and 
	\begin{align}
		d(q, z) < \frac{d(p, q)}{2}. 
	\end{align}
	Therefore, by the triangle inequality for metric spaces, we have: 
	\begin{align}
		d(p, q) \leq d(p, z) + d(q, z) < d(p, q), 
	\end{align}
	which is clearly a contradiction. 
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}
		Let $(X, \tau)$ be a topological space. Then it is said to be \textit{second countable} if $\tau$ has a countable basis. 
	\end{defn}
	
	\begin{exmp}
		For any $n > 0$, the topological spaces $\mathbb R^n$ are second countable. 
	\end{exmp}
	
	\begin{remark}
		Metric spaces are not automatically Hausdorff spaces. For example, take an uncountable set $X$, endow it with the discrete metric and because of Example \ref{basis_discrete_metric}, \ref{basis_metric_space},  the topological space $X$ is not second countable, since $X$ is uncountable. 
	\end{remark}
	
	\begin{defn}[Homeomorphism] A \textit{homeomorphism} between two topological spaces $X$ and $Y$ is an invertible function $f: X\rightarrow Y$ such that $f$ and $f^{-1}$ are continuous \cite[p. 33]{topology-singh}. 
	\end{defn}

	\begin{exmp}
		The Euclidean space $\mathbb R^n$, equipped with the usual topology, is homeomorphic to the open ball $B_{r}(\varphi) =  \{x\in\mathbb R^n \mid \lvert\lvert x-\varphi \rvert\rvert < r \} \subset \mathbb R^n$ (consider the open ball as a metric space with the \textit{induced metric} from the whole space of $\mathbb R^n$ and then equip it with the usual topology on a metric space given by the open sets). 
	\end{exmp}
	\noindent\textit{Proof:} Consider the map $$f: \mathbb R^n\rightarrow B_r(\varphi), \ x\mapsto \frac{r\cdot (x-\varphi)}{1+\lvert\lvert x-\varphi\rvert\rvert}.$$ Obviously, $f$ is continuous with inverse $$f^{-1}: B_r(\varphi)\rightarrow \mathbb R^n, \ x \mapsto \frac{x}{r-\lvert\lvert x\rvert\rvert}+\varphi,$$ which is also continuous. One can easily show that $f$ and $f^{-1}$ are inverses to each other. Thus, $f$ \\ describes a homeomorphism. 	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad$\blacksquare$
	
	\begin{exmp}[Stereographic projection]
		The unit sphere $S^n$, embedded in $\mathbb R^{n+1}$, without the north pole, i.e. $S^n\backslash \{p\} \subset \mathbb R^{n+1}$, where $S^n := \{ x\in\mathbb R^{n+1} \mid \norm{x}{2} = 1 \}$ and $p := \{ x\in\mathbb R^{n+1} \mid x_i = 0 \ \forall i\in[1, n], x_{n+1} = 1 \}$, is homeomorphic to $\mathbb R^n$ (consider the unit sphere as a metric space with the \textit{induced metric} from the whole space of $\mathbb R^{n+1}$ and then equip it with the usual topology on a metric space given by the open sets). 
	\end{exmp}
	\begin{proof}
			Consider the map 
		\begin{align}
			f: S^n\backslash\{p\}\rightarrow \mathbb R^n, \ x\mapsto \frac{1}{1-x_{n+1}}\begin{pmatrix} x_1, \dots, x_n \end{pmatrix}^T. 
		\end{align}
		Obviously, $f$ is continuous. Its inverse is given by 
		\begin{align}\label{stereographic_map_inverse}
			f^{-1}: \mathbb R^n \rightarrow S^n\backslash\{p\}, \ x\mapsto \begin{pmatrix} 2x_1/ \left(1+\norm{x}{2}^2\right)\\ \vdots\\ 2x_n/\left(1+\norm{x}{2}^2\right) \\[4pt] 1-2/\left(1+\norm{x}{2}^2\right) \end{pmatrix}.  
		\end{align}
		It is trivial to show that $f$ and $f^{-1}$ are inverses to each other. One can also easily show that $\norm{f^{-1}(x)}{2} = 1$; thus, $f^{-1}$ does indeed bring us to the unit ball $S^n$. 
		To see that $f^{-1}$ is continuous, note that all components are continuous and thus, according to Lemma \ref{continuity_vector_components}, the function itself is continuous. 
	\end{proof} 
	
	\begin{defn}[Homotopy mapping \cite{ding2023sides}]
		The function $H(z, \lambda) = \lambda r(z) + (1 - \lambda) g(z)$ is said to be a homotopy mapping from $g(z)$ to $r(z)$ if $\lambda\in [0, 1]$ and $g$ is a smooth function. The equation $H(z, \lambda) = 0$ is the so-called \enquote{zero path} of this homotopy mapping.
	\end{defn}
	
	\newpage 
	\section{Set Theory}
	\begin{defn}[Binary Relation \cite{binary_relations}]
		A \textit{binary relation} over a set $X$ is some relation $R$ where \\ $\forall x$, $y\in X$ the statement $xRy$ is either true or false. 
	\end{defn}
	
	\begin{defn}[Equivalence Relation and Class \cite{equivalence_relation}]
		An \textit{equivalence relation} on a set $X$ is a binary relation $\sim$ with the following properties $\forall x$, $y$, $z \in X$: 
		\begin{itemize}
			\item \textbf{reflexivity}: $x\sim x$, 
			\item \textbf{symmetry}: $x \sim y\Leftrightarrow y\sim x$, 
			\item  \textbf{transitivity}: $\left(x\sim y\right) \wedge \left(y\sim z\right)\Rightarrow x\sim z$.  
		\end{itemize}
		The \textit{equivalence class} of an element $x\in X$ is defined as 
		\begin{align}
			\left[x\right] := \left\{y\in X \mid x\sim y \right\} = \left\{ y\in X \mid y\sim x \right\}. 
		\end{align}
	\end{defn}

	\begin{defn}[Partially Ordered Set \cite{kuratowski_zorn_lemma}]\label{partially_ordered_set}
		A \textit{partially ordered set}  $\left(X, \leq\right)$ is a set $X$, equipped with a binary relation $\leq$, that satisfies the following properties $\forall x$, $y$, $z\in X$:
		\begin{itemize}
			\item \textbf{reflexivity}: $x\leq x$, 
			\item \textbf{antisymmetry}: $\left(x\leq y\right) \wedge \left(y\leq x\right) \Rightarrow x = y$, 
			\item \textbf{transitivity}: $\left(x\leq y\right) \wedge \left(y\leq z\right)\Rightarrow x\leq z$. 
		\end{itemize}
	\end{defn}

	\begin{defn}[Incomparability]
		In Definition \ref{partially_ordered_set}, the phrasing \enquote{partially ordered} is used to emphasize that there might exist elements $x$, $y\in X$ s.t. both $x\leq y$ and $y\leq x$ are wrong. These pairs are called \textit{incomparable}. If either $x\leq y$ or $y \leq x$ is true, then we say that the pair is \textit{comparable}. 
	\end{defn}
	
	\begin{exmp}
		Consider $X := \left\{\{1\}, \{2\}, \{1, 2\}\right\}$ with $\subset$ as partial ordering. Obviously, the elements $\{1\}$ and $\{2\}$ are incomparable. 
	\end{exmp}

	\begin{defn}[Chain, Upper Bound, Maximal Element]
	For preparing the Kuratowski-Zorn lemma, the following definitions come in handy: 
		\begin{enumerate}[label=\alph*)]
			\item A \textit{chain} $C$ is a partially ordered set where every pair of elements in $C$ is comparable.\ One might also say that $C$ is a \textit{totally ordered set}. 
			\item An \textit{upper bound} (if existent)  of a subset $S\subset X$, where $X$ is a partially ordered set, is an element $u\in X$ such that 
			\begin{align}
				s \leq u \ \forall s\in S. 
			\end{align}
			Since $S\subset X$, $S$ itself is a partially ordered set. 
			\item A \textit{maximal element} (if existent) of a partially ordered set $X$ is an element $m\in X$ such that 
			\begin{align}
				\text{if}\ m\leq x \ \text{for some}\ x\in X,\ \text{then}\ x=m.
			\end{align}
			This is equivalent to saying that there is no $x\in X$ such that $m\leq x$ and $x\ne m$. 
		\end{enumerate}
	\end{defn}

	\begin{remark}
		For an arbitrary partially ordered set $X$, a maximal element (if existent) does not have to be unique.\ For example, consider $X := \left\{ \{1\}, \{2\}, \{3\}, \{1, 2\} \right\}$ with $\subset$ as partial ordering. Both $\{3\}$ and $\{1, 2\}$ are maximal elements.\ However, if we consider chains, then maximal elements are indeed unique by definition. 
	\end{remark}

	\begin{theorem}[Kuratowski-Zorn Lemma]
		Let $\left(M, \leq\right)$ be a non-empty partially ordered set.\ If every chain $C\subset M$ has an upper bound, then $M$ has a maximal element. 
	\end{theorem}

	\begin{remark}
		The upper bound of every chain $C\subset M$ need not be in $C$, by definition of a chain, but it must be in $M$. 
	\end{remark}
	
	\section{Differential Geometry}
	\begin{samepage}
		\begin{defn}[Smooth Atlas \cite{Lindemann-lec1}]
			Let $M$ be a second countable Hausdorff topological space. An \textit{$n$-dimensional smooth atlas on $M$}  is a collection of maps
			\begin{align*}
				\mathcal A = \left\{\left(\varphi_i, U_i\right)\mid i\in I\right\}, \quad \varphi_i: U_i \rightarrow \varphi_i(U_i) \subset \mathbb R^n
			\end{align*}
			such that all $U_i \subset M$ are open, all $\varphi_i$ are homeomorphisms, $I$ is an index set and 
			\begin{itemize}
				\item $\{U_i \mid i\in I\}$ is an open covering of $M$, 
				\item $\varphi_i \circ \varphi_j^{-1}: \varphi_j(U_i \cap U_j) \rightarrow \varphi_i(U_i \cap U_j)$ are smooth $\forall i$, $j \in I$. 
			\end{itemize}
			The tuples $(\varphi_i, U_i)$, $i\in I$, are so-called \textit{charts} on $M$, the maps $\varphi_i\circ \varphi_j^{-1}$ are called \textit{transition maps} or \textit{changes of coordinates} and $n$ is the \textit{dimension} of $M$.  \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\begin{tikzpicture}
				\draw (0,0) circle (0.15cm);
			\end{tikzpicture}	
		\end{defn}
	\end{samepage}

	\begin{remark}
		To see why the domain of the transition maps $\varphi_i \circ \varphi_j^{-1}$ is $\varphi_j\left(U_i \cap U_j\right)$, note that the expression $\varphi_i\left(\varphi_j^{-1}\left(x\right)\right)$ only makes sense if 
		\begin{align*}
			\left(x\in \varphi_j(U_j)\right) \wedge  \left(\varphi_j^{-1}\left(x\right)\in U_i\right) \Rightarrow \left(x\in \varphi_j(U_j)\right) \wedge \left(x\in \varphi_j\left(U_i\right)\right) \Rightarrow x\in \varphi_j(U_j \cap U_i). 
		\end{align*}
		Similarly, we can convince ourselves that the codomain of the transition maps $\varphi_i\circ\varphi_j^{-1}$ is given by $\varphi_i(U_i\cap U_j)$. Since $x\in \varphi_j(U_j)$, it follows that $\varphi_j^{-1}(x)\in U_j$. In addition, due to the domain of the homeomorphism $\varphi_i$, it must hold that $\varphi_j^{-1}(x) \in U_i$. Thus: 
		\begin{align*}
			\left(\varphi_j^{-1}(x)\in U_j\right) &\wedge \left(\varphi_j^{-1}(x) \in U_i\right) 
			\\
			\Rightarrow 			\left(\varphi_i(\varphi_j^{-1}(x))\in \varphi_i(U_j)\right) &\wedge \left(\varphi_i(\varphi_j^{-1}(x))\in \varphi_i(U_i)\right) 
			\\ 
			\Rightarrow \varphi_i\left(\varphi_j^{-1}(x)\right) &\in \varphi_i\left(U_i\cap U_j\right). 
		\end{align*}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	\end{remark}
	
	\begin{defn}[Equivalence of Atlases]
		Let $M$ be a second countable Hausdorff topological space. Two atlases $\mathcal A$ and $\mathcal B$ on $M$ are called \textit{equivalent} if $\mathcal A\cup \mathcal B$ is an atlas on $M$.  
	\end{defn}

	\begin{remark}
		To see that not all atlases are equivalent to each other, consider $M = \mathbb R$ (which is a second countable Hausdorff topological space). Consider the atlases $\mathcal A = \{ (\varphi, M) \}$ with $\varphi: M\rightarrow M$, $x\mapsto x$ and $\mathcal B = \{(\psi, M)\}$ with $\psi: M\rightarrow M$, $x\mapsto x^3$. The atlases are not equivalent, since $\varphi\circ \psi^{-1}: M \rightarrow M$, $x\mapsto \sqrt[3]{x}$ is not smooth (the derivative is not continuous). 
	\end{remark}
	
	\section{Normalizing Flows}
	\begin{defn}\label{defn-diffeomorphism}
		Let $M$, $N$ be two manifolds, $g: M\rightarrow N$ a differentiable map. If $g$ is a bijection and its inverse $g^{-1}: N\rightarrow M$ is differentiable as well, then we call $f$ a \textit{diffeomorphism}. We talk of a $C^k$ \textit{diffeomorphism} if both $g$ and $g^{-1}$ are $k$-times continuously differentiable. 
	\end{defn}
	
	\begin{theorem}[Change of variables \cite{MfPIII}]\label{change-of-variables}
		Let $U$, $V\subset\mathbb{R}^n$ be open subsets and $T: U\rightarrow V$ a diffeomorphism, cf. Def.  \ref{defn-diffeomorphism}. Then the function $f: V\rightarrow \mathbb{C} \ \cup \{\infty\}$ is integrable over $V$ if and only if the function 
		\begin{align}
			\left(f\circ T\right) \cdot \left\vert \det \left( \frac{\partial T_{\mu}}{\partial x_{\nu}} \right)_{\mu\nu} \right\vert 
		\end{align}
		is integrable over $U$. In this case, it holds that 
		\begin{align}\label{change-of-variables-formula}
			\int_{U} \left(f\circ T\right)(x) \cdot \left\vert \det\left(\frac{\partial T_{\mu}}{\partial x_{\nu}}(x)\right)_{\mu, \nu} \right\vert dx = \int_{V} f(y)dy. 
		\end{align}
	\end{theorem}
	\begin{remark}\label{diffeomorphism-inverse}
		If $T$ is a diffeomorphism, then also $T^{-1}$ is a diffeomorphism, thus we could also have chosen $T^{-1}$ in the formulation of Theorem \ref{change-of-variables}.
	\end{remark}
	
	\begin{theorem}[Inverse function theorem \cite{IFT}] Let $g: \mathbb R^n \rightarrow \mathbb R^n$ be continuously differentiable on some open set $V\subset \mathbb R^n$ containing $\mathbf{a}$ and suppose $\det Jg(\mathbf{a}) \ne 0$, where $J$ shall be the Jacobi matrix of $g$. Then there is some open set containing $\mathbf{a}$ and an open set $W\subset \mathbb{R}^n$ containing $g(\mathbf{a})$ such that $g: V\rightarrow W$ has a continuous inverse $g^{-1}: W\rightarrow V$ which is differentiable for all $\mathbf{y}\in W$. 
		\\ \\	
		As matrices, we can write this as 
		\begin{align}
			J(g^{-1})(\mathbf{y}) = \left[\left(Jg\right)\left(g^{-1}(\mathbf{y})\right)\right] ^{-1}
		\end{align} 
	\end{theorem}
	
	\begin{remark} An example for a function that is invertible and continuously differentiable but not a diffeomorphism is $g: \mathbb{R}\rightarrow\mathbb{R}$, $x\mapsto x^3$. Its inverse is obviously $g^{-1}: \mathbb{R}\rightarrow\mathbb{R}, x\mapsto \sqrt[3]{x}$, cf. \cite{cube-root} for a nice plot. However, $\frac{dg^{-1}}{dx}|_{x = 0}$ does not exist. The reason is that $\det Jg(0) = 0$ and hence the inverse function theorem does not apply. 
	\end{remark}
	\begin{itemize}
		\item Let $\mathbf{U}$ be a random variable and let $p(\mathbf{U})$ describe the probability distribution of it, e.g. a uniform distribution between $0$ and $1$. We now make a simple transformation and obtain a new random variable $\mathbf{X}$, where we again denote by $p(\mathbf{X})$ the probability distribution of $\mathbf{X}$. We obtain $\mathbf{X}$ in the following way: 
		\begin{align}\label{normalizing-flow-forward}
			p(\mathbf{X}) = p(\mathbf{U})\left\vert \det\left(\frac{\partial\mathbf{f}}{\partial \mathbf{U}}\right)\right\vert^{-1}, 
		\end{align}
		where $\mathbf{f}$ denotes an invertible (and hence bijective) mapping. 
		
		\item Without proof, it holds that 
		\begin{align}
			\left\vert \det\left(\frac{\partial\mathbf{f}}{\partial\mathbf{U}}\right)\right\vert^{-1} = \left\vert \text{det}\left(\frac{\partial\mathbf{f}^{-1}}{\partial\mathbf{U}}\right) \right\vert 
		\end{align}
		and thus we can rewrite Eq. \eqref{normalizing-flow-forward} as 
		\begin{align}\label{normalizing-flow-backward}
			p(\mathbf{U}) = p(\mathbf{X})\left\vert \det\left(\frac{\partial\mathbf{f^{-1}}}{\partial \mathbf{U}}\right)\right\vert^{-1}. 
		\end{align}
		Since we assumed $\mathbf{f}$ to be invertible, $\mathbf{f^{-1}}$ is well-defined. 
		
		\item In practice, we will want $\mathbf{f}$ to be both invertible \underline{and} to have a \textbf{tractable} Jacobian, i.e. a Jacobian that we can easily calculate. For $\mathbf{f}$ to have a Jacobian at all, each of its first-order partial derivatives must exist \cite{jacobi-matrix}.
		So-called \textit{autoregressive flows} have the property that their Jacobian is an \underline{upper triangular matrix}. For an upper triangular matrix, it holds that its determinant is given by the product of its diagonal elements \cite{triangular-matrices}.
	\end{itemize}

	\begin{defn}[Determinant]
		Let $D$ be an $n\times n$ matrix and let $S_n$ denote the symmetric group over $n$. Then the determinant of $D$ is defined as:  
		\begin{align}
			\det(D) := \sum_{\sigma\in S_n} \left( \text{sgn}(\sigma) \prod_{i = 1}^{n}a_{i, \sigma(i)} \right),  
		\end{align}
		cf. \cite{leibniz-formula}. 
	\end{defn}

	\begin{lemma}
		Let $A$ be a $k\times k$, $0$ an $k\times n$, $C$ an $n\times k$ and $D$ an $n\times n$ matrix; then 
		\begin{align}
			\det\left( \begin{pmatrix}	A & 0 \\ C & D \end{pmatrix} \right) = \det(A)\det(D). 
		\end{align}
	\end{lemma}
	\noindent \textit{Proof.} Define 
	\begin{align}
		B := \begin{pmatrix} A & 0 \\ C & D
		\end{pmatrix}. 
	\end{align}
	Clearly, 
	\begin{align}\label{case-diff}
		b_{i,j} = 
		\begin{cases}
			a_{i, j} \qquad &i, j\leq k, \\
			0 		 \qquad &i\leq k, j \geq k+1, \\
			c_{i-k, j} \qquad &i\geq k+1, j \leq k, \\
			d_{i-k, j-k} \qquad &i, j\geq k+1. 
		\end{cases}
	\end{align}
	We can write the determinant of $B$ as 
	\begin{align}
		\det(B) = \sum_{\sigma\in S_{n+k}} \text{sgn}(\sigma)\prod_{i = 1}^{n+k}b_{i, \sigma(i)}. 
	\end{align}
	From Eq. \eqref{case-diff} we know that all summands of the form $\sigma(i) = j$ with $i\leq k$, $j\geq k+1$ are $0$. Therefore, we can consider all permutations of the form $\sigma(i) = j$ with $i, j \leq k$ or $\sigma(i) = j$ with $i\geq k+1$, $j\leq k$. We can also write this in the form $\sigma(i) = \pi(i)$ for $i\leq k$ and $\sigma(k + i) = k + \tau(i)$ for $1\leq i \leq n$, where $\pi\in S_k$ and $\tau\in S_n$. Denote the set of all such permutations by $\tilde{S}_{k+n}$.  Thus: 
	\begin{align}
		\det(B) &= \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{n+k}b_{i, \sigma(i)}
		\\ &= \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{k}b_{i, \sigma(i)}\prod_{i = k+1}^{n+k}b_{i, \sigma(i)}
		\\ &\overset{\tiny\eqref{case-diff}}{=} \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{k}a_{i, \sigma(i)}\prod_{i=k+1}^{n+k}d_{i-k, \sigma(i)-k}
		\\ &= \sum_{\sigma\in\tilde{S}_{k+n}}\text{sgn}(\sigma)\prod_{i = 1}^{k}a_{i, \sigma(i)}\prod_{i = 1}^{n}d_{i, \sigma(i+k)-k}
		\\ &= \sum_{\pi\in S_k, \tau\in S_n}\text{sgn}(\pi)\text{sgn}(\tau)\prod_{i = 1}^{k}a_{i, \pi(i)}\prod_{i = 1}^{n}d_{i, \tau(i)}
		\\ &= \sum_{\pi\in S_k}\text{sgn}(\pi)\prod_{i = 1}^{k}a_{i, \pi(i)}\sum_{\tau\in S_n}\text{sgn}(\tau)\prod_{i = 1}^{n}d_{i, \tau(i)}
		\\ &= \det(A)\det(D)
	\end{align}
	\cite{block-triangular-matrix}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}
		Let $h(\bm{\cdot}\ ; \theta): \mathbb{R}\rightarrow\mathbb{R}$ be a bijection parametrized by $\theta$. Then an \textit{autoregressive model} is a function \begin{align}
			g: \mathbb{R}^D\rightarrow\mathbb{R}^D, \begin{pmatrix}
				x_1 \\ \dots \\ x_D 
			\end{pmatrix} \mapsto 
			\begin{pmatrix}
				h\left(x_1; \Theta_1\right) \\ h\left(x_2; 	\Theta_2\left(x_1\right)\right) \\ \dots \\ h\left(x_D; \Theta_D\left(x_1, \dots, x_{D-1}\right)\right)
			\end{pmatrix}
		\end{align}
		The functions $\Theta_t$ for $t = 2$, $\dots$, $D$ are arbitrary functions whose domain is $\mathbb{R}^{t-1}$, $\Theta_1$ is a constant. 
	\end{defn}

	\begin{remark}
		The Jacobian matrix of an autoregressive flow is given as follows: 
		\begin{align}
			Dg &= \begin{pmatrix}
				\partial g_1/\partial x_1 & \partial g_1/\partial x_2 & \dots & \partial g_1/\partial x_D 
								\\ 
				\partial g_2/\partial x_1 & \partial g_2/\partial x_2 & \dots & \partial g_2/\partial x_D
								\\
				\vdots & \vdots & \ddots & \vdots 
								\\ 
				\partial g_D/\partial x_1 & \partial g_D / \partial x_2 & \dots & \partial g_D / \partial x_D
			\end{pmatrix} 
		\end{align}
	One can easily convince oneself that $Dg$ is a lower triangular matrix. 
	\end{remark}

	\begin{theorem}
		Normalizing flows in $\mathbb{R}^D$ come from the push-forward of a measure. 
	\end{theorem}
	\noindent \textit{Proof.} Let $\mathcal Y$, $\mathcal Z\subset\mathbb{R}^D$ be open,  $\Sigma_{\mathcal Y} = \mathcal B(\mathcal Y)$, $\Sigma_{\mathcal Z} = \mathcal B(\mathcal Z)$ and $g: \mathcal Z\rightarrow\mathcal Y$ be a diffeomorphism. The function $g: \mathcal Z \rightarrow \mathcal Y$ is measurable if and only if $g^{-1}(G)\in \Sigma_{\mathcal Z}$ for every set $G$ that is open in $\mathcal Y$, cf. Theorem \ref{generator-and-measurable-function} (Borel $\sigma$-algebras are generated by the open sets). Since for functions between two topological spaces it holds that they are continuous if and only if the inverse image of an open set is again open, we have that $g$ is indeed measurable. 
	\\ \\ Now define the probability measure $\mu$ on the measurable space $(\mathcal Z, \Sigma_{\mathcal Z})$ as 
	\begin{align}\label{prob-measure-proof}
		\mu(I) := \int_{I} p_{\mathbf{Z}}(z)d\lambda(z) \ \forall I\in \Sigma_{\mathcal Z}, 
	\end{align}
	where $p_{\mathbf{Z}}: \mathcal Z \rightarrow\mathbb{R}$ shall be a PDF and $\lambda$ the Lebesgue measure. (The Lebesgue measure is defined on the completion of $\mathcal B(\mathbb R^D)$, which is \enquote{larger} than $\mathcal B(\mathbb R^D)$.) By considering the push-forward of $\mu$ under the measurable map $g$, we have $\forall J \in \Sigma_{\mathcal Y}$: 
	\begin{align}
		g_{\star}\mu\left(J\right) = \mu(g^{-1}(J))  \overset{\tiny\eqref{prob-measure-proof}}{=} \int_{g^{-1}(J)} p_{\mathbf{Z}}(z)d\lambda(z)
	\end{align}
	Since by assumption $g:\mathcal Z\rightarrow \mathcal Y$ and therefore also the inverse $g^{-1}: \mathcal Y \rightarrow \mathcal Z$ are a diffeomorphism, we can use the change of variables formula from Theorem \ref{change-of-variables}, since we assume $p_{\mathbf{Z}}$ to be integrable over $\mathcal Z$. We apply Theorem \ref{change-of-variables} to $g^{-1}$ instead of $g$, cf. Remark \ref{diffeomorphism-inverse}: 
	\begin{align}
		g_{\star}\mu(J) = \int_{g^{-1}(J)}p_{\mathbf{Z}}(z)d\lambda(z) = \int_{J} \underbrace{\left(p_{\mathbf{Z}}\circ g^{-1}\right)(y) \cdot \left\vert \det Dg^{-1}(y) \right\vert }_{= p_{\mathbf{Y}}(y)} d\lambda(y) 						
	\end{align}
	\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad$\blacksquare$
	
	\begin{defn}
		A rational-quadratic function takes the form of a quotient of two quadratic polynomials. 
		\begin{align}
			\frac{\alpha^{(k)}\left(\xi\right)}{\beta^{(k)}(\xi)} = \frac{a_0\xi^2 + a_1\xi + a_2}{b_0\xi^2 + b_1\xi + b_2} 
		\end{align}
	\end{defn}	

\newpage 

\section{Diffusion-Based Models}
	
	\begin{remark}[ELBO VAEs]
		One can easily show that the (marginal) log-likelihood of the data is given as
		\cite{cs_231n_lec_13}
		\begin{align}\label{eq:elbo__marginal_LL}
			\log p(x) = \text{ELBO} + D_{\text{KL}}\left[ q_{\phi}(z\vert x) \vert\vert p(z\vert x)\right], 
		\end{align}
		
		\noindent where $p(z\vert x)$ is the true posterior and ELBO is the usual evidence lower bound, i.e.
		\begin{align}
			\text{ELBO} = \mathbb E_{q_{\phi}(z\vert x)}\left[ \log p_{\psi}(x\vert z) \right] - D_{\text{KL}}\left[ q_{\phi}(z\vert x) \vert\vert p(z) \right]
		\end{align}
		
		\noindent Eq.~\eqref{eq:elbo__marginal_LL} shows that since the (marginal) log-likelihood is not parameterized by any NN parameters, maximizing the ELBO necessary leads to a lower $D_{\text{KL}}\left[ q_{\phi}(z\vert x) \vert\vert p(z\vert x)\right]$, which is the reverse KLD. 
		
	\end{remark}

\begin{lemma}[ELBO]
	Let $q(x_{0})$ denote the true (unknown) distribution of a real image $x_{0}$, and let $p_{\theta}(x_{0})$ be the model's approximation to $q(x_{0})$, then we have the following ELBO-like loss: 
	\begin{align}\label{elbo_diffusion_models}
		\mathbb E_{q(x_{0})}\left[\log p_{\theta}(x_{0})\right] \geq -\mathbb E_{q(x_{0}, \dots, x_{T})}\left[\log \frac{q(x_{1}, \dots, x_{T} \mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right].    
	\end{align}
\end{lemma}

\begin{proof}
\cite{lilian_weng}
	\begin{align}
		\log p_{\theta}(x_{0}) &\geq \log p_{\theta}(x_{0}) - D_{\text{KL}}\left[ q(x_{1}, \dots, x_{T}\mid x_{0})\mid\mid p_{\theta}(x_{1}, \dots, x_{T}\mid x_{0}) \right]
		\\[4pt] &= \log p_{\theta}(x_{0}) - \mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{1}, \dots, x_{T} \mid x_{0})}\right]
		\\[4pt] &= \log p_{\theta}(x_{0}) - \mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})/ p_{\theta}(x_{0})}\right]
		\\[4pt] &= -\mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right]
		\\[4pt] \Rightarrow \mathbb E_{q(x_{0})}\left[\log p_{\theta}(x_{0})\right] &\geq -\mathbb E_{q(x_{0})}\mathbb E_{q(x_{1}, \dots, x_{T}\mid x_{0})}\left[\log\frac{q(x_{1}, \dots, x_{T}\mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right]
	\end{align}
	Assuming that the assumptions of Fubini's theorem hold and from the monotonicity of the expectation, we have: 
	\begin{align}\label{diff_elbo_to_be_proved}
		\mathbb E_{q(x_{0})}\left[ \log p_{\theta}(x_{0}) \right] \geq \mathbb E_{q(x_{1}, \dots,  x_{T} \mid x_{0})q(x_{0})}\left[\log \frac{q(x_{1}, \dots, x_{T} \mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right] = E_{q(x_{0}, \dots,  x_{T} \mid x_{0})}\left[\log \frac{q(x_{1}, \dots, x_{T} \mid x_{0})}{p_{\theta}(x_{0}, \dots, x_{T})}\right].
	\end{align}
\end{proof}

\begin{defn}[Wiener process]
	Let $W_{t}$ be a real-valued continuous-time stochastic process.\ It is said to be a \textit{Wiener process} if the following properties hold: 
	\begin{itemize}
		\item $W_{0} = 0$, 
		\item $W$ has independent increments, i.e.\ $\forall t > 0:$, the terms $W_{t+u} - W_{t}$, $u\geq 0$ are independent of past values $W_{s}$, $s\leq t$, 
		\item $W$ has Gaussian increments:\ $W_{t+u} - W_{t} \sim \mathcal N(0, u)$, 
		\item $W$ has continuous paths, i.e.\ $\forall t$, $W_{t}$ is continuous in $t$. 
	\end{itemize}
	source:\ \url{https://en.wikipedia.org/wiki/Wiener_process}
\end{defn}

\begin{remark}
	If $\xi_{1}$, $\xi_{2}$, $\dots$ be i.i.d random variables with a mean of $0$ and standard deviation of $1$.\ For every $n$, define a continuous time stochastic process 
	\begin{align}\label{random_walk_Wiener}
		W_{n}(t) := \frac{1}{\sqrt{n}}\sum_{1\leq k\leq\lfloor nt \rfloor}\xi_{k} 
	\end{align}
	This is what makes Wiener processes so powerful (and explains the ubiquity of Brownian motion).\ According to Donsker's theorem, the  above expression becomes a Wiener process.
\end{remark}

\begin{defn}[Globally Lipschitz continuous]
	
	
\end{defn}

\newpage 

\section{Miscellaneous}

	\begin{lemma}[Chain rule for KL-divergences]
		Let $p(x, y)$ and $q(x, y)$ be two arbitrary PDF's. Then the following hods:
		
		\begin{align}
			D_{\text{KL}}(p(x, y) \vert\vert q(x, y)) = D_{\text{KL}}(p(x)\vert\vert q(x)) + D_{\text{KL}}(p(y\vert x) \vert\vert q(y\vert x))
		\end{align}
		
		\begin{proof}
			Brute-force calculation yields:
			\begin{align}
				D_{\text{KL}}(p(x, y) \vert\vert q(x, y)) &= \int\int p(x, y)\log\frac{p(x, y)}{q(x, y)}d\lambda(x)d\lambda(y)
				\\[4pt] &= \int\int p(x, y)\log\frac{p(x)p(y\vert x)}{q(x)q(y\vert x)}d\lambda(x)d\lambda(y)
				\\[4pt] &= \int\int p(x, y)\log\frac{p(x)}{q(x)}d\lambda(x)d\lambda(y) + \int\int p(x, y)\log\frac{p(y\vert x)}{q(y\vert x)}d\lambda(x)d\lambda(y)
				\\[4pt] &= D_{\text{KL}}(p(x) \vert\vert q(x)) + \int p(x)d\lambda(x)\int p(y\vert x)\log\frac{p(y\vert x)}{q(y\vert x)}d\lambda(y)
				\\[4pt] &= D_{\text{KL}}(p(x) \vert\vert q(x)) + D_{\text{KL}}(p(y\vert x) \vert\vert q(y\vert x))
			\end{align}
		\end{proof}
	\end{lemma}
	
	\begin{defn}[Mutual information]
		Let $(X, Y)\sim P_{\left(X, Y\right)}\in \mathcal P(\mathcal X\times \mathcal Y)$, where $\mathcal P(\mathcal X\times \mathcal Y)$ is the space of all probability measures over the space $\mathcal X\times \mathcal Y$. The mutual information between the random variables $X$ and $Y$ is now defined as 
		\cite[Def.~10.1]{ece_ece_5630_lectures10}:
		
		\begin{align}
			I(X; Y) := D_{\text{KL}}\left(P_{(X, Y)} \vert\vert P_{X} \otimes P_{Y}\right),
		\end{align}
	
		\noindent where $P_{X}$ and $P_{Y}$ are the marginal measures of the coupling measure $P_{(X, Y)}$ and $P_{X}\otimes P_{Y}$ is the induced \textbf{product measure}.
	\end{defn}

	\begin{defn}[Divergence]
		Let $p$ and $q$ be two probability distributions, then a divergence $D$ must satisfy:~$D(p\vert\vert q) \geq 0$ $\forall p,q$ and $D(p\vert\vert q) = 0 \Leftrightarrow p = q$ a.e. Note that the triangle inequality and the symmetry property need not be satisfied in general.
	\end{defn}

	\begin{exmp}[$f$-Divergence]
		Let $P$ and $Q$ be two probability measures defined on the $\sigma$-algebra over a space $\Omega$ such that $P \ll Q$, i.e.~$P$ is absolutely continuous wrt $Q$. Then, for a convex function $f:[0, \infty)\rightarrow \mathbb R$ s.t.
		
		\begin{enumerate}[label=(\roman*)]
			\item $f(1) = 0$, 
			\item $f$ is strictly convex at $x = 1$,
		\end{enumerate}
	
		\noindent the $f$-divergence is now defined as \cite{ece_ece_5630_lectures6}, 
		\begin{align}\label{eq:def__f_divergence}
			D_{f}(P\vert\vert Q) := \int_{\Omega} f\left( \frac{dP}{dQ} \right)dQ, 
		\end{align}
		where $dP/dQ$ is the Radon-Nikodym derivative.
	\end{exmp}

	\begin{proof}
		\begin{enumerate}
			\item \textbf{Non-negativity}:~We know that 
			\begin{align}
				\mathbb E_{Q}\left[ \frac{dP}{dQ} \right] = \int_{\Omega} \frac{dP}{dQ} dQ = 1.
			\end{align}
			
			Since $f$ is a convex function, by Jensen's inequality,
			\begin{align}
				0 = f(1) = f\left(\mathbb E_{Q}\left[ \frac{dP}{dQ} \right]\right) \leq \mathbb E_{Q}\left[ f\left( \frac{dP}{dQ} \right) \right] = \int_{\Omega} f\left( \frac{dP}{dQ} \right) dQ \overset{\scriptsize\eqref{eq:def__f_divergence}}{=} D_{f}(P\vert\vert Q).
			\end{align}
			
			\item \textbf{Zero iff equal}:~If $P = Q$ a.e., then $dP / dQ = 1$ a.e., and hence
			\begin{align}
				D_{f}(P\vert\vert Q) = \int_{\Omega} f(1)dQ = \int_{\Omega} 0dQ = 0.
			\end{align}
			
			Conversely, if $D_{f}(P\vert\vert Q) = 0$, since $f$ is strictly convex at $1$ and convex everywhere else, this implies that the only minimum point of $f$ is at $f(1) = 0$. Therefore, the integrand of Eq.~\eqref{eq:def__f_divergence}, i.e.~$f\left(\frac{dP}{dQ}\right)$, must be $0$ a.e., implying that 
			\begin{align}
				\frac{dP}{dQ} = 1\Rightarrow P = Q.
			\end{align}
			
		\end{enumerate}
	\end{proof}

\newpage 
\printbibliography

\end{document} 