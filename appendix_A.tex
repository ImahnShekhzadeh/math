\section{Weierstrass Approximation Theorem}\label{app:weierstrass-app-theorem}

In the following, we will prove the Weierstrass approximation theorem, as stated in Example \ref{exmp:weierstrass-approx-thrm}. The proof will use \textit{Korovkin\footnote{\textsc{Pavel Petrovich Korovkin} (1913 -- 1985), Russian mathematician.} sequences}. The approach is taken from \mbox{Chapter 6} of \cite{iske:approximation}.

\begin{defn}[Positivity of Operator]\label{defn:positivity-operator}
	A linear operator $K: \mathcal C[a, b] \to \mathcal C[a, b]$ is \textit{positive} on $\mathcal C[a, b]$ if $Kf \geq 0$ for all $f\in \mathcal C[a, b]$ satisfying $f\geq 0$, where all inequalities are taken pointwise on $[a, b]$.
\end{defn}

\begin{defn}[Monotonicity of Operator]\label{defn:monotonicity-operator}
	A linear operator $K$: $\mathcal C[a, b] \to \mathcal C[a, b]$ is \textit{monotone} on $\mathcal C[a, b]$ if for any \mbox{$f$, $g\in\mathcal C[a, b]$} satisfying $f\leq g$, we have $Kf \leq Kg$, where all inequalities are taken pointwise on $[a, b]$.
\end{defn}

\begin{remark}
	A linear operator is positive iff it is monotone.
\end{remark}

\begin{proof}
	\enquote{$\Longrightarrow$} Consider $h := g - f \in \mathcal C[a, b]$, where $g$, $f\in\mathcal C[a, b]$, with $h = g -f \geq 0$, i.e. $f\leq g$, then by the positivity and linearity of $K$ we have $Kh = Kg - Kf\geq 0$, or equivalently $Kf \leq Kg$.
	
	\enquote{$\Longleftarrow$} Assume that for any \mbox{$f$, $g\in\mathcal C[a, b]$} satisfying $f\leq g$, we have $Kf \leq Kg$. Take $f = 0$, then $g\geq 0$ implies $Kg \geq 0$ for any $g\in\mathcal C[a, b]$.
\end{proof}

\begin{defn}[Korovkin sequence]
	A sequence $\seq[K_n]$ of linear and positive operators $K_n: \mathcal C[a, b]\to\mathcal C[a, b]$ is called a \textit{Korovkin sequence} on $\mathcal C[a, b]$ if $K_np \overset{n\to\infty}{\longrightarrow}p$ wrt $d_{\infty}$ for all $p\in \mathcal P_{2}$, where $\mathcal P_n$ is the linear space of all univariate polynomials of degree at most $n$.
\end{defn}

\begin{remark}
	In the following, we might restrict ourselves to the continuous functions $\mathcal C[0, 1]$ on $[0, 1]$. This is WLOG, since for any $x\in [a, b]$, we can transform it into $[0, 1]$ via a simple affine-linear transformation.
\end{remark}

\begin{defn}
	Let $x\in[0, 1]$, then the \textit{Bernstein\footnote{\textsc{Sergei Natanovich Bernstein} (1880 -- 1968)} polynomials} are defined as
	\begin{align}
		\beta_{j}^{(n)}(x) := \begin{pmatrix}
			n \\ j
		\end{pmatrix} x^j (1 - x)^{n-j} \in\mathcal P_n \quad \text{with}\ 0\leq j\leq n, n\in \mathbb N_{0}
	\end{align}
\end{defn}

\begin{exmp}
	Here are some Bernstein polynomials:
	\begin{gather*}
		\bm{n = 0}: \quad \beta_{0}^{(0)}(x) = 1
		\\[6pt]
		\bm{n = 1}: \quad \beta_{0}^{(1)}(x) = 1 - x \quad \beta_{1}^{(1)}(x) = x
		\\[6pt]
		\bm{n = 2}: \quad \beta_{0}^{(2)}(x) = (1 - x)^2 \quad \beta_{1}^{(2)}(x) = 2x(1 - x) \quad \beta_{2}^{(2)}(x) = x^2
		\\[6pt]
		\bm{n = 3}: \quad \beta_{0}^{(3)}(x) = (1-x)^3 \quad \beta_{1}^{(3)}(x) = 3x(1-x)^2 \quad \beta_{2}^{(3)}(x) = 3x^2(1-x) \quad \beta_{3}^{(3)}(x) = x^3
	\end{gather*} 
\end{exmp}

\begin{remark}
	The Bernstein polynomials $\beta_{0}^{(n)}, \dots, \beta_{n}^{(n)}\in\mathcal P_n$, for $n\in\mathbb N_{0}$, \dots 
	\begin{enumerate}[label=(\alph*)]
		\item \dots form a basis for the polynomial space $\mathcal P_n$;
		\item \dots are positive on $[0, 1]$, i.e. $\beta_{j}^{(n)}(x) \geq 0$ for all $x\in[0, 1]$;
		\item \dots are symmetrical around $x = 1/2$, i.e. $\beta_{j}^{(n)}(x) = \beta_{n-j}^{(n)}(1 - x)$;
		\item \dots are a \textit{partition of unity} on $[0, 1]$, i.e. 
		\begin{align}
			\sum_{j=0}^{n}\beta_j^{(n)}(x) = 1 \ \forall x\in [0, 1].
		\end{align} 
	\end{enumerate}
\end{remark}

\begin{proof}
	\begin{enumerate}[label=(\alph*)]
		\item From linear algebra, we know that a set is a basis of a finite-dimensional vector space iff it has the same dimension and all vectors in the set are linearly independent. Since $\dim(\mathcal P_n) = n + 1$ and $\left\{ \beta_{0}^{(n)}(x), \dots, \beta_{n}^{(n)}(x) \right\}$ contains $n + 1$ vectors, it only remains to show that the $\beta_{0}^{(n)}(x), \dots, \beta_{n}^{(n)}(x)$ are linearly independent.
		
		For this, note that \cite[proof Lemma 2.5 ix)]{thesis:bernstein_polynomials}
		\begin{align}
			\beta_j^{(n)}(x) &= \begin{pmatrix}
				n \\ j
			\end{pmatrix} x^j (1 - x)^{n-j} \in\mathcal P_n \quad \text{with}\ 0\leq j\leq n
			\\[4pt] &= \begin{pmatrix}
				n \\ j
			\end{pmatrix} x^j\sum_{i = 0}^{n - j}\begin{pmatrix}
				n - j\\ i
			\end{pmatrix} \left(-x\right)^{i}
			\\[4pt] &= \sum_{i=0}^{n - j}\begin{pmatrix}
				n \\ j
			\end{pmatrix}\begin{pmatrix} n - j\\ i \end{pmatrix} \left(-1\right)^i x^{j + i}
			\\[4pt] &= \sum_{i=j}^{n}\begin{pmatrix}
				n \\ j
			\end{pmatrix}\begin{pmatrix} n - j\\ i - j\end{pmatrix} \left(-1\right)^{i - j} x^{i}
			\\[4pt] \label{eq:bernstein_polynomial_rewriting} &= \sum_{i=j}^{n}\begin{pmatrix}
				n \\ i
			\end{pmatrix}\begin{pmatrix} i\\ j\end{pmatrix} \left(-1\right)^{i - j} x^{i}
		\end{align}
		Now consider the equation (with $\lambda_k\in\mathbb R$) for all $0\leq k\leq n$:
		\begin{align}\label{eq:bernstein_polynomial_rewriting_2}
			0 = \sum_{k=0}^{n} \lambda_k \beta_{k}^{(n)} \overset{\tiny\eqref{eq:bernstein_polynomial_rewriting}}{=} \sum_{k=0}^{n}\lambda_k\sum_{i=k}^{n}\begin{pmatrix}
				n \\ i
			\end{pmatrix}\begin{pmatrix} i\\ k\end{pmatrix} \left(-1\right)^{i - k} x^{i}
		\end{align}
		For the double sum, we will use the following identity with $a_{ik}\in\mathbb R$ \cite[p. 2]{article:sums}:
		\begin{align}
			\sum_{k=0}^{n}\sum_{i=k}^{n}a_{ik} = \sum_{0\leq k \leq i\leq n} a_{ik} = \sum_{i=0}^{n}\sum_{k=0}^{i}a_{ik} = \sum_{k=0}^{n}\sum_{i=0}^{k}a_{ki},
		\end{align}
		since $0\leq i\leq n$, and for a fixed $i$, $0\leq k\leq i$, and where we exchanged the indices ($i\mapsto k$, $k\mapsto i$). Thus, Eq. \eqref{eq:bernstein_polynomial_rewriting_2} becomes 
		\begin{align}
			0 = \sum_{k=0}^{n} \lambda_k \beta_{k}^{(n)} &= \sum_{k=0}^{n}x^{k}\sum_{i=0}^{k}\lambda_i \begin{pmatrix}
				n \\ k
			\end{pmatrix}\begin{pmatrix} k\\ i\end{pmatrix} \left(-1\right)^{k - i} \label{eq:bernstein_polynomials_lin_indep}
			\\ &= \lambda_0 +\left(-n\lambda_0 + n\lambda_1\right)x + \left(\lambda_0n\left(n-1\right) - \lambda_1 n(n-1) + \frac{\lambda_2 n(n-1)}{2} \right)x^2 + \dots
		\end{align}
		Thus, every polynomial $p\in\mathcal P_n$ can be written as the linear combination of \\ $\left\{ \beta_{0}^{(n)}(x), \dots, \beta_{n}^{(n)}(x) \right\}$. Also, note that we already know that the monomials $\{x^0, x^1, \dots, x^n\}$ form a basis for $\mathcal P_n$, thus 
		\begin{align}
			\lambda_0 &= 0
			\\ -n\lambda_0 + n\lambda_1 &= 0 \label{eq:bernstein_lin_indepen_2}
			\\ \left(\lambda_0n\left(n-1\right) - \lambda_1 n(n-1) + \frac{\lambda_2 n(n-1)}{2} \right) &= 0 \label{eq:bernstein_lin_indepen_3}
			\\ \nonumber  &\vdots 
		\end{align}
		By putting $\lambda_0 = 0$ into Eq. \eqref{eq:bernstein_lin_indepen_2}, we get $\lambda_1 = 0$. Putting $\lambda_0 = \lambda_1 = 0$ into Eq. \eqref{eq:bernstein_lin_indepen_3}, we have $\lambda_2 = 0$. Thus, we can recursively show that $\lambda_k = 0$ for all $0\leq k\leq n$, and hence the set $\left\{ \beta_{j}^{(n)}(x) \right\}_{0\leq j\leq n}$ is linearly independent. 
		
		\item This is straightforward, since $x^j\geq 0$, $(1 -x)^{n - j} \geq 0$ and $\begin{pmatrix}
			n \\ j
		\end{pmatrix} \geq 0$ for all $x\in [0, 1]$, hence $\beta_{j}^{(n)}(x) \geq 0$.
	
		\item We have
		\begin{align}
			\beta_{n-j}^{(n)}(1 - x) = \begin{pmatrix}
				n \\ n -j 
			\end{pmatrix}(1 - x)^{n - j}x^j = \begin{pmatrix}
				n \\ j 
			\end{pmatrix}(1 - x)^{n - j}x^j = \beta_{j}^{(n)}(x).		
		\end{align} 
	
		\item By the binomial theorem we have
		\begin{align}
			\sum_{j=0}^{n}\beta_{j}^{(n)}(x) = \sum_{j=0}^{n}\begin{pmatrix}
				n \\ j
			\end{pmatrix}x^j(1-x)^{n-j} = (x + (1-x))^n = 1.
		\end{align}
	\end{enumerate}		
\end{proof}
